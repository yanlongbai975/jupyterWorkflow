{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "abccc83e",
   "metadata": {},
   "source": [
    "**I have updated two parts 'Resampling for unbalanced dataset' and 'Using k-fold validation to train CNN (NEW)'.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e63add21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  0\n",
      "2.9.1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e996e894",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd   ## data analysis and manipulation\n",
    "import numpy as np    ## numerial computing\n",
    "import seaborn as sns ##  data visualization library based on matplotlib\n",
    "import tensorflow.keras as keras ## main deep learning API\n",
    "\n",
    "## additional functions\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, accuracy_score\n",
    "from sklearn.metrics import average_precision_score\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0b103734",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Conv1D\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import MaxPooling1D\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from sklearn.utils import class_weight\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f6f4a85",
   "metadata": {},
   "source": [
    "Load the data, change to your directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "89c802e4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>EID</th>\n",
       "      <th>PID</th>\n",
       "      <th>DOB</th>\n",
       "      <th>Eye</th>\n",
       "      <th>ImageID</th>\n",
       "      <th>Scan.Type</th>\n",
       "      <th>Diameter..mm.</th>\n",
       "      <th>Diameter....</th>\n",
       "      <th>Fixed.in.mm</th>\n",
       "      <th>ExamDate</th>\n",
       "      <th>...</th>\n",
       "      <th>VF_EXAM_START</th>\n",
       "      <th>VF_EXAM_END</th>\n",
       "      <th>VF_FOLLOW_UP</th>\n",
       "      <th>VF_N</th>\n",
       "      <th>VF_OCT_BASELINE_DIFF</th>\n",
       "      <th>VF_OCT_FINAL_DIFF</th>\n",
       "      <th>MD_BASELINE</th>\n",
       "      <th>MD_FINAL</th>\n",
       "      <th>VFI_BASELINE</th>\n",
       "      <th>VFI_FINAL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10/24/1949</td>\n",
       "      <td>LE</td>\n",
       "      <td>282596.0</td>\n",
       "      <td>OCT Circle Scan</td>\n",
       "      <td>3.7</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0</td>\n",
       "      <td>5/11/2017</td>\n",
       "      <td>...</td>\n",
       "      <td>5/11/2017</td>\n",
       "      <td>12/2/2020</td>\n",
       "      <td>3.561944</td>\n",
       "      <td>6</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.561944</td>\n",
       "      <td>-2.15</td>\n",
       "      <td>-3.26</td>\n",
       "      <td>98</td>\n",
       "      <td>96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>10/24/1949</td>\n",
       "      <td>RE</td>\n",
       "      <td>282593.0</td>\n",
       "      <td>OCT Circle Scan</td>\n",
       "      <td>3.7</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0</td>\n",
       "      <td>5/11/2017</td>\n",
       "      <td>...</td>\n",
       "      <td>5/11/2017</td>\n",
       "      <td>12/2/2020</td>\n",
       "      <td>3.561944</td>\n",
       "      <td>6</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.561944</td>\n",
       "      <td>-7.73</td>\n",
       "      <td>-11.45</td>\n",
       "      <td>82</td>\n",
       "      <td>73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>8/7/1939</td>\n",
       "      <td>LE</td>\n",
       "      <td>239514.0</td>\n",
       "      <td>OCT Circle Scan</td>\n",
       "      <td>3.4</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0</td>\n",
       "      <td>8/26/2014</td>\n",
       "      <td>...</td>\n",
       "      <td>8/26/2014</td>\n",
       "      <td>10/20/2020</td>\n",
       "      <td>6.151951</td>\n",
       "      <td>6</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.151951</td>\n",
       "      <td>-1.28</td>\n",
       "      <td>-1.13</td>\n",
       "      <td>98</td>\n",
       "      <td>97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>8/7/1939</td>\n",
       "      <td>RE</td>\n",
       "      <td>239512.0</td>\n",
       "      <td>OCT Circle Scan</td>\n",
       "      <td>3.4</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0</td>\n",
       "      <td>8/26/2014</td>\n",
       "      <td>...</td>\n",
       "      <td>8/26/2014</td>\n",
       "      <td>10/20/2020</td>\n",
       "      <td>6.151951</td>\n",
       "      <td>6</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.151951</td>\n",
       "      <td>-0.72</td>\n",
       "      <td>0.60</td>\n",
       "      <td>98</td>\n",
       "      <td>99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>5/20/1943</td>\n",
       "      <td>LE</td>\n",
       "      <td>238460.0</td>\n",
       "      <td>OCT Circle Scan</td>\n",
       "      <td>3.5</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0</td>\n",
       "      <td>7/9/2014</td>\n",
       "      <td>...</td>\n",
       "      <td>6/30/2014</td>\n",
       "      <td>10/14/2020</td>\n",
       "      <td>6.291581</td>\n",
       "      <td>10</td>\n",
       "      <td>0.024641</td>\n",
       "      <td>6.266940</td>\n",
       "      <td>-1.69</td>\n",
       "      <td>-0.51</td>\n",
       "      <td>99</td>\n",
       "      <td>99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>579</th>\n",
       "      <td>580</td>\n",
       "      <td>329</td>\n",
       "      <td>3/22/1952</td>\n",
       "      <td>RE</td>\n",
       "      <td>837.0</td>\n",
       "      <td>OCT Circle Scan</td>\n",
       "      <td>3.7</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0</td>\n",
       "      <td>5/5/2011</td>\n",
       "      <td>...</td>\n",
       "      <td>5/5/2011</td>\n",
       "      <td>12/10/2020</td>\n",
       "      <td>9.601643</td>\n",
       "      <td>17</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.601643</td>\n",
       "      <td>0.53</td>\n",
       "      <td>-2.51</td>\n",
       "      <td>98</td>\n",
       "      <td>93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>580</th>\n",
       "      <td>581</td>\n",
       "      <td>330</td>\n",
       "      <td>5/15/1945</td>\n",
       "      <td>LE</td>\n",
       "      <td>243095.0</td>\n",
       "      <td>OCT Circle Scan</td>\n",
       "      <td>3.5</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0</td>\n",
       "      <td>12/17/2014</td>\n",
       "      <td>...</td>\n",
       "      <td>12/17/2014</td>\n",
       "      <td>11/25/2020</td>\n",
       "      <td>5.941136</td>\n",
       "      <td>10</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.941136</td>\n",
       "      <td>-8.97</td>\n",
       "      <td>-14.71</td>\n",
       "      <td>78</td>\n",
       "      <td>56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>581</th>\n",
       "      <td>582</td>\n",
       "      <td>330</td>\n",
       "      <td>5/15/1945</td>\n",
       "      <td>RE</td>\n",
       "      <td>243093.0</td>\n",
       "      <td>OCT Circle Scan</td>\n",
       "      <td>3.7</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0</td>\n",
       "      <td>12/17/2014</td>\n",
       "      <td>...</td>\n",
       "      <td>12/17/2014</td>\n",
       "      <td>11/25/2020</td>\n",
       "      <td>5.941136</td>\n",
       "      <td>9</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.941136</td>\n",
       "      <td>-11.39</td>\n",
       "      <td>-11.37</td>\n",
       "      <td>70</td>\n",
       "      <td>67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>582</th>\n",
       "      <td>583</td>\n",
       "      <td>331</td>\n",
       "      <td>5/31/1939</td>\n",
       "      <td>LE</td>\n",
       "      <td>109347.0</td>\n",
       "      <td>OCT Circle Scan</td>\n",
       "      <td>3.5</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0</td>\n",
       "      <td>8/13/2013</td>\n",
       "      <td>...</td>\n",
       "      <td>6/11/2013</td>\n",
       "      <td>10/23/2019</td>\n",
       "      <td>6.365503</td>\n",
       "      <td>11</td>\n",
       "      <td>0.172485</td>\n",
       "      <td>6.193018</td>\n",
       "      <td>-3.48</td>\n",
       "      <td>-19.28</td>\n",
       "      <td>97</td>\n",
       "      <td>51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>583</th>\n",
       "      <td>584</td>\n",
       "      <td>331</td>\n",
       "      <td>5/31/1939</td>\n",
       "      <td>RE</td>\n",
       "      <td>109343.2</td>\n",
       "      <td>OCT Circle Scan</td>\n",
       "      <td>3.5</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0</td>\n",
       "      <td>8/13/2013</td>\n",
       "      <td>...</td>\n",
       "      <td>6/11/2013</td>\n",
       "      <td>10/23/2019</td>\n",
       "      <td>6.365503</td>\n",
       "      <td>12</td>\n",
       "      <td>0.172485</td>\n",
       "      <td>6.193018</td>\n",
       "      <td>-3.34</td>\n",
       "      <td>-16.15</td>\n",
       "      <td>93</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>584 rows × 811 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     EID  PID         DOB Eye   ImageID        Scan.Type  Diameter..mm.  \\\n",
       "0      1    1  10/24/1949  LE  282596.0  OCT Circle Scan            3.7   \n",
       "1      2    1  10/24/1949  RE  282593.0  OCT Circle Scan            3.7   \n",
       "2      3    2    8/7/1939  LE  239514.0  OCT Circle Scan            3.4   \n",
       "3      4    2    8/7/1939  RE  239512.0  OCT Circle Scan            3.4   \n",
       "4      5    3   5/20/1943  LE  238460.0  OCT Circle Scan            3.5   \n",
       "..   ...  ...         ...  ..       ...              ...            ...   \n",
       "579  580  329   3/22/1952  RE     837.0  OCT Circle Scan            3.7   \n",
       "580  581  330   5/15/1945  LE  243095.0  OCT Circle Scan            3.5   \n",
       "581  582  330   5/15/1945  RE  243093.0  OCT Circle Scan            3.7   \n",
       "582  583  331   5/31/1939  LE  109347.0  OCT Circle Scan            3.5   \n",
       "583  584  331   5/31/1939  RE  109343.2  OCT Circle Scan            3.5   \n",
       "\n",
       "     Diameter....  Fixed.in.mm    ExamDate  ... VF_EXAM_START VF_EXAM_END  \\\n",
       "0            12.0            0   5/11/2017  ...     5/11/2017   12/2/2020   \n",
       "1            12.0            0   5/11/2017  ...     5/11/2017   12/2/2020   \n",
       "2            12.0            0   8/26/2014  ...     8/26/2014  10/20/2020   \n",
       "3            12.0            0   8/26/2014  ...     8/26/2014  10/20/2020   \n",
       "4            12.0            0    7/9/2014  ...     6/30/2014  10/14/2020   \n",
       "..            ...          ...         ...  ...           ...         ...   \n",
       "579          12.0            0    5/5/2011  ...      5/5/2011  12/10/2020   \n",
       "580          12.0            0  12/17/2014  ...    12/17/2014  11/25/2020   \n",
       "581          12.0            0  12/17/2014  ...    12/17/2014  11/25/2020   \n",
       "582          12.0            0   8/13/2013  ...     6/11/2013  10/23/2019   \n",
       "583          12.0            0   8/13/2013  ...     6/11/2013  10/23/2019   \n",
       "\n",
       "     VF_FOLLOW_UP  VF_N  VF_OCT_BASELINE_DIFF  VF_OCT_FINAL_DIFF  MD_BASELINE  \\\n",
       "0        3.561944     6              0.000000           3.561944        -2.15   \n",
       "1        3.561944     6              0.000000           3.561944        -7.73   \n",
       "2        6.151951     6              0.000000           6.151951        -1.28   \n",
       "3        6.151951     6              0.000000           6.151951        -0.72   \n",
       "4        6.291581    10              0.024641           6.266940        -1.69   \n",
       "..            ...   ...                   ...                ...          ...   \n",
       "579      9.601643    17              0.000000           9.601643         0.53   \n",
       "580      5.941136    10              0.000000           5.941136        -8.97   \n",
       "581      5.941136     9              0.000000           5.941136       -11.39   \n",
       "582      6.365503    11              0.172485           6.193018        -3.48   \n",
       "583      6.365503    12              0.172485           6.193018        -3.34   \n",
       "\n",
       "     MD_FINAL  VFI_BASELINE  VFI_FINAL  \n",
       "0       -3.26            98         96  \n",
       "1      -11.45            82         73  \n",
       "2       -1.13            98         97  \n",
       "3        0.60            98         99  \n",
       "4       -0.51            99         99  \n",
       "..        ...           ...        ...  \n",
       "579     -2.51            98         93  \n",
       "580    -14.71            78         56  \n",
       "581    -11.37            70         67  \n",
       "582    -19.28            97         51  \n",
       "583    -16.15            93         47  \n",
       "\n",
       "[584 rows x 811 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load the data\n",
    "df = pd.read_csv(\"/Users/a123456/Desktop/Fei's Project/Data/OCT_BASELINE_GRI__VF_6-3_FP-15_NO_PHI.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "22cd641a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(580, 811)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#filter only circle scan data\n",
    "circle_scan = (df['Scan.Type'] == 'OCT Circle Scan')\n",
    "df = df[circle_scan]\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a37b74f",
   "metadata": {},
   "source": [
    "Replace NA values in GRI with 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b5c39649",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['EID',\n",
       " 'PID',\n",
       " 'DOB',\n",
       " 'Eye',\n",
       " 'ImageID',\n",
       " 'Scan.Type',\n",
       " 'Diameter..mm.',\n",
       " 'Diameter....',\n",
       " 'Fixed.in.mm',\n",
       " 'ExamDate',\n",
       " 'ExamTime',\n",
       " 'AQMVersion',\n",
       " 'Quality',\n",
       " 'ARTMean',\n",
       " 'RNFLMean_G',\n",
       " 'RNFLMean_T',\n",
       " 'RNFLMean_TS',\n",
       " 'RNFLMean_TI',\n",
       " 'RNFLMean_N',\n",
       " 'RNFLMean_NS',\n",
       " 'RNFLMean_NI',\n",
       " 'RNFLClass_G',\n",
       " 'RNFLClass_T',\n",
       " 'RNFLClass_TS',\n",
       " 'RNFLClass_TI',\n",
       " 'RNFLClass_N',\n",
       " 'RNFLClass_NS',\n",
       " 'RNFLClass_NI',\n",
       " 'RNFLT.1',\n",
       " 'RNFLT.2',\n",
       " 'RNFLT.3',\n",
       " 'RNFLT.4',\n",
       " 'RNFLT.5',\n",
       " 'RNFLT.6',\n",
       " 'RNFLT.7',\n",
       " 'RNFLT.8',\n",
       " 'RNFLT.9',\n",
       " 'RNFLT.10',\n",
       " 'RNFLT.11',\n",
       " 'RNFLT.12',\n",
       " 'RNFLT.13',\n",
       " 'RNFLT.14',\n",
       " 'RNFLT.15',\n",
       " 'RNFLT.16',\n",
       " 'RNFLT.17',\n",
       " 'RNFLT.18',\n",
       " 'RNFLT.19',\n",
       " 'RNFLT.20',\n",
       " 'RNFLT.21',\n",
       " 'RNFLT.22',\n",
       " 'RNFLT.23',\n",
       " 'RNFLT.24',\n",
       " 'RNFLT.25',\n",
       " 'RNFLT.26',\n",
       " 'RNFLT.27',\n",
       " 'RNFLT.28',\n",
       " 'RNFLT.29',\n",
       " 'RNFLT.30',\n",
       " 'RNFLT.31',\n",
       " 'RNFLT.32',\n",
       " 'RNFLT.33',\n",
       " 'RNFLT.34',\n",
       " 'RNFLT.35',\n",
       " 'RNFLT.36',\n",
       " 'RNFLT.37',\n",
       " 'RNFLT.38',\n",
       " 'RNFLT.39',\n",
       " 'RNFLT.40',\n",
       " 'RNFLT.41',\n",
       " 'RNFLT.42',\n",
       " 'RNFLT.43',\n",
       " 'RNFLT.44',\n",
       " 'RNFLT.45',\n",
       " 'RNFLT.46',\n",
       " 'RNFLT.47',\n",
       " 'RNFLT.48',\n",
       " 'RNFLT.49',\n",
       " 'RNFLT.50',\n",
       " 'RNFLT.51',\n",
       " 'RNFLT.52',\n",
       " 'RNFLT.53',\n",
       " 'RNFLT.54',\n",
       " 'RNFLT.55',\n",
       " 'RNFLT.56',\n",
       " 'RNFLT.57',\n",
       " 'RNFLT.58',\n",
       " 'RNFLT.59',\n",
       " 'RNFLT.60',\n",
       " 'RNFLT.61',\n",
       " 'RNFLT.62',\n",
       " 'RNFLT.63',\n",
       " 'RNFLT.64',\n",
       " 'RNFLT.65',\n",
       " 'RNFLT.66',\n",
       " 'RNFLT.67',\n",
       " 'RNFLT.68',\n",
       " 'RNFLT.69',\n",
       " 'RNFLT.70',\n",
       " 'RNFLT.71',\n",
       " 'RNFLT.72',\n",
       " 'RNFLT.73',\n",
       " 'RNFLT.74',\n",
       " 'RNFLT.75',\n",
       " 'RNFLT.76',\n",
       " 'RNFLT.77',\n",
       " 'RNFLT.78',\n",
       " 'RNFLT.79',\n",
       " 'RNFLT.80',\n",
       " 'RNFLT.81',\n",
       " 'RNFLT.82',\n",
       " 'RNFLT.83',\n",
       " 'RNFLT.84',\n",
       " 'RNFLT.85',\n",
       " 'RNFLT.86',\n",
       " 'RNFLT.87',\n",
       " 'RNFLT.88',\n",
       " 'RNFLT.89',\n",
       " 'RNFLT.90',\n",
       " 'RNFLT.91',\n",
       " 'RNFLT.92',\n",
       " 'RNFLT.93',\n",
       " 'RNFLT.94',\n",
       " 'RNFLT.95',\n",
       " 'RNFLT.96',\n",
       " 'RNFLT.97',\n",
       " 'RNFLT.98',\n",
       " 'RNFLT.99',\n",
       " 'RNFLT.100',\n",
       " 'RNFLT.101',\n",
       " 'RNFLT.102',\n",
       " 'RNFLT.103',\n",
       " 'RNFLT.104',\n",
       " 'RNFLT.105',\n",
       " 'RNFLT.106',\n",
       " 'RNFLT.107',\n",
       " 'RNFLT.108',\n",
       " 'RNFLT.109',\n",
       " 'RNFLT.110',\n",
       " 'RNFLT.111',\n",
       " 'RNFLT.112',\n",
       " 'RNFLT.113',\n",
       " 'RNFLT.114',\n",
       " 'RNFLT.115',\n",
       " 'RNFLT.116',\n",
       " 'RNFLT.117',\n",
       " 'RNFLT.118',\n",
       " 'RNFLT.119',\n",
       " 'RNFLT.120',\n",
       " 'RNFLT.121',\n",
       " 'RNFLT.122',\n",
       " 'RNFLT.123',\n",
       " 'RNFLT.124',\n",
       " 'RNFLT.125',\n",
       " 'RNFLT.126',\n",
       " 'RNFLT.127',\n",
       " 'RNFLT.128',\n",
       " 'RNFLT.129',\n",
       " 'RNFLT.130',\n",
       " 'RNFLT.131',\n",
       " 'RNFLT.132',\n",
       " 'RNFLT.133',\n",
       " 'RNFLT.134',\n",
       " 'RNFLT.135',\n",
       " 'RNFLT.136',\n",
       " 'RNFLT.137',\n",
       " 'RNFLT.138',\n",
       " 'RNFLT.139',\n",
       " 'RNFLT.140',\n",
       " 'RNFLT.141',\n",
       " 'RNFLT.142',\n",
       " 'RNFLT.143',\n",
       " 'RNFLT.144',\n",
       " 'RNFLT.145',\n",
       " 'RNFLT.146',\n",
       " 'RNFLT.147',\n",
       " 'RNFLT.148',\n",
       " 'RNFLT.149',\n",
       " 'RNFLT.150',\n",
       " 'RNFLT.151',\n",
       " 'RNFLT.152',\n",
       " 'RNFLT.153',\n",
       " 'RNFLT.154',\n",
       " 'RNFLT.155',\n",
       " 'RNFLT.156',\n",
       " 'RNFLT.157',\n",
       " 'RNFLT.158',\n",
       " 'RNFLT.159',\n",
       " 'RNFLT.160',\n",
       " 'RNFLT.161',\n",
       " 'RNFLT.162',\n",
       " 'RNFLT.163',\n",
       " 'RNFLT.164',\n",
       " 'RNFLT.165',\n",
       " 'RNFLT.166',\n",
       " 'RNFLT.167',\n",
       " 'RNFLT.168',\n",
       " 'RNFLT.169',\n",
       " 'RNFLT.170',\n",
       " 'RNFLT.171',\n",
       " 'RNFLT.172',\n",
       " 'RNFLT.173',\n",
       " 'RNFLT.174',\n",
       " 'RNFLT.175',\n",
       " 'RNFLT.176',\n",
       " 'RNFLT.177',\n",
       " 'RNFLT.178',\n",
       " 'RNFLT.179',\n",
       " 'RNFLT.180',\n",
       " 'RNFLT.181',\n",
       " 'RNFLT.182',\n",
       " 'RNFLT.183',\n",
       " 'RNFLT.184',\n",
       " 'RNFLT.185',\n",
       " 'RNFLT.186',\n",
       " 'RNFLT.187',\n",
       " 'RNFLT.188',\n",
       " 'RNFLT.189',\n",
       " 'RNFLT.190',\n",
       " 'RNFLT.191',\n",
       " 'RNFLT.192',\n",
       " 'RNFLT.193',\n",
       " 'RNFLT.194',\n",
       " 'RNFLT.195',\n",
       " 'RNFLT.196',\n",
       " 'RNFLT.197',\n",
       " 'RNFLT.198',\n",
       " 'RNFLT.199',\n",
       " 'RNFLT.200',\n",
       " 'RNFLT.201',\n",
       " 'RNFLT.202',\n",
       " 'RNFLT.203',\n",
       " 'RNFLT.204',\n",
       " 'RNFLT.205',\n",
       " 'RNFLT.206',\n",
       " 'RNFLT.207',\n",
       " 'RNFLT.208',\n",
       " 'RNFLT.209',\n",
       " 'RNFLT.210',\n",
       " 'RNFLT.211',\n",
       " 'RNFLT.212',\n",
       " 'RNFLT.213',\n",
       " 'RNFLT.214',\n",
       " 'RNFLT.215',\n",
       " 'RNFLT.216',\n",
       " 'RNFLT.217',\n",
       " 'RNFLT.218',\n",
       " 'RNFLT.219',\n",
       " 'RNFLT.220',\n",
       " 'RNFLT.221',\n",
       " 'RNFLT.222',\n",
       " 'RNFLT.223',\n",
       " 'RNFLT.224',\n",
       " 'RNFLT.225',\n",
       " 'RNFLT.226',\n",
       " 'RNFLT.227',\n",
       " 'RNFLT.228',\n",
       " 'RNFLT.229',\n",
       " 'RNFLT.230',\n",
       " 'RNFLT.231',\n",
       " 'RNFLT.232',\n",
       " 'RNFLT.233',\n",
       " 'RNFLT.234',\n",
       " 'RNFLT.235',\n",
       " 'RNFLT.236',\n",
       " 'RNFLT.237',\n",
       " 'RNFLT.238',\n",
       " 'RNFLT.239',\n",
       " 'RNFLT.240',\n",
       " 'RNFLT.241',\n",
       " 'RNFLT.242',\n",
       " 'RNFLT.243',\n",
       " 'RNFLT.244',\n",
       " 'RNFLT.245',\n",
       " 'RNFLT.246',\n",
       " 'RNFLT.247',\n",
       " 'RNFLT.248',\n",
       " 'RNFLT.249',\n",
       " 'RNFLT.250',\n",
       " 'RNFLT.251',\n",
       " 'RNFLT.252',\n",
       " 'RNFLT.253',\n",
       " 'RNFLT.254',\n",
       " 'RNFLT.255',\n",
       " 'RNFLT.256',\n",
       " 'RNFLT.257',\n",
       " 'RNFLT.258',\n",
       " 'RNFLT.259',\n",
       " 'RNFLT.260',\n",
       " 'RNFLT.261',\n",
       " 'RNFLT.262',\n",
       " 'RNFLT.263',\n",
       " 'RNFLT.264',\n",
       " 'RNFLT.265',\n",
       " 'RNFLT.266',\n",
       " 'RNFLT.267',\n",
       " 'RNFLT.268',\n",
       " 'RNFLT.269',\n",
       " 'RNFLT.270',\n",
       " 'RNFLT.271',\n",
       " 'RNFLT.272',\n",
       " 'RNFLT.273',\n",
       " 'RNFLT.274',\n",
       " 'RNFLT.275',\n",
       " 'RNFLT.276',\n",
       " 'RNFLT.277',\n",
       " 'RNFLT.278',\n",
       " 'RNFLT.279',\n",
       " 'RNFLT.280',\n",
       " 'RNFLT.281',\n",
       " 'RNFLT.282',\n",
       " 'RNFLT.283',\n",
       " 'RNFLT.284',\n",
       " 'RNFLT.285',\n",
       " 'RNFLT.286',\n",
       " 'RNFLT.287',\n",
       " 'RNFLT.288',\n",
       " 'RNFLT.289',\n",
       " 'RNFLT.290',\n",
       " 'RNFLT.291',\n",
       " 'RNFLT.292',\n",
       " 'RNFLT.293',\n",
       " 'RNFLT.294',\n",
       " 'RNFLT.295',\n",
       " 'RNFLT.296',\n",
       " 'RNFLT.297',\n",
       " 'RNFLT.298',\n",
       " 'RNFLT.299',\n",
       " 'RNFLT.300',\n",
       " 'RNFLT.301',\n",
       " 'RNFLT.302',\n",
       " 'RNFLT.303',\n",
       " 'RNFLT.304',\n",
       " 'RNFLT.305',\n",
       " 'RNFLT.306',\n",
       " 'RNFLT.307',\n",
       " 'RNFLT.308',\n",
       " 'RNFLT.309',\n",
       " 'RNFLT.310',\n",
       " 'RNFLT.311',\n",
       " 'RNFLT.312',\n",
       " 'RNFLT.313',\n",
       " 'RNFLT.314',\n",
       " 'RNFLT.315',\n",
       " 'RNFLT.316',\n",
       " 'RNFLT.317',\n",
       " 'RNFLT.318',\n",
       " 'RNFLT.319',\n",
       " 'RNFLT.320',\n",
       " 'RNFLT.321',\n",
       " 'RNFLT.322',\n",
       " 'RNFLT.323',\n",
       " 'RNFLT.324',\n",
       " 'RNFLT.325',\n",
       " 'RNFLT.326',\n",
       " 'RNFLT.327',\n",
       " 'RNFLT.328',\n",
       " 'RNFLT.329',\n",
       " 'RNFLT.330',\n",
       " 'RNFLT.331',\n",
       " 'RNFLT.332',\n",
       " 'RNFLT.333',\n",
       " 'RNFLT.334',\n",
       " 'RNFLT.335',\n",
       " 'RNFLT.336',\n",
       " 'RNFLT.337',\n",
       " 'RNFLT.338',\n",
       " 'RNFLT.339',\n",
       " 'RNFLT.340',\n",
       " 'RNFLT.341',\n",
       " 'RNFLT.342',\n",
       " 'RNFLT.343',\n",
       " 'RNFLT.344',\n",
       " 'RNFLT.345',\n",
       " 'RNFLT.346',\n",
       " 'RNFLT.347',\n",
       " 'RNFLT.348',\n",
       " 'RNFLT.349',\n",
       " 'RNFLT.350',\n",
       " 'RNFLT.351',\n",
       " 'RNFLT.352',\n",
       " 'RNFLT.353',\n",
       " 'RNFLT.354',\n",
       " 'RNFLT.355',\n",
       " 'RNFLT.356',\n",
       " 'RNFLT.357',\n",
       " 'RNFLT.358',\n",
       " 'RNFLT.359',\n",
       " 'RNFLT.360',\n",
       " 'RNFLT.361',\n",
       " 'RNFLT.362',\n",
       " 'RNFLT.363',\n",
       " 'RNFLT.364',\n",
       " 'RNFLT.365',\n",
       " 'RNFLT.366',\n",
       " 'RNFLT.367',\n",
       " 'RNFLT.368',\n",
       " 'RNFLT.369',\n",
       " 'RNFLT.370',\n",
       " 'RNFLT.371',\n",
       " 'RNFLT.372',\n",
       " 'RNFLT.373',\n",
       " 'RNFLT.374',\n",
       " 'RNFLT.375',\n",
       " 'RNFLT.376',\n",
       " 'RNFLT.377',\n",
       " 'RNFLT.378',\n",
       " 'RNFLT.379',\n",
       " 'RNFLT.380',\n",
       " 'RNFLT.381',\n",
       " 'RNFLT.382',\n",
       " 'RNFLT.383',\n",
       " 'RNFLT.384',\n",
       " 'RNFLT.385',\n",
       " 'RNFLT.386',\n",
       " 'RNFLT.387',\n",
       " 'RNFLT.388',\n",
       " 'RNFLT.389',\n",
       " 'RNFLT.390',\n",
       " 'RNFLT.391',\n",
       " 'RNFLT.392',\n",
       " 'RNFLT.393',\n",
       " 'RNFLT.394',\n",
       " 'RNFLT.395',\n",
       " 'RNFLT.396',\n",
       " 'RNFLT.397',\n",
       " 'RNFLT.398',\n",
       " 'RNFLT.399',\n",
       " 'RNFLT.400',\n",
       " 'RNFLT.401',\n",
       " 'RNFLT.402',\n",
       " 'RNFLT.403',\n",
       " 'RNFLT.404',\n",
       " 'RNFLT.405',\n",
       " 'RNFLT.406',\n",
       " 'RNFLT.407',\n",
       " 'RNFLT.408',\n",
       " 'RNFLT.409',\n",
       " 'RNFLT.410',\n",
       " 'RNFLT.411',\n",
       " 'RNFLT.412',\n",
       " 'RNFLT.413',\n",
       " 'RNFLT.414',\n",
       " 'RNFLT.415',\n",
       " 'RNFLT.416',\n",
       " 'RNFLT.417',\n",
       " 'RNFLT.418',\n",
       " 'RNFLT.419',\n",
       " 'RNFLT.420',\n",
       " 'RNFLT.421',\n",
       " 'RNFLT.422',\n",
       " 'RNFLT.423',\n",
       " 'RNFLT.424',\n",
       " 'RNFLT.425',\n",
       " 'RNFLT.426',\n",
       " 'RNFLT.427',\n",
       " 'RNFLT.428',\n",
       " 'RNFLT.429',\n",
       " 'RNFLT.430',\n",
       " 'RNFLT.431',\n",
       " 'RNFLT.432',\n",
       " 'RNFLT.433',\n",
       " 'RNFLT.434',\n",
       " 'RNFLT.435',\n",
       " 'RNFLT.436',\n",
       " 'RNFLT.437',\n",
       " 'RNFLT.438',\n",
       " 'RNFLT.439',\n",
       " 'RNFLT.440',\n",
       " 'RNFLT.441',\n",
       " 'RNFLT.442',\n",
       " 'RNFLT.443',\n",
       " 'RNFLT.444',\n",
       " 'RNFLT.445',\n",
       " 'RNFLT.446',\n",
       " 'RNFLT.447',\n",
       " 'RNFLT.448',\n",
       " 'RNFLT.449',\n",
       " 'RNFLT.450',\n",
       " 'RNFLT.451',\n",
       " 'RNFLT.452',\n",
       " 'RNFLT.453',\n",
       " 'RNFLT.454',\n",
       " 'RNFLT.455',\n",
       " 'RNFLT.456',\n",
       " 'RNFLT.457',\n",
       " 'RNFLT.458',\n",
       " 'RNFLT.459',\n",
       " 'RNFLT.460',\n",
       " 'RNFLT.461',\n",
       " 'RNFLT.462',\n",
       " 'RNFLT.463',\n",
       " 'RNFLT.464',\n",
       " 'RNFLT.465',\n",
       " 'RNFLT.466',\n",
       " 'RNFLT.467',\n",
       " 'RNFLT.468',\n",
       " 'RNFLT.469',\n",
       " 'RNFLT.470',\n",
       " 'RNFLT.471',\n",
       " 'RNFLT.472',\n",
       " 'RNFLT.473',\n",
       " 'RNFLT.474',\n",
       " 'RNFLT.475',\n",
       " 'RNFLT.476',\n",
       " 'RNFLT.477',\n",
       " 'RNFLT.478',\n",
       " 'RNFLT.479',\n",
       " 'RNFLT.480',\n",
       " 'RNFLT.481',\n",
       " 'RNFLT.482',\n",
       " 'RNFLT.483',\n",
       " 'RNFLT.484',\n",
       " 'RNFLT.485',\n",
       " 'RNFLT.486',\n",
       " 'RNFLT.487',\n",
       " 'RNFLT.488',\n",
       " 'RNFLT.489',\n",
       " 'RNFLT.490',\n",
       " 'RNFLT.491',\n",
       " 'RNFLT.492',\n",
       " 'RNFLT.493',\n",
       " 'RNFLT.494',\n",
       " 'RNFLT.495',\n",
       " 'RNFLT.496',\n",
       " 'RNFLT.497',\n",
       " 'RNFLT.498',\n",
       " 'RNFLT.499',\n",
       " 'RNFLT.500',\n",
       " 'RNFLT.501',\n",
       " 'RNFLT.502',\n",
       " 'RNFLT.503',\n",
       " 'RNFLT.504',\n",
       " 'RNFLT.505',\n",
       " 'RNFLT.506',\n",
       " 'RNFLT.507',\n",
       " 'RNFLT.508',\n",
       " 'RNFLT.509',\n",
       " 'RNFLT.510',\n",
       " 'RNFLT.511',\n",
       " 'RNFLT.512',\n",
       " 'RNFLT.513',\n",
       " 'RNFLT.514',\n",
       " 'RNFLT.515',\n",
       " 'RNFLT.516',\n",
       " 'RNFLT.517',\n",
       " 'RNFLT.518',\n",
       " 'RNFLT.519',\n",
       " 'RNFLT.520',\n",
       " 'RNFLT.521',\n",
       " 'RNFLT.522',\n",
       " 'RNFLT.523',\n",
       " 'RNFLT.524',\n",
       " 'RNFLT.525',\n",
       " 'RNFLT.526',\n",
       " 'RNFLT.527',\n",
       " 'RNFLT.528',\n",
       " 'RNFLT.529',\n",
       " 'RNFLT.530',\n",
       " 'RNFLT.531',\n",
       " 'RNFLT.532',\n",
       " 'RNFLT.533',\n",
       " 'RNFLT.534',\n",
       " 'RNFLT.535',\n",
       " 'RNFLT.536',\n",
       " 'RNFLT.537',\n",
       " 'RNFLT.538',\n",
       " 'RNFLT.539',\n",
       " 'RNFLT.540',\n",
       " 'RNFLT.541',\n",
       " 'RNFLT.542',\n",
       " 'RNFLT.543',\n",
       " 'RNFLT.544',\n",
       " 'RNFLT.545',\n",
       " 'RNFLT.546',\n",
       " 'RNFLT.547',\n",
       " 'RNFLT.548',\n",
       " 'RNFLT.549',\n",
       " 'RNFLT.550',\n",
       " 'RNFLT.551',\n",
       " 'RNFLT.552',\n",
       " 'RNFLT.553',\n",
       " 'RNFLT.554',\n",
       " 'RNFLT.555',\n",
       " 'RNFLT.556',\n",
       " 'RNFLT.557',\n",
       " 'RNFLT.558',\n",
       " 'RNFLT.559',\n",
       " 'RNFLT.560',\n",
       " 'RNFLT.561',\n",
       " 'RNFLT.562',\n",
       " 'RNFLT.563',\n",
       " 'RNFLT.564',\n",
       " 'RNFLT.565',\n",
       " 'RNFLT.566',\n",
       " 'RNFLT.567',\n",
       " 'RNFLT.568',\n",
       " 'RNFLT.569',\n",
       " 'RNFLT.570',\n",
       " 'RNFLT.571',\n",
       " 'RNFLT.572',\n",
       " 'RNFLT.573',\n",
       " 'RNFLT.574',\n",
       " 'RNFLT.575',\n",
       " 'RNFLT.576',\n",
       " 'RNFLT.577',\n",
       " 'RNFLT.578',\n",
       " 'RNFLT.579',\n",
       " 'RNFLT.580',\n",
       " 'RNFLT.581',\n",
       " 'RNFLT.582',\n",
       " 'RNFLT.583',\n",
       " 'RNFLT.584',\n",
       " 'RNFLT.585',\n",
       " 'RNFLT.586',\n",
       " 'RNFLT.587',\n",
       " 'RNFLT.588',\n",
       " 'RNFLT.589',\n",
       " 'RNFLT.590',\n",
       " 'RNFLT.591',\n",
       " 'RNFLT.592',\n",
       " 'RNFLT.593',\n",
       " 'RNFLT.594',\n",
       " 'RNFLT.595',\n",
       " 'RNFLT.596',\n",
       " 'RNFLT.597',\n",
       " 'RNFLT.598',\n",
       " 'RNFLT.599',\n",
       " 'RNFLT.600',\n",
       " 'RNFLT.601',\n",
       " 'RNFLT.602',\n",
       " 'RNFLT.603',\n",
       " 'RNFLT.604',\n",
       " 'RNFLT.605',\n",
       " 'RNFLT.606',\n",
       " 'RNFLT.607',\n",
       " 'RNFLT.608',\n",
       " 'RNFLT.609',\n",
       " 'RNFLT.610',\n",
       " 'RNFLT.611',\n",
       " 'RNFLT.612',\n",
       " 'RNFLT.613',\n",
       " 'RNFLT.614',\n",
       " 'RNFLT.615',\n",
       " 'RNFLT.616',\n",
       " 'RNFLT.617',\n",
       " 'RNFLT.618',\n",
       " 'RNFLT.619',\n",
       " 'RNFLT.620',\n",
       " 'RNFLT.621',\n",
       " 'RNFLT.622',\n",
       " 'RNFLT.623',\n",
       " 'RNFLT.624',\n",
       " 'RNFLT.625',\n",
       " 'RNFLT.626',\n",
       " 'RNFLT.627',\n",
       " 'RNFLT.628',\n",
       " 'RNFLT.629',\n",
       " 'RNFLT.630',\n",
       " 'RNFLT.631',\n",
       " 'RNFLT.632',\n",
       " 'RNFLT.633',\n",
       " 'RNFLT.634',\n",
       " 'RNFLT.635',\n",
       " 'RNFLT.636',\n",
       " 'RNFLT.637',\n",
       " 'RNFLT.638',\n",
       " 'RNFLT.639',\n",
       " 'RNFLT.640',\n",
       " 'RNFLT.641',\n",
       " 'RNFLT.642',\n",
       " 'RNFLT.643',\n",
       " 'RNFLT.644',\n",
       " 'RNFLT.645',\n",
       " 'RNFLT.646',\n",
       " 'RNFLT.647',\n",
       " 'RNFLT.648',\n",
       " 'RNFLT.649',\n",
       " 'RNFLT.650',\n",
       " 'RNFLT.651',\n",
       " 'RNFLT.652',\n",
       " 'RNFLT.653',\n",
       " 'RNFLT.654',\n",
       " 'RNFLT.655',\n",
       " 'RNFLT.656',\n",
       " 'RNFLT.657',\n",
       " 'RNFLT.658',\n",
       " 'RNFLT.659',\n",
       " 'RNFLT.660',\n",
       " 'RNFLT.661',\n",
       " 'RNFLT.662',\n",
       " 'RNFLT.663',\n",
       " 'RNFLT.664',\n",
       " 'RNFLT.665',\n",
       " 'RNFLT.666',\n",
       " 'RNFLT.667',\n",
       " 'RNFLT.668',\n",
       " 'RNFLT.669',\n",
       " 'RNFLT.670',\n",
       " 'RNFLT.671',\n",
       " 'RNFLT.672',\n",
       " 'RNFLT.673',\n",
       " 'RNFLT.674',\n",
       " 'RNFLT.675',\n",
       " 'RNFLT.676',\n",
       " 'RNFLT.677',\n",
       " 'RNFLT.678',\n",
       " 'RNFLT.679',\n",
       " 'RNFLT.680',\n",
       " 'RNFLT.681',\n",
       " 'RNFLT.682',\n",
       " 'RNFLT.683',\n",
       " 'RNFLT.684',\n",
       " 'RNFLT.685',\n",
       " 'RNFLT.686',\n",
       " 'RNFLT.687',\n",
       " 'RNFLT.688',\n",
       " 'RNFLT.689',\n",
       " 'RNFLT.690',\n",
       " 'RNFLT.691',\n",
       " 'RNFLT.692',\n",
       " 'RNFLT.693',\n",
       " 'RNFLT.694',\n",
       " 'RNFLT.695',\n",
       " 'RNFLT.696',\n",
       " 'RNFLT.697',\n",
       " 'RNFLT.698',\n",
       " 'RNFLT.699',\n",
       " 'RNFLT.700',\n",
       " 'RNFLT.701',\n",
       " 'RNFLT.702',\n",
       " 'RNFLT.703',\n",
       " 'RNFLT.704',\n",
       " 'RNFLT.705',\n",
       " 'RNFLT.706',\n",
       " 'RNFLT.707',\n",
       " 'RNFLT.708',\n",
       " 'RNFLT.709',\n",
       " 'RNFLT.710',\n",
       " 'RNFLT.711',\n",
       " 'RNFLT.712',\n",
       " 'RNFLT.713',\n",
       " 'RNFLT.714',\n",
       " 'RNFLT.715',\n",
       " 'RNFLT.716',\n",
       " 'RNFLT.717',\n",
       " 'RNFLT.718',\n",
       " 'RNFLT.719',\n",
       " 'RNFLT.720',\n",
       " 'RNFLT.721',\n",
       " 'RNFLT.722',\n",
       " 'RNFLT.723',\n",
       " 'RNFLT.724',\n",
       " 'RNFLT.725',\n",
       " 'RNFLT.726',\n",
       " 'RNFLT.727',\n",
       " 'RNFLT.728',\n",
       " 'RNFLT.729',\n",
       " 'RNFLT.730',\n",
       " 'RNFLT.731',\n",
       " 'RNFLT.732',\n",
       " 'RNFLT.733',\n",
       " 'RNFLT.734',\n",
       " 'RNFLT.735',\n",
       " 'RNFLT.736',\n",
       " 'RNFLT.737',\n",
       " 'RNFLT.738',\n",
       " 'RNFLT.739',\n",
       " 'RNFLT.740',\n",
       " 'RNFLT.741',\n",
       " 'RNFLT.742',\n",
       " 'RNFLT.743',\n",
       " 'RNFLT.744',\n",
       " 'RNFLT.745',\n",
       " 'RNFLT.746',\n",
       " 'RNFLT.747',\n",
       " 'RNFLT.748',\n",
       " 'RNFLT.749',\n",
       " 'RNFLT.750',\n",
       " 'RNFLT.751',\n",
       " 'RNFLT.752',\n",
       " 'RNFLT.753',\n",
       " 'RNFLT.754',\n",
       " 'RNFLT.755',\n",
       " 'RNFLT.756',\n",
       " 'RNFLT.757',\n",
       " 'RNFLT.758',\n",
       " 'RNFLT.759',\n",
       " 'RNFLT.760',\n",
       " 'RNFLT.761',\n",
       " 'RNFLT.762',\n",
       " 'RNFLT.763',\n",
       " 'RNFLT.764',\n",
       " 'RNFLT.765',\n",
       " 'RNFLT.766',\n",
       " 'RNFLT.767',\n",
       " 'RNFLT.768',\n",
       " 'GRI',\n",
       " 'MD_Slope',\n",
       " 'MD_P',\n",
       " 'VFI_Slope',\n",
       " 'VFI_P',\n",
       " 'VF_EXAM_START',\n",
       " 'VF_EXAM_END',\n",
       " 'VF_FOLLOW_UP',\n",
       " 'VF_N',\n",
       " 'VF_OCT_BASELINE_DIFF',\n",
       " 'VF_OCT_FINAL_DIFF',\n",
       " 'MD_BASELINE',\n",
       " 'MD_FINAL',\n",
       " 'VFI_BASELINE',\n",
       " 'VFI_FINAL']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "33a3f1d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ld/l5hmq0nn6tg8mfw0_3xycn180000gn/T/ipykernel_63063/2630850070.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['GRI'] = df['GRI'].fillna(0)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0      -3.688171\n",
       "1      -6.827438\n",
       "2       0.329429\n",
       "3       0.581343\n",
       "4       0.000000\n",
       "         ...    \n",
       "579   -11.691467\n",
       "580   -19.908699\n",
       "581   -10.130481\n",
       "582   -24.731627\n",
       "583   -18.674765\n",
       "Name: GRI, Length: 580, dtype: float64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['GRI'] = df['GRI'].fillna(0)\n",
    "df['GRI']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f89cfff2",
   "metadata": {},
   "source": [
    "Without filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "787bd36e",
   "metadata": {},
   "source": [
    "#filter MD_BASELINE > -12\n",
    "md_filter = (df['MD_BASELINE'] > -12)\n",
    "df = df[md_filter]\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f35d7f1",
   "metadata": {},
   "source": [
    "Define binary progression outcome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "142144f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      Non-Progressor\n",
       "1          Progressor\n",
       "2      Non-Progressor\n",
       "3      Non-Progressor\n",
       "4      Non-Progressor\n",
       "            ...      \n",
       "579        Progressor\n",
       "580        Progressor\n",
       "581        Progressor\n",
       "582        Progressor\n",
       "583        Progressor\n",
       "Name: GRI, Length: 580, dtype: category\n",
       "Categories (2, object): ['Progressor', 'Non-Progressor']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_raw = df\n",
    "y = pd.cut(df.GRI, bins=[-float('inf'), -6, float('inf')],\n",
    "                  labels=['Progressor','Non-Progressor'], ordered=False)\n",
    "\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e9325750",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PID</th>\n",
       "      <th>RNFLT.1</th>\n",
       "      <th>RNFLT.2</th>\n",
       "      <th>RNFLT.3</th>\n",
       "      <th>RNFLT.4</th>\n",
       "      <th>RNFLT.5</th>\n",
       "      <th>RNFLT.6</th>\n",
       "      <th>RNFLT.7</th>\n",
       "      <th>RNFLT.8</th>\n",
       "      <th>RNFLT.9</th>\n",
       "      <th>...</th>\n",
       "      <th>RNFLT.760</th>\n",
       "      <th>RNFLT.761</th>\n",
       "      <th>RNFLT.762</th>\n",
       "      <th>RNFLT.763</th>\n",
       "      <th>RNFLT.764</th>\n",
       "      <th>RNFLT.765</th>\n",
       "      <th>RNFLT.766</th>\n",
       "      <th>RNFLT.767</th>\n",
       "      <th>RNFLT.768</th>\n",
       "      <th>GRI</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>47.0</td>\n",
       "      <td>47.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>...</td>\n",
       "      <td>47.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>47.0</td>\n",
       "      <td>-3.688171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>70.0</td>\n",
       "      <td>71.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>...</td>\n",
       "      <td>59.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>61.0</td>\n",
       "      <td>62.0</td>\n",
       "      <td>63.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>69.0</td>\n",
       "      <td>-6.827438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>44.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>47.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>...</td>\n",
       "      <td>44.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>0.329429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>44.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>47.0</td>\n",
       "      <td>47.0</td>\n",
       "      <td>...</td>\n",
       "      <td>43.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>0.581343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>37.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>...</td>\n",
       "      <td>35.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>579</th>\n",
       "      <td>329</td>\n",
       "      <td>100.0</td>\n",
       "      <td>103.0</td>\n",
       "      <td>106.0</td>\n",
       "      <td>108.0</td>\n",
       "      <td>111.0</td>\n",
       "      <td>112.0</td>\n",
       "      <td>113.0</td>\n",
       "      <td>113.0</td>\n",
       "      <td>113.0</td>\n",
       "      <td>...</td>\n",
       "      <td>82.0</td>\n",
       "      <td>83.0</td>\n",
       "      <td>84.0</td>\n",
       "      <td>86.0</td>\n",
       "      <td>87.0</td>\n",
       "      <td>89.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>94.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>-11.691467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>580</th>\n",
       "      <td>330</td>\n",
       "      <td>52.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>59.0</td>\n",
       "      <td>...</td>\n",
       "      <td>46.0</td>\n",
       "      <td>47.0</td>\n",
       "      <td>47.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>-19.908699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>581</th>\n",
       "      <td>330</td>\n",
       "      <td>62.0</td>\n",
       "      <td>63.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>...</td>\n",
       "      <td>54.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>59.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>61.0</td>\n",
       "      <td>-10.130481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>582</th>\n",
       "      <td>331</td>\n",
       "      <td>47.0</td>\n",
       "      <td>47.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>...</td>\n",
       "      <td>47.0</td>\n",
       "      <td>47.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>-24.731627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>583</th>\n",
       "      <td>331</td>\n",
       "      <td>31.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>...</td>\n",
       "      <td>32.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>-18.674765</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>580 rows × 770 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     PID  RNFLT.1  RNFLT.2  RNFLT.3  RNFLT.4  RNFLT.5  RNFLT.6  RNFLT.7  \\\n",
       "0      1     47.0     47.0     46.0     46.0     45.0     45.0     45.0   \n",
       "1      1     70.0     71.0     72.0     72.0     73.0     73.0     73.0   \n",
       "2      2     44.0     45.0     45.0     45.0     46.0     47.0     48.0   \n",
       "3      2     44.0     44.0     44.0     45.0     45.0     46.0     46.0   \n",
       "4      3     37.0     38.0     39.0     40.0     41.0     42.0     43.0   \n",
       "..   ...      ...      ...      ...      ...      ...      ...      ...   \n",
       "579  329    100.0    103.0    106.0    108.0    111.0    112.0    113.0   \n",
       "580  330     52.0     52.0     53.0     54.0     55.0     56.0     57.0   \n",
       "581  330     62.0     63.0     64.0     65.0     66.0     67.0     68.0   \n",
       "582  331     47.0     47.0     48.0     48.0     49.0     49.0     50.0   \n",
       "583  331     31.0     31.0     32.0     33.0     33.0     34.0     35.0   \n",
       "\n",
       "     RNFLT.8  RNFLT.9  ...  RNFLT.760  RNFLT.761  RNFLT.762  RNFLT.763  \\\n",
       "0       45.0     45.0  ...       47.0       48.0       48.0       48.0   \n",
       "1       73.0     74.0  ...       59.0       60.0       61.0       62.0   \n",
       "2       50.0     51.0  ...       44.0       45.0       45.0       45.0   \n",
       "3       47.0     47.0  ...       43.0       43.0       43.0       43.0   \n",
       "4       44.0     46.0  ...       35.0       35.0       35.0       35.0   \n",
       "..       ...      ...  ...        ...        ...        ...        ...   \n",
       "579    113.0    113.0  ...       82.0       83.0       84.0       86.0   \n",
       "580     58.0     59.0  ...       46.0       47.0       47.0       48.0   \n",
       "581     68.0     68.0  ...       54.0       55.0       56.0       57.0   \n",
       "582     50.0     50.0  ...       47.0       47.0       46.0       46.0   \n",
       "583     36.0     37.0  ...       32.0       31.0       31.0       30.0   \n",
       "\n",
       "     RNFLT.764  RNFLT.765  RNFLT.766  RNFLT.767  RNFLT.768        GRI  \n",
       "0         48.0       48.0       48.0       48.0       47.0  -3.688171  \n",
       "1         63.0       65.0       66.0       67.0       69.0  -6.827438  \n",
       "2         45.0       45.0       45.0       45.0       44.0   0.329429  \n",
       "3         43.0       43.0       43.0       43.0       43.0   0.581343  \n",
       "4         35.0       35.0       35.0       36.0       36.0   0.000000  \n",
       "..         ...        ...        ...        ...        ...        ...  \n",
       "579       87.0       89.0       92.0       94.0       97.0 -11.691467  \n",
       "580       48.0       49.0       49.0       50.0       51.0 -19.908699  \n",
       "581       58.0       58.0       59.0       60.0       61.0 -10.130481  \n",
       "582       45.0       45.0       46.0       46.0       46.0 -24.731627  \n",
       "583       30.0       30.0       30.0       30.0       30.0 -18.674765  \n",
       "\n",
       "[580 rows x 770 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df_raw.iloc[:, np.r_[1, 28:797]]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3b26cb34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(575, 770)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#drop missing values\n",
    "df = df.dropna()\n",
    "df.isnull().values.sum()\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "35d642e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      Non-Progressor\n",
       "1          Progressor\n",
       "2      Non-Progressor\n",
       "3      Non-Progressor\n",
       "4      Non-Progressor\n",
       "            ...      \n",
       "579        Progressor\n",
       "580        Progressor\n",
       "581        Progressor\n",
       "582        Progressor\n",
       "583        Progressor\n",
       "Name: GRI, Length: 575, dtype: category\n",
       "Categories (2, object): ['Progressor', 'Non-Progressor']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = pd.cut(df.GRI, bins=[-float('inf'), -6, float('inf')],\n",
    "                  labels=['Progressor','Non-Progressor'], ordered=False)\n",
    "\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "29c7cc77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/applied-systems-biology/Dynamic_SPHARM/blob/master/SPHARM/classes/stratified_group_shuffle_split.py\n",
    "\n",
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "\n",
    "from sklearn.utils.validation import check_array\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "\n",
    "class GroupShuffleSplitStratified(StratifiedShuffleSplit):\n",
    "\n",
    "    def __init__(self, n_splits=5, test_size=2, train_size=None, random_state=None):\n",
    "\n",
    "        super(GroupShuffleSplitStratified, self).__init__(\n",
    "            n_splits=n_splits,\n",
    "            test_size=test_size,\n",
    "            train_size=train_size,\n",
    "            random_state=random_state)\n",
    "\n",
    "    def _iter_indices(self, X, y, groups):\n",
    "        if groups is None:\n",
    "            raise ValueError(\"The 'groups' parameter should not be None.\")\n",
    "        groups = check_array(groups, ensure_2d=False, dtype=None)\n",
    "        groups_unique, group_indices = np.unique(groups, return_inverse=True)\n",
    "        classes = []\n",
    "        for gr in groups_unique:\n",
    "            classes.append(y[np.where(groups==gr)[0][0]])\n",
    "\n",
    "        for group_train, group_test in super(\n",
    "                GroupShuffleSplitStratified, self)._iter_indices(X=groups_unique, y=classes):\n",
    "            # these are the indices of classes in the partition\n",
    "            # invert them into data indices\n",
    "\n",
    "            train = np.flatnonzero(np.in1d(group_indices, group_train))\n",
    "            test = np.flatnonzero(np.in1d(group_indices, group_test))\n",
    "\n",
    "            yield train, test\n",
    "\n",
    "    def split(self, X, y=None, groups=None):\n",
    "        return super(GroupShuffleSplitStratified, self).split(X, y, groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "01ef6efb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(523, 770)\n",
      "(52, 770)\n"
     ]
    }
   ],
   "source": [
    "train_i,test_i = next(GroupShuffleSplitStratified(n_splits=2, test_size=0.1,\n",
    "                                        random_state=8).split(df,y, groups=df['PID']))\n",
    "TrainVal = df.iloc[train_i]\n",
    "TestSet = df.iloc[test_i]\n",
    "print(TrainVal.shape)\n",
    "print(TestSet.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1321fce8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 26,  27,  51,  52,  60,  61, 116, 141, 142, 143, 144, 193, 196,\n",
       "       197, 198, 218, 219, 246, 247, 248, 249, 255, 256, 257, 258, 260,\n",
       "       261, 270, 271, 274, 275, 285, 309, 310, 316, 317, 330, 331, 335,\n",
       "       336, 366, 432, 472, 473, 476, 483, 495, 512, 524, 525, 536, 537])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# y.iloc[train_i]\n",
    "test_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "83cac2de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(468, 770)\n",
      "(55, 770)\n"
     ]
    }
   ],
   "source": [
    "train_id,val_id = next(GroupShuffleSplitStratified(n_splits=2, test_size=0.1,\n",
    "                                        random_state=8).split(TrainVal,y.iloc[train_i], groups=TrainVal['PID']))\n",
    "TrainSet = TrainVal.iloc[train_id]\n",
    "ValSet = TrainVal.iloc[val_id]\n",
    "print(TrainSet.shape)\n",
    "print(ValSet.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b90a0a53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "(52, 768)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RNFLT.1</th>\n",
       "      <th>RNFLT.2</th>\n",
       "      <th>RNFLT.3</th>\n",
       "      <th>RNFLT.4</th>\n",
       "      <th>RNFLT.5</th>\n",
       "      <th>RNFLT.6</th>\n",
       "      <th>RNFLT.7</th>\n",
       "      <th>RNFLT.8</th>\n",
       "      <th>RNFLT.9</th>\n",
       "      <th>RNFLT.10</th>\n",
       "      <th>...</th>\n",
       "      <th>RNFLT.759</th>\n",
       "      <th>RNFLT.760</th>\n",
       "      <th>RNFLT.761</th>\n",
       "      <th>RNFLT.762</th>\n",
       "      <th>RNFLT.763</th>\n",
       "      <th>RNFLT.764</th>\n",
       "      <th>RNFLT.765</th>\n",
       "      <th>RNFLT.766</th>\n",
       "      <th>RNFLT.767</th>\n",
       "      <th>RNFLT.768</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>34.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>...</td>\n",
       "      <td>34.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>33.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>42.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>47.0</td>\n",
       "      <td>47.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>...</td>\n",
       "      <td>41.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>41.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>34.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>47.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>...</td>\n",
       "      <td>29.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>32.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>52.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>...</td>\n",
       "      <td>52.0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>52.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>53.0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>...</td>\n",
       "      <td>55.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>53.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 768 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    RNFLT.1  RNFLT.2  RNFLT.3  RNFLT.4  RNFLT.5  RNFLT.6  RNFLT.7  RNFLT.8  \\\n",
       "26     34.0     34.0     35.0     35.0     36.0     36.0     37.0     38.0   \n",
       "27     42.0     43.0     44.0     45.0     46.0     46.0     47.0     47.0   \n",
       "51     34.0     35.0     37.0     40.0     42.0     44.0     46.0     47.0   \n",
       "52     52.0     52.0     52.0     52.0     52.0     52.0     51.0     51.0   \n",
       "62     53.0     53.0     53.0     54.0     54.0     55.0     56.0     56.0   \n",
       "\n",
       "    RNFLT.9  RNFLT.10  ...  RNFLT.759  RNFLT.760  RNFLT.761  RNFLT.762  \\\n",
       "26     39.0      39.0  ...       34.0       33.0       33.0       33.0   \n",
       "27     48.0      48.0  ...       41.0       40.0       40.0       40.0   \n",
       "51     48.0      48.0  ...       29.0       29.0       28.0       28.0   \n",
       "52     51.0      51.0  ...       52.0       53.0       53.0       53.0   \n",
       "62     57.0      58.0  ...       55.0       55.0       55.0       55.0   \n",
       "\n",
       "    RNFLT.763  RNFLT.764  RNFLT.765  RNFLT.766  RNFLT.767  RNFLT.768  \n",
       "26       32.0       32.0       32.0       33.0       33.0       33.0  \n",
       "27       40.0       40.0       40.0       40.0       41.0       41.0  \n",
       "51       28.0       29.0       29.0       30.0       31.0       32.0  \n",
       "52       53.0       53.0       53.0       53.0       52.0       52.0  \n",
       "62       54.0       54.0       54.0       53.0       53.0       53.0  \n",
       "\n",
       "[5 rows x 768 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = df.iloc[test_i, 1:769]\n",
    "print(x.isnull().values.sum())\n",
    "print(x.shape)\n",
    "x.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "02c5dfcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(52, 768, 1)\n"
     ]
    }
   ],
   "source": [
    "x = np.asarray(x)\n",
    "scaled_x = x/381\n",
    "scaled_x = scaled_x.reshape(scaled_x.shape[0],scaled_x.shape[1],1)\n",
    "X_test = scaled_x\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "63d7b9db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(468, 768, 1)\n"
     ]
    }
   ],
   "source": [
    "x = TrainVal.iloc[train_id, 1:769]\n",
    "x = np.asarray(x)\n",
    "scaled_x = x/381\n",
    "scaled_x = scaled_x.reshape(scaled_x.shape[0],scaled_x.shape[1],1)\n",
    "X_train = scaled_x\n",
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c4a3d31d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(55, 768, 1)\n"
     ]
    }
   ],
   "source": [
    "x = TrainVal.iloc[val_id, 1:769]\n",
    "x = np.asarray(x)\n",
    "scaled_x = x/381\n",
    "scaled_x = scaled_x.reshape(scaled_x.shape[0],scaled_x.shape[1],1)\n",
    "X_val = scaled_x\n",
    "print(X_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56d2496e",
   "metadata": {},
   "source": [
    "Reshape the X matrix ONLY FOR CNN models, do not need to reshape for RF or SVM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "df3a0e60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progressor  Non-Progressor\n",
      "0           1                 407\n",
      "1           0                 168\n",
      "dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Progressor</th>\n",
       "      <th>Non-Progressor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>579</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>580</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>581</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>582</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>583</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>575 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Progressor  Non-Progressor\n",
       "0             0               1\n",
       "1             1               0\n",
       "2             0               1\n",
       "3             0               1\n",
       "4             0               1\n",
       "..          ...             ...\n",
       "579           1               0\n",
       "580           1               0\n",
       "581           1               0\n",
       "582           1               0\n",
       "583           1               0\n",
       "\n",
       "[575 rows x 2 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#one-hot-encoding our label\n",
    "y = pd.get_dummies(y)\n",
    "print(y.value_counts())\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2ec6c722",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(52, 2)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test = y.iloc[test_i]\n",
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "dcc77677",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Progressor</th>\n",
       "      <th>Non-Progressor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>222</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>250</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>251</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>252</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>258</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>259</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>260</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>261</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>263</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>264</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>273</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>274</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>277</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>278</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>288</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>312</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>313</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>320</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>321</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>334</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>335</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>339</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>340</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>370</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>437</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>478</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>479</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>482</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>489</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>501</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>519</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>532</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>533</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>544</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>545</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Progressor  Non-Progressor\n",
       "26            0               1\n",
       "27            0               1\n",
       "51            1               0\n",
       "52            0               1\n",
       "62            1               0\n",
       "63            0               1\n",
       "118           0               1\n",
       "143           1               0\n",
       "144           0               1\n",
       "145           0               1\n",
       "146           0               1\n",
       "196           0               1\n",
       "199           0               1\n",
       "200           0               1\n",
       "201           0               1\n",
       "221           0               1\n",
       "222           0               1\n",
       "249           0               1\n",
       "250           0               1\n",
       "251           0               1\n",
       "252           0               1\n",
       "258           0               1\n",
       "259           0               1\n",
       "260           0               1\n",
       "261           0               1\n",
       "263           1               0\n",
       "264           0               1\n",
       "273           0               1\n",
       "274           0               1\n",
       "277           1               0\n",
       "278           0               1\n",
       "288           1               0\n",
       "312           0               1\n",
       "313           1               0\n",
       "320           0               1\n",
       "321           0               1\n",
       "334           1               0\n",
       "335           1               0\n",
       "339           0               1\n",
       "340           0               1\n",
       "370           0               1\n",
       "437           0               1\n",
       "478           0               1\n",
       "479           1               0\n",
       "482           1               0\n",
       "489           0               1\n",
       "501           0               1\n",
       "519           1               0\n",
       "532           1               0\n",
       "533           1               0\n",
       "544           0               1\n",
       "545           0               1"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "54cceebc",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(468, 2)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train = y.iloc[train_i].iloc[train_id]\n",
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fd7ff49a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Progressor</th>\n",
       "      <th>Non-Progressor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>579</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>580</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>581</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>582</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>583</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>468 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Progressor  Non-Progressor\n",
       "0             0               1\n",
       "1             1               0\n",
       "2             0               1\n",
       "3             0               1\n",
       "4             0               1\n",
       "..          ...             ...\n",
       "579           1               0\n",
       "580           1               0\n",
       "581           1               0\n",
       "582           1               0\n",
       "583           1               0\n",
       "\n",
       "[468 rows x 2 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fb226e88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(55, 2)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_val = y.iloc[train_i].iloc[val_id]\n",
    "y_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a46fa45f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(468, 768, 1)\n",
      "(52, 768, 1)\n",
      "(55, 768, 1)\n",
      "Progressor  Non-Progressor\n",
      "0           1                 331\n",
      "1           0                 137\n",
      "dtype: int64 \n",
      "\n",
      "Progressor  Non-Progressor\n",
      "0           1                 38\n",
      "1           0                 17\n",
      "dtype: int64 \n",
      "\n",
      "Progressor  Non-Progressor\n",
      "0           1                 38\n",
      "1           0                 14\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(X_val.shape)\n",
    "print(y_train.value_counts(), '\\n')\n",
    "print(y_val.value_counts(), '\\n')\n",
    "print(y_test.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f260340",
   "metadata": {},
   "source": [
    "## Resampling for unbalanced dataset (NEW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8641ba76",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
       "       0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "       0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "       1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1,\n",
       "       0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1,\n",
       "       0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1,\n",
       "       0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1,\n",
       "       0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
       "       0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0,\n",
       "       0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1,\n",
       "       1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "       0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0,\n",
       "       1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0,\n",
       "       0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "       0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0,\n",
       "       1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0,\n",
       "       0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0,\n",
       "       0, 1, 1, 1, 1, 1], dtype=uint8)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_progressor = np.array(y_train)[:,0]\n",
    "y_progressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5db5df10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "468"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_progressor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c73be771",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(468, 768)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_2d = np.reshape(X_train, (468, 768))\n",
    "X_train_2d.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "953e525c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(662, 768)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
       "       0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "       0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "       1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1,\n",
       "       0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1,\n",
       "       0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1,\n",
       "       0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1,\n",
       "       0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
       "       0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0,\n",
       "       0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1,\n",
       "       1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "       0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0,\n",
       "       1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0,\n",
       "       0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "       0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0,\n",
       "       1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0,\n",
       "       0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0,\n",
       "       0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1], dtype=uint8)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from imblearn.over_sampling import RandomOverSampler\n",
    "oversample = RandomOverSampler(sampling_strategy = 'minority')\n",
    "X_train_over, y_train_over = oversample.fit_resample(X_train_2d, y_progressor)\n",
    "print(X_train_over.shape)\n",
    "y_train_over"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "205655d4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progressor  Non-Progressor\n",
      "0           1                 331\n",
      "1           0                 331\n",
      "dtype: int64 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_train_over = pd.get_dummies(y_train_over)\n",
    "y_train_over = pd.cut(y_train_over[0], bins=[-float('inf'), 0.5, float('inf')],\n",
    "                  labels=['Progressor','Non-Progressor'], ordered=False)\n",
    "y_train_over = pd.get_dummies(y_train_over)\n",
    "print(y_train_over.value_counts(), '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2c74e256",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Progressor</th>\n",
       "      <th>Non-Progressor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>657</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>658</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>659</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>660</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>661</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>662 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Progressor  Non-Progressor\n",
       "0             0               1\n",
       "1             1               0\n",
       "2             0               1\n",
       "3             0               1\n",
       "4             0               1\n",
       "..          ...             ...\n",
       "657           1               0\n",
       "658           1               0\n",
       "659           1               0\n",
       "660           1               0\n",
       "661           1               0\n",
       "\n",
       "[662 rows x 2 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_over"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "dea79d3f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.12335958],\n",
       "        [0.12335958],\n",
       "        [0.12073491],\n",
       "        ...,\n",
       "        [0.12598425],\n",
       "        [0.12598425],\n",
       "        [0.12335958]],\n",
       "\n",
       "       [[0.18372703],\n",
       "        [0.18635171],\n",
       "        [0.18897638],\n",
       "        ...,\n",
       "        [0.17322835],\n",
       "        [0.17585302],\n",
       "        [0.18110236]],\n",
       "\n",
       "       [[0.11548556],\n",
       "        [0.11811024],\n",
       "        [0.11811024],\n",
       "        ...,\n",
       "        [0.11811024],\n",
       "        [0.11811024],\n",
       "        [0.11548556]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[0.12073491],\n",
       "        [0.11811024],\n",
       "        [0.11811024],\n",
       "        ...,\n",
       "        [0.12860892],\n",
       "        [0.12598425],\n",
       "        [0.12335958]],\n",
       "\n",
       "       [[0.24409449],\n",
       "        [0.23884514],\n",
       "        [0.2335958 ],\n",
       "        ...,\n",
       "        [0.25721785],\n",
       "        [0.2519685 ],\n",
       "        [0.24671916]],\n",
       "\n",
       "       [[0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        ...,\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ]]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_over = np.reshape(X_train_over, (662, 768, 1))\n",
    "X_train_over"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74009dc8",
   "metadata": {},
   "source": [
    "### An Example CNN Model for orginal dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ca0209c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv1d (Conv1D)             (None, 766, 64)           256       \n",
      "                                                                 \n",
      " max_pooling1d (MaxPooling1D  (None, 255, 64)          0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 255, 64)           0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 16320)             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 64)                1044544   \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 16)                1040      \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 2)                 34        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,045,874\n",
      "Trainable params: 1,045,874\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-05 11:24:47.576520: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "#create model1\n",
    "model_1 = Sequential()\n",
    "\n",
    "#add layers\n",
    "model_1.add(Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(768,1)))\n",
    "model_1.add(MaxPooling1D(pool_size=3))\n",
    "# model_1.add(Conv1D(filters=32, kernel_size=3, activation='relu'))\n",
    "model_1.add(Dropout(0.5))\n",
    "# model_1.add(MaxPooling1D(pool_size=2))\n",
    "model_1.add(Flatten())\n",
    "model_1.add(Dense(64, activation='relu'))\n",
    "model_1.add(Dense(16, activation='relu'))\n",
    "model_1.add(Dense(2, activation='softmax'))\n",
    "model_1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5d4a6d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "early_stopping_monitor = EarlyStopping(\n",
    "    monitor='val_accuracy',\n",
    "    min_delta=0,\n",
    "    patience=100,\n",
    "    verbose=0,\n",
    "    mode='auto',\n",
    "    baseline=None,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "opt1 = keras.optimizers.Adam(learning_rate = 1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4399650d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "15/15 [==============================] - 1s 35ms/step - loss: 0.6345 - accuracy: 0.7051 - val_loss: 0.6073 - val_accuracy: 0.6909\n",
      "Epoch 2/500\n",
      "15/15 [==============================] - 0s 21ms/step - loss: 0.5968 - accuracy: 0.7073 - val_loss: 0.6091 - val_accuracy: 0.6909\n",
      "Epoch 3/500\n",
      "15/15 [==============================] - 0s 21ms/step - loss: 0.5977 - accuracy: 0.7073 - val_loss: 0.6065 - val_accuracy: 0.6909\n",
      "Epoch 4/500\n",
      "15/15 [==============================] - 0s 21ms/step - loss: 0.5959 - accuracy: 0.7073 - val_loss: 0.6046 - val_accuracy: 0.6909\n",
      "Epoch 5/500\n",
      "15/15 [==============================] - 0s 21ms/step - loss: 0.5953 - accuracy: 0.7073 - val_loss: 0.6048 - val_accuracy: 0.6909\n",
      "Epoch 6/500\n",
      "15/15 [==============================] - 0s 22ms/step - loss: 0.5929 - accuracy: 0.7073 - val_loss: 0.6044 - val_accuracy: 0.6909\n",
      "Epoch 7/500\n",
      "15/15 [==============================] - 0s 22ms/step - loss: 0.5940 - accuracy: 0.7073 - val_loss: 0.6026 - val_accuracy: 0.6909\n",
      "Epoch 8/500\n",
      "15/15 [==============================] - 0s 22ms/step - loss: 0.5935 - accuracy: 0.7073 - val_loss: 0.6018 - val_accuracy: 0.6909\n",
      "Epoch 9/500\n",
      "15/15 [==============================] - 0s 22ms/step - loss: 0.5915 - accuracy: 0.7073 - val_loss: 0.6032 - val_accuracy: 0.6909\n",
      "Epoch 10/500\n",
      "15/15 [==============================] - 0s 24ms/step - loss: 0.5917 - accuracy: 0.7073 - val_loss: 0.6025 - val_accuracy: 0.6909\n",
      "Epoch 11/500\n",
      "15/15 [==============================] - 0s 30ms/step - loss: 0.5913 - accuracy: 0.7073 - val_loss: 0.6000 - val_accuracy: 0.6909\n",
      "Epoch 12/500\n",
      "15/15 [==============================] - 0s 30ms/step - loss: 0.5897 - accuracy: 0.7073 - val_loss: 0.6003 - val_accuracy: 0.6909\n",
      "Epoch 13/500\n",
      "15/15 [==============================] - 1s 35ms/step - loss: 0.5876 - accuracy: 0.7073 - val_loss: 0.5993 - val_accuracy: 0.6909\n",
      "Epoch 14/500\n",
      "15/15 [==============================] - 1s 41ms/step - loss: 0.5865 - accuracy: 0.7073 - val_loss: 0.5994 - val_accuracy: 0.6909\n",
      "Epoch 15/500\n",
      "15/15 [==============================] - 0s 32ms/step - loss: 0.5850 - accuracy: 0.7073 - val_loss: 0.5978 - val_accuracy: 0.6909\n",
      "Epoch 16/500\n",
      "15/15 [==============================] - 1s 40ms/step - loss: 0.5832 - accuracy: 0.7073 - val_loss: 0.5969 - val_accuracy: 0.6909\n",
      "Epoch 17/500\n",
      "15/15 [==============================] - 1s 45ms/step - loss: 0.5821 - accuracy: 0.7073 - val_loss: 0.5973 - val_accuracy: 0.6909\n",
      "Epoch 18/500\n",
      "15/15 [==============================] - 1s 46ms/step - loss: 0.5815 - accuracy: 0.7073 - val_loss: 0.5952 - val_accuracy: 0.6909\n",
      "Epoch 19/500\n",
      "15/15 [==============================] - 1s 35ms/step - loss: 0.5802 - accuracy: 0.7073 - val_loss: 0.5947 - val_accuracy: 0.6909\n",
      "Epoch 20/500\n",
      "15/15 [==============================] - 1s 35ms/step - loss: 0.5778 - accuracy: 0.7073 - val_loss: 0.5943 - val_accuracy: 0.6909\n",
      "Epoch 21/500\n",
      "15/15 [==============================] - 0s 33ms/step - loss: 0.5770 - accuracy: 0.7073 - val_loss: 0.5926 - val_accuracy: 0.6909\n",
      "Epoch 22/500\n",
      "15/15 [==============================] - 0s 27ms/step - loss: 0.5728 - accuracy: 0.7073 - val_loss: 0.5925 - val_accuracy: 0.6909\n",
      "Epoch 23/500\n",
      "15/15 [==============================] - 0s 25ms/step - loss: 0.5737 - accuracy: 0.7115 - val_loss: 0.5917 - val_accuracy: 0.6909\n",
      "Epoch 24/500\n",
      "15/15 [==============================] - 0s 26ms/step - loss: 0.5723 - accuracy: 0.7115 - val_loss: 0.5902 - val_accuracy: 0.6909\n",
      "Epoch 25/500\n",
      "15/15 [==============================] - 0s 25ms/step - loss: 0.5707 - accuracy: 0.7158 - val_loss: 0.5898 - val_accuracy: 0.6909\n",
      "Epoch 26/500\n",
      "15/15 [==============================] - 0s 30ms/step - loss: 0.5681 - accuracy: 0.7137 - val_loss: 0.5896 - val_accuracy: 0.6909\n",
      "Epoch 27/500\n",
      "15/15 [==============================] - 0s 27ms/step - loss: 0.5686 - accuracy: 0.7137 - val_loss: 0.5872 - val_accuracy: 0.6909\n",
      "Epoch 28/500\n",
      "15/15 [==============================] - 0s 33ms/step - loss: 0.5641 - accuracy: 0.7158 - val_loss: 0.5886 - val_accuracy: 0.6909\n",
      "Epoch 29/500\n",
      "15/15 [==============================] - 1s 43ms/step - loss: 0.5618 - accuracy: 0.7137 - val_loss: 0.5900 - val_accuracy: 0.6909\n",
      "Epoch 30/500\n",
      "15/15 [==============================] - 0s 30ms/step - loss: 0.5600 - accuracy: 0.7179 - val_loss: 0.5878 - val_accuracy: 0.6909\n",
      "Epoch 31/500\n",
      "15/15 [==============================] - 0s 27ms/step - loss: 0.5584 - accuracy: 0.7158 - val_loss: 0.5859 - val_accuracy: 0.6909\n",
      "Epoch 32/500\n",
      "15/15 [==============================] - 0s 26ms/step - loss: 0.5554 - accuracy: 0.7222 - val_loss: 0.5854 - val_accuracy: 0.6909\n",
      "Epoch 33/500\n",
      "15/15 [==============================] - 0s 30ms/step - loss: 0.5524 - accuracy: 0.7201 - val_loss: 0.5843 - val_accuracy: 0.6909\n",
      "Epoch 34/500\n",
      "15/15 [==============================] - 0s 26ms/step - loss: 0.5508 - accuracy: 0.7201 - val_loss: 0.5826 - val_accuracy: 0.6909\n",
      "Epoch 35/500\n",
      "15/15 [==============================] - 0s 29ms/step - loss: 0.5474 - accuracy: 0.7265 - val_loss: 0.5868 - val_accuracy: 0.6909\n",
      "Epoch 36/500\n",
      "15/15 [==============================] - 0s 30ms/step - loss: 0.5457 - accuracy: 0.7329 - val_loss: 0.5828 - val_accuracy: 0.6909\n",
      "Epoch 37/500\n",
      "15/15 [==============================] - 0s 24ms/step - loss: 0.5449 - accuracy: 0.7201 - val_loss: 0.5845 - val_accuracy: 0.6909\n",
      "Epoch 38/500\n",
      "15/15 [==============================] - 0s 27ms/step - loss: 0.5399 - accuracy: 0.7329 - val_loss: 0.5826 - val_accuracy: 0.6909\n",
      "Epoch 39/500\n",
      "15/15 [==============================] - 0s 25ms/step - loss: 0.5396 - accuracy: 0.7372 - val_loss: 0.5837 - val_accuracy: 0.7091\n",
      "Epoch 40/500\n",
      "15/15 [==============================] - 0s 26ms/step - loss: 0.5335 - accuracy: 0.7308 - val_loss: 0.5828 - val_accuracy: 0.7091\n",
      "Epoch 41/500\n",
      "15/15 [==============================] - 0s 25ms/step - loss: 0.5358 - accuracy: 0.7350 - val_loss: 0.5787 - val_accuracy: 0.6727\n",
      "Epoch 42/500\n",
      "15/15 [==============================] - 0s 24ms/step - loss: 0.5333 - accuracy: 0.7393 - val_loss: 0.5862 - val_accuracy: 0.7091\n",
      "Epoch 43/500\n",
      "15/15 [==============================] - 0s 24ms/step - loss: 0.5326 - accuracy: 0.7372 - val_loss: 0.5804 - val_accuracy: 0.6727\n",
      "Epoch 44/500\n",
      "15/15 [==============================] - 0s 27ms/step - loss: 0.5273 - accuracy: 0.7393 - val_loss: 0.5799 - val_accuracy: 0.6727\n",
      "Epoch 45/500\n",
      "15/15 [==============================] - 0s 26ms/step - loss: 0.5274 - accuracy: 0.7521 - val_loss: 0.5922 - val_accuracy: 0.6909\n",
      "Epoch 46/500\n",
      "15/15 [==============================] - 0s 26ms/step - loss: 0.5281 - accuracy: 0.7286 - val_loss: 0.5781 - val_accuracy: 0.6727\n",
      "Epoch 47/500\n",
      "15/15 [==============================] - 0s 26ms/step - loss: 0.5219 - accuracy: 0.7564 - val_loss: 0.5823 - val_accuracy: 0.6909\n",
      "Epoch 48/500\n",
      "15/15 [==============================] - 0s 27ms/step - loss: 0.5203 - accuracy: 0.7393 - val_loss: 0.5784 - val_accuracy: 0.6909\n",
      "Epoch 49/500\n",
      "15/15 [==============================] - 0s 24ms/step - loss: 0.5183 - accuracy: 0.7521 - val_loss: 0.5820 - val_accuracy: 0.6727\n",
      "Epoch 50/500\n",
      "15/15 [==============================] - 0s 26ms/step - loss: 0.5140 - accuracy: 0.7436 - val_loss: 0.5793 - val_accuracy: 0.6727\n",
      "Epoch 51/500\n",
      "15/15 [==============================] - 0s 26ms/step - loss: 0.5099 - accuracy: 0.7543 - val_loss: 0.5806 - val_accuracy: 0.6727\n",
      "Epoch 52/500\n",
      "15/15 [==============================] - 0s 25ms/step - loss: 0.5108 - accuracy: 0.7436 - val_loss: 0.5898 - val_accuracy: 0.6909\n",
      "Epoch 53/500\n",
      "15/15 [==============================] - 0s 25ms/step - loss: 0.5102 - accuracy: 0.7479 - val_loss: 0.5817 - val_accuracy: 0.6545\n",
      "Epoch 54/500\n",
      "15/15 [==============================] - 0s 25ms/step - loss: 0.5138 - accuracy: 0.7479 - val_loss: 0.5779 - val_accuracy: 0.7091\n",
      "Epoch 55/500\n",
      "15/15 [==============================] - 0s 25ms/step - loss: 0.5046 - accuracy: 0.7671 - val_loss: 0.5826 - val_accuracy: 0.6727\n",
      "Epoch 56/500\n",
      "15/15 [==============================] - 0s 25ms/step - loss: 0.5054 - accuracy: 0.7692 - val_loss: 0.5837 - val_accuracy: 0.6727\n",
      "Epoch 57/500\n",
      "15/15 [==============================] - 0s 23ms/step - loss: 0.5064 - accuracy: 0.7607 - val_loss: 0.5845 - val_accuracy: 0.6727\n",
      "Epoch 58/500\n",
      "15/15 [==============================] - 0s 24ms/step - loss: 0.5096 - accuracy: 0.7521 - val_loss: 0.5811 - val_accuracy: 0.7091\n",
      "Epoch 59/500\n",
      "15/15 [==============================] - 0s 24ms/step - loss: 0.5049 - accuracy: 0.7521 - val_loss: 0.5841 - val_accuracy: 0.6909\n",
      "Epoch 60/500\n",
      "15/15 [==============================] - 0s 24ms/step - loss: 0.4952 - accuracy: 0.7671 - val_loss: 0.5846 - val_accuracy: 0.6727\n",
      "Epoch 61/500\n",
      "15/15 [==============================] - 0s 25ms/step - loss: 0.4970 - accuracy: 0.7543 - val_loss: 0.5851 - val_accuracy: 0.7091\n",
      "Epoch 62/500\n",
      "15/15 [==============================] - 0s 24ms/step - loss: 0.5068 - accuracy: 0.7564 - val_loss: 0.5938 - val_accuracy: 0.6727\n",
      "Epoch 63/500\n",
      "15/15 [==============================] - 0s 24ms/step - loss: 0.5042 - accuracy: 0.7607 - val_loss: 0.5793 - val_accuracy: 0.7273\n",
      "Epoch 64/500\n",
      "15/15 [==============================] - 0s 22ms/step - loss: 0.4929 - accuracy: 0.7500 - val_loss: 0.5912 - val_accuracy: 0.6727\n",
      "Epoch 65/500\n",
      "15/15 [==============================] - 0s 23ms/step - loss: 0.4964 - accuracy: 0.7585 - val_loss: 0.5851 - val_accuracy: 0.7091\n",
      "Epoch 66/500\n",
      "15/15 [==============================] - 0s 24ms/step - loss: 0.4837 - accuracy: 0.7692 - val_loss: 0.5874 - val_accuracy: 0.6909\n",
      "Epoch 67/500\n",
      "15/15 [==============================] - 0s 24ms/step - loss: 0.4831 - accuracy: 0.7756 - val_loss: 0.5854 - val_accuracy: 0.7091\n",
      "Epoch 68/500\n",
      "15/15 [==============================] - 0s 22ms/step - loss: 0.4854 - accuracy: 0.7735 - val_loss: 0.5913 - val_accuracy: 0.7091\n",
      "Epoch 69/500\n",
      "15/15 [==============================] - 0s 23ms/step - loss: 0.4826 - accuracy: 0.7778 - val_loss: 0.5840 - val_accuracy: 0.7091\n",
      "Epoch 70/500\n",
      "15/15 [==============================] - 0s 24ms/step - loss: 0.4847 - accuracy: 0.7671 - val_loss: 0.5888 - val_accuracy: 0.6909\n",
      "Epoch 71/500\n",
      "15/15 [==============================] - 0s 25ms/step - loss: 0.4854 - accuracy: 0.7735 - val_loss: 0.5885 - val_accuracy: 0.7091\n",
      "Epoch 72/500\n",
      "15/15 [==============================] - 0s 23ms/step - loss: 0.4809 - accuracy: 0.7714 - val_loss: 0.5878 - val_accuracy: 0.6909\n",
      "Epoch 73/500\n",
      "15/15 [==============================] - 0s 33ms/step - loss: 0.4788 - accuracy: 0.7842 - val_loss: 0.5953 - val_accuracy: 0.6909\n",
      "Epoch 74/500\n",
      "15/15 [==============================] - 0s 32ms/step - loss: 0.4741 - accuracy: 0.7692 - val_loss: 0.5916 - val_accuracy: 0.6909\n",
      "Epoch 75/500\n",
      "15/15 [==============================] - 0s 24ms/step - loss: 0.4773 - accuracy: 0.8056 - val_loss: 0.5922 - val_accuracy: 0.7091\n",
      "Epoch 76/500\n",
      "15/15 [==============================] - 0s 24ms/step - loss: 0.4745 - accuracy: 0.7650 - val_loss: 0.5903 - val_accuracy: 0.7091\n",
      "Epoch 77/500\n",
      "15/15 [==============================] - 0s 32ms/step - loss: 0.4757 - accuracy: 0.7863 - val_loss: 0.5975 - val_accuracy: 0.6909\n",
      "Epoch 78/500\n",
      "15/15 [==============================] - 0s 30ms/step - loss: 0.4748 - accuracy: 0.7650 - val_loss: 0.5906 - val_accuracy: 0.6909\n",
      "Epoch 79/500\n",
      "15/15 [==============================] - 0s 33ms/step - loss: 0.4645 - accuracy: 0.7949 - val_loss: 0.6013 - val_accuracy: 0.6909\n",
      "Epoch 80/500\n",
      "15/15 [==============================] - 0s 33ms/step - loss: 0.4738 - accuracy: 0.7714 - val_loss: 0.5963 - val_accuracy: 0.7091\n",
      "Epoch 81/500\n",
      "15/15 [==============================] - 0s 29ms/step - loss: 0.4671 - accuracy: 0.7863 - val_loss: 0.6003 - val_accuracy: 0.6909\n",
      "Epoch 82/500\n",
      "15/15 [==============================] - 0s 25ms/step - loss: 0.4718 - accuracy: 0.7949 - val_loss: 0.5979 - val_accuracy: 0.6909\n",
      "Epoch 83/500\n",
      "15/15 [==============================] - 0s 25ms/step - loss: 0.4616 - accuracy: 0.7821 - val_loss: 0.5942 - val_accuracy: 0.7273\n",
      "Epoch 84/500\n",
      "15/15 [==============================] - 0s 29ms/step - loss: 0.4644 - accuracy: 0.7906 - val_loss: 0.6005 - val_accuracy: 0.6909\n",
      "Epoch 85/500\n",
      "15/15 [==============================] - 0s 26ms/step - loss: 0.4669 - accuracy: 0.7799 - val_loss: 0.5944 - val_accuracy: 0.7273\n",
      "Epoch 86/500\n",
      "15/15 [==============================] - 0s 26ms/step - loss: 0.4645 - accuracy: 0.7863 - val_loss: 0.6091 - val_accuracy: 0.6909\n",
      "Epoch 87/500\n",
      "15/15 [==============================] - 0s 29ms/step - loss: 0.4605 - accuracy: 0.8013 - val_loss: 0.5997 - val_accuracy: 0.7273\n",
      "Epoch 88/500\n",
      "15/15 [==============================] - 0s 26ms/step - loss: 0.4574 - accuracy: 0.8034 - val_loss: 0.5969 - val_accuracy: 0.6909\n",
      "Epoch 89/500\n",
      "15/15 [==============================] - 0s 29ms/step - loss: 0.4594 - accuracy: 0.7949 - val_loss: 0.6026 - val_accuracy: 0.7273\n",
      "Epoch 90/500\n",
      "15/15 [==============================] - 0s 26ms/step - loss: 0.4475 - accuracy: 0.8056 - val_loss: 0.5997 - val_accuracy: 0.6909\n",
      "Epoch 91/500\n",
      "15/15 [==============================] - 0s 28ms/step - loss: 0.4507 - accuracy: 0.8120 - val_loss: 0.6106 - val_accuracy: 0.6909\n",
      "Epoch 92/500\n",
      "15/15 [==============================] - 0s 24ms/step - loss: 0.4551 - accuracy: 0.7863 - val_loss: 0.6005 - val_accuracy: 0.6727\n",
      "Epoch 93/500\n",
      "15/15 [==============================] - 0s 28ms/step - loss: 0.4538 - accuracy: 0.7970 - val_loss: 0.6051 - val_accuracy: 0.7091\n",
      "Epoch 94/500\n",
      "15/15 [==============================] - 0s 26ms/step - loss: 0.4438 - accuracy: 0.8098 - val_loss: 0.6118 - val_accuracy: 0.6909\n",
      "Epoch 95/500\n",
      "15/15 [==============================] - 0s 30ms/step - loss: 0.4505 - accuracy: 0.8098 - val_loss: 0.6026 - val_accuracy: 0.7091\n",
      "Epoch 96/500\n",
      "15/15 [==============================] - 0s 24ms/step - loss: 0.4364 - accuracy: 0.8184 - val_loss: 0.6085 - val_accuracy: 0.7091\n",
      "Epoch 97/500\n",
      "15/15 [==============================] - 0s 30ms/step - loss: 0.4376 - accuracy: 0.7970 - val_loss: 0.6018 - val_accuracy: 0.6909\n",
      "Epoch 98/500\n",
      "15/15 [==============================] - 0s 25ms/step - loss: 0.4423 - accuracy: 0.8120 - val_loss: 0.6128 - val_accuracy: 0.7091\n",
      "Epoch 99/500\n",
      "15/15 [==============================] - 0s 24ms/step - loss: 0.4484 - accuracy: 0.7991 - val_loss: 0.6071 - val_accuracy: 0.6909\n",
      "Epoch 100/500\n",
      "15/15 [==============================] - 0s 24ms/step - loss: 0.4362 - accuracy: 0.8034 - val_loss: 0.6195 - val_accuracy: 0.7091\n",
      "Epoch 101/500\n",
      "15/15 [==============================] - 0s 24ms/step - loss: 0.4416 - accuracy: 0.8098 - val_loss: 0.6141 - val_accuracy: 0.7273\n",
      "Epoch 102/500\n",
      "15/15 [==============================] - 0s 24ms/step - loss: 0.4428 - accuracy: 0.7970 - val_loss: 0.6078 - val_accuracy: 0.6909\n",
      "Epoch 103/500\n",
      "15/15 [==============================] - 0s 26ms/step - loss: 0.4349 - accuracy: 0.8098 - val_loss: 0.6104 - val_accuracy: 0.7273\n",
      "Epoch 104/500\n",
      "15/15 [==============================] - 0s 26ms/step - loss: 0.4387 - accuracy: 0.8056 - val_loss: 0.6107 - val_accuracy: 0.6727\n",
      "Epoch 105/500\n",
      "15/15 [==============================] - 0s 30ms/step - loss: 0.4297 - accuracy: 0.8312 - val_loss: 0.6098 - val_accuracy: 0.7091\n",
      "Epoch 106/500\n",
      "15/15 [==============================] - 0s 31ms/step - loss: 0.4403 - accuracy: 0.8184 - val_loss: 0.6130 - val_accuracy: 0.7273\n",
      "Epoch 107/500\n",
      "15/15 [==============================] - 1s 34ms/step - loss: 0.4401 - accuracy: 0.8077 - val_loss: 0.6192 - val_accuracy: 0.7091\n",
      "Epoch 108/500\n",
      "15/15 [==============================] - 0s 29ms/step - loss: 0.4292 - accuracy: 0.8098 - val_loss: 0.6080 - val_accuracy: 0.6909\n",
      "Epoch 109/500\n",
      "15/15 [==============================] - 0s 32ms/step - loss: 0.4306 - accuracy: 0.8077 - val_loss: 0.6135 - val_accuracy: 0.7091\n",
      "Epoch 110/500\n",
      "15/15 [==============================] - 0s 25ms/step - loss: 0.4224 - accuracy: 0.8291 - val_loss: 0.6213 - val_accuracy: 0.7091\n",
      "Epoch 111/500\n",
      "15/15 [==============================] - 0s 24ms/step - loss: 0.4285 - accuracy: 0.8141 - val_loss: 0.6076 - val_accuracy: 0.6909\n",
      "Epoch 112/500\n",
      "15/15 [==============================] - 0s 25ms/step - loss: 0.4238 - accuracy: 0.8269 - val_loss: 0.6213 - val_accuracy: 0.7091\n",
      "Epoch 113/500\n",
      "15/15 [==============================] - 0s 32ms/step - loss: 0.4217 - accuracy: 0.8205 - val_loss: 0.6129 - val_accuracy: 0.6909\n",
      "Epoch 114/500\n",
      "15/15 [==============================] - 0s 30ms/step - loss: 0.4128 - accuracy: 0.8226 - val_loss: 0.6246 - val_accuracy: 0.7091\n",
      "Epoch 115/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15/15 [==============================] - 0s 30ms/step - loss: 0.4151 - accuracy: 0.8226 - val_loss: 0.6175 - val_accuracy: 0.6909\n",
      "Epoch 116/500\n",
      "15/15 [==============================] - 0s 24ms/step - loss: 0.4143 - accuracy: 0.8355 - val_loss: 0.6177 - val_accuracy: 0.6909\n",
      "Epoch 117/500\n",
      "15/15 [==============================] - 0s 25ms/step - loss: 0.4180 - accuracy: 0.8269 - val_loss: 0.6221 - val_accuracy: 0.7091\n",
      "Epoch 118/500\n",
      "15/15 [==============================] - 0s 26ms/step - loss: 0.4169 - accuracy: 0.8205 - val_loss: 0.6324 - val_accuracy: 0.6909\n",
      "Epoch 119/500\n",
      "15/15 [==============================] - 0s 26ms/step - loss: 0.4211 - accuracy: 0.8205 - val_loss: 0.6226 - val_accuracy: 0.6909\n",
      "Epoch 120/500\n",
      "15/15 [==============================] - 0s 25ms/step - loss: 0.4224 - accuracy: 0.8205 - val_loss: 0.6202 - val_accuracy: 0.6545\n",
      "Epoch 121/500\n",
      "15/15 [==============================] - 0s 31ms/step - loss: 0.4146 - accuracy: 0.8226 - val_loss: 0.6252 - val_accuracy: 0.6727\n",
      "Epoch 122/500\n",
      "15/15 [==============================] - 0s 27ms/step - loss: 0.4132 - accuracy: 0.8269 - val_loss: 0.6344 - val_accuracy: 0.7091\n",
      "Epoch 123/500\n",
      "15/15 [==============================] - 0s 27ms/step - loss: 0.4104 - accuracy: 0.8226 - val_loss: 0.6197 - val_accuracy: 0.6727\n",
      "Epoch 124/500\n",
      "15/15 [==============================] - 0s 27ms/step - loss: 0.4018 - accuracy: 0.8397 - val_loss: 0.6288 - val_accuracy: 0.7091\n",
      "Epoch 125/500\n",
      "15/15 [==============================] - 0s 29ms/step - loss: 0.4044 - accuracy: 0.8269 - val_loss: 0.6225 - val_accuracy: 0.6909\n",
      "Epoch 126/500\n",
      "15/15 [==============================] - 1s 41ms/step - loss: 0.4015 - accuracy: 0.8397 - val_loss: 0.6265 - val_accuracy: 0.6909\n",
      "Epoch 127/500\n",
      "15/15 [==============================] - 0s 28ms/step - loss: 0.4063 - accuracy: 0.8333 - val_loss: 0.6359 - val_accuracy: 0.7091\n",
      "Epoch 128/500\n",
      "15/15 [==============================] - 0s 28ms/step - loss: 0.4073 - accuracy: 0.8226 - val_loss: 0.6211 - val_accuracy: 0.6727\n",
      "Epoch 129/500\n",
      "15/15 [==============================] - 0s 27ms/step - loss: 0.4066 - accuracy: 0.8162 - val_loss: 0.6279 - val_accuracy: 0.6909\n",
      "Epoch 130/500\n",
      "15/15 [==============================] - 0s 27ms/step - loss: 0.4044 - accuracy: 0.8291 - val_loss: 0.6323 - val_accuracy: 0.6909\n",
      "Epoch 131/500\n",
      "15/15 [==============================] - 0s 27ms/step - loss: 0.4009 - accuracy: 0.8269 - val_loss: 0.6301 - val_accuracy: 0.6909\n",
      "Epoch 132/500\n",
      "15/15 [==============================] - 0s 26ms/step - loss: 0.3940 - accuracy: 0.8376 - val_loss: 0.6295 - val_accuracy: 0.6545\n",
      "Epoch 133/500\n",
      "15/15 [==============================] - 0s 27ms/step - loss: 0.3812 - accuracy: 0.8355 - val_loss: 0.6364 - val_accuracy: 0.6909\n",
      "Epoch 134/500\n",
      "15/15 [==============================] - 0s 27ms/step - loss: 0.3853 - accuracy: 0.8333 - val_loss: 0.6354 - val_accuracy: 0.6545\n",
      "Epoch 135/500\n",
      "15/15 [==============================] - 0s 28ms/step - loss: 0.3840 - accuracy: 0.8419 - val_loss: 0.6327 - val_accuracy: 0.6727\n",
      "Epoch 136/500\n",
      "15/15 [==============================] - 0s 28ms/step - loss: 0.3872 - accuracy: 0.8333 - val_loss: 0.6345 - val_accuracy: 0.6545\n",
      "Epoch 137/500\n",
      "15/15 [==============================] - 0s 29ms/step - loss: 0.3873 - accuracy: 0.8291 - val_loss: 0.6567 - val_accuracy: 0.7091\n",
      "Epoch 138/500\n",
      "15/15 [==============================] - 0s 27ms/step - loss: 0.3896 - accuracy: 0.8291 - val_loss: 0.6410 - val_accuracy: 0.6909\n",
      "Epoch 139/500\n",
      "15/15 [==============================] - 0s 26ms/step - loss: 0.3797 - accuracy: 0.8526 - val_loss: 0.6388 - val_accuracy: 0.6545\n",
      "Epoch 140/500\n",
      "15/15 [==============================] - 0s 25ms/step - loss: 0.3831 - accuracy: 0.8397 - val_loss: 0.6412 - val_accuracy: 0.6727\n",
      "Epoch 141/500\n",
      "15/15 [==============================] - 0s 24ms/step - loss: 0.3836 - accuracy: 0.8440 - val_loss: 0.6372 - val_accuracy: 0.6545\n",
      "Epoch 142/500\n",
      "15/15 [==============================] - 0s 25ms/step - loss: 0.3818 - accuracy: 0.8376 - val_loss: 0.6495 - val_accuracy: 0.6909\n",
      "Epoch 143/500\n",
      "15/15 [==============================] - 0s 26ms/step - loss: 0.3802 - accuracy: 0.8397 - val_loss: 0.6431 - val_accuracy: 0.6727\n",
      "Epoch 144/500\n",
      "15/15 [==============================] - 0s 25ms/step - loss: 0.3751 - accuracy: 0.8397 - val_loss: 0.6451 - val_accuracy: 0.6545\n",
      "Epoch 145/500\n",
      "15/15 [==============================] - 0s 26ms/step - loss: 0.3855 - accuracy: 0.8462 - val_loss: 0.6484 - val_accuracy: 0.6727\n",
      "Epoch 146/500\n",
      "15/15 [==============================] - 0s 25ms/step - loss: 0.3770 - accuracy: 0.8397 - val_loss: 0.6522 - val_accuracy: 0.6727\n",
      "Epoch 147/500\n",
      "15/15 [==============================] - 0s 25ms/step - loss: 0.3793 - accuracy: 0.8291 - val_loss: 0.6453 - val_accuracy: 0.6545\n",
      "Epoch 148/500\n",
      "15/15 [==============================] - 0s 25ms/step - loss: 0.3861 - accuracy: 0.8333 - val_loss: 0.6570 - val_accuracy: 0.6909\n",
      "Epoch 149/500\n",
      "15/15 [==============================] - 0s 26ms/step - loss: 0.3850 - accuracy: 0.8333 - val_loss: 0.6504 - val_accuracy: 0.6727\n",
      "Epoch 150/500\n",
      "15/15 [==============================] - 0s 27ms/step - loss: 0.3801 - accuracy: 0.8419 - val_loss: 0.6571 - val_accuracy: 0.6727\n",
      "Epoch 151/500\n",
      "15/15 [==============================] - 0s 26ms/step - loss: 0.3782 - accuracy: 0.8440 - val_loss: 0.6558 - val_accuracy: 0.6545\n",
      "Epoch 152/500\n",
      "15/15 [==============================] - 0s 26ms/step - loss: 0.3680 - accuracy: 0.8590 - val_loss: 0.6591 - val_accuracy: 0.6909\n",
      "Epoch 153/500\n",
      "15/15 [==============================] - 0s 27ms/step - loss: 0.3640 - accuracy: 0.8483 - val_loss: 0.6558 - val_accuracy: 0.6545\n",
      "Epoch 154/500\n",
      "15/15 [==============================] - 0s 25ms/step - loss: 0.3634 - accuracy: 0.8611 - val_loss: 0.6571 - val_accuracy: 0.6727\n",
      "Epoch 155/500\n",
      "15/15 [==============================] - 0s 25ms/step - loss: 0.3512 - accuracy: 0.8568 - val_loss: 0.6626 - val_accuracy: 0.6727\n",
      "Epoch 156/500\n",
      "15/15 [==============================] - 0s 27ms/step - loss: 0.3621 - accuracy: 0.8419 - val_loss: 0.6653 - val_accuracy: 0.6727\n",
      "Epoch 157/500\n",
      "15/15 [==============================] - 0s 31ms/step - loss: 0.3567 - accuracy: 0.8632 - val_loss: 0.6658 - val_accuracy: 0.6909\n",
      "Epoch 158/500\n",
      "15/15 [==============================] - 1s 37ms/step - loss: 0.3550 - accuracy: 0.8611 - val_loss: 0.6694 - val_accuracy: 0.6727\n",
      "Epoch 159/500\n",
      "15/15 [==============================] - 0s 31ms/step - loss: 0.3598 - accuracy: 0.8547 - val_loss: 0.6737 - val_accuracy: 0.6909\n",
      "Epoch 160/500\n",
      "15/15 [==============================] - 1s 37ms/step - loss: 0.3645 - accuracy: 0.8526 - val_loss: 0.6641 - val_accuracy: 0.6364\n",
      "Epoch 161/500\n",
      "15/15 [==============================] - 0s 33ms/step - loss: 0.3592 - accuracy: 0.8376 - val_loss: 0.6765 - val_accuracy: 0.6727\n",
      "Epoch 162/500\n",
      "15/15 [==============================] - 1s 43ms/step - loss: 0.3570 - accuracy: 0.8568 - val_loss: 0.6697 - val_accuracy: 0.6364\n",
      "Epoch 163/500\n",
      "15/15 [==============================] - 0s 33ms/step - loss: 0.3564 - accuracy: 0.8590 - val_loss: 0.6749 - val_accuracy: 0.6909\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fa441998cd0>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_1.compile(optimizer=opt1, \n",
    "                loss='categorical_crossentropy', \n",
    "                metrics=['accuracy'])\n",
    "#Here we use cross-entropy as the criteria for loss.\n",
    "model_1.fit(X_train, y_train, \n",
    "            validation_data=(X_val, y_val), \n",
    "            epochs=500, verbose=True, \n",
    "            callbacks=[early_stopping_monitor])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "abca35fd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 5ms/step - loss: 0.5982 - accuracy: 0.6923\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.5793 - accuracy: 0.7273\n"
     ]
    }
   ],
   "source": [
    "m1_eval_test = model_1.evaluate(X_test, y_test)\n",
    "m1_eval_val = model_1.evaluate(X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "aaa7431e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 6ms/step\n",
      "roc auc score:  0.6259398496240601\n",
      "average precision score:  0.5961695324258406\n"
     ]
    }
   ],
   "source": [
    "pred = model_1.predict(X_test)\n",
    "roc_value = roc_auc_score(y_test, pred)\n",
    "ap_score = average_precision_score(y_test, pred)\n",
    "print('roc auc score: ', roc_value)\n",
    "print('average precision score: ', ap_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "db30c8c7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.18998067, 0.8100193 ],\n",
       "       [0.17546584, 0.8245342 ],\n",
       "       [0.389724  , 0.610276  ],\n",
       "       [0.63259715, 0.3674028 ],\n",
       "       [0.5275844 , 0.47241572],\n",
       "       [0.58354455, 0.4164555 ],\n",
       "       [0.27969486, 0.7203052 ],\n",
       "       [0.44592366, 0.5540764 ],\n",
       "       [0.2682963 , 0.7317037 ],\n",
       "       [0.2268041 , 0.7731959 ],\n",
       "       [0.24433485, 0.7556652 ],\n",
       "       [0.2723401 , 0.72765994],\n",
       "       [0.28753412, 0.71246594],\n",
       "       [0.11288755, 0.88711244],\n",
       "       [0.46026534, 0.53973466],\n",
       "       [0.15257062, 0.8474294 ],\n",
       "       [0.18056458, 0.8194354 ],\n",
       "       [0.16823608, 0.8317639 ],\n",
       "       [0.39217192, 0.60782814],\n",
       "       [0.43647423, 0.5635258 ],\n",
       "       [0.4224828 , 0.5775173 ],\n",
       "       [0.2053162 , 0.79468375],\n",
       "       [0.22911114, 0.7708888 ],\n",
       "       [0.3429843 , 0.6570157 ],\n",
       "       [0.17773196, 0.82226807],\n",
       "       [0.83191013, 0.1680899 ],\n",
       "       [0.13548681, 0.86451316],\n",
       "       [0.07943593, 0.92056406],\n",
       "       [0.13682348, 0.86317647],\n",
       "       [0.41709983, 0.58290017],\n",
       "       [0.6897583 , 0.31024173],\n",
       "       [0.36096612, 0.63903385],\n",
       "       [0.6005804 , 0.39941964],\n",
       "       [0.22897415, 0.7710258 ],\n",
       "       [0.08152577, 0.91847426],\n",
       "       [0.11129761, 0.88870245],\n",
       "       [0.32567465, 0.6743253 ],\n",
       "       [0.36995524, 0.63004476],\n",
       "       [0.20556341, 0.79443663],\n",
       "       [0.36754867, 0.63245136],\n",
       "       [0.18421534, 0.8157847 ],\n",
       "       [0.21679695, 0.7832031 ],\n",
       "       [0.64290726, 0.35709277],\n",
       "       [0.5924852 , 0.40751484],\n",
       "       [0.47337312, 0.5266269 ],\n",
       "       [0.41409698, 0.58590305],\n",
       "       [0.09414499, 0.90585506],\n",
       "       [0.07786958, 0.92213047],\n",
       "       [0.14203027, 0.85796976],\n",
       "       [0.05330012, 0.94669986],\n",
       "       [0.3543367 , 0.64566326],\n",
       "       [0.26648214, 0.7335178 ]], dtype=float32)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "cacdd11e",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = pred\n",
    "y_c = (y_pred > 0.5).astype(\"int32\")\n",
    "y_test_np = y_test.to_numpy()\n",
    "y_test_np = y_test_np.astype('int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e8e84f78",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1],\n",
       "       [0, 1],\n",
       "       [0, 1],\n",
       "       [1, 0],\n",
       "       [1, 0],\n",
       "       [1, 0],\n",
       "       [0, 1],\n",
       "       [0, 1],\n",
       "       [0, 1],\n",
       "       [0, 1],\n",
       "       [0, 1],\n",
       "       [0, 1],\n",
       "       [0, 1],\n",
       "       [0, 1],\n",
       "       [0, 1],\n",
       "       [0, 1],\n",
       "       [0, 1],\n",
       "       [0, 1],\n",
       "       [0, 1],\n",
       "       [0, 1],\n",
       "       [0, 1],\n",
       "       [0, 1],\n",
       "       [0, 1],\n",
       "       [0, 1],\n",
       "       [0, 1],\n",
       "       [1, 0],\n",
       "       [0, 1],\n",
       "       [0, 1],\n",
       "       [0, 1],\n",
       "       [0, 1],\n",
       "       [1, 0],\n",
       "       [0, 1],\n",
       "       [1, 0],\n",
       "       [0, 1],\n",
       "       [0, 1],\n",
       "       [0, 1],\n",
       "       [0, 1],\n",
       "       [0, 1],\n",
       "       [0, 1],\n",
       "       [0, 1],\n",
       "       [0, 1],\n",
       "       [0, 1],\n",
       "       [1, 0],\n",
       "       [1, 0],\n",
       "       [0, 1],\n",
       "       [0, 1],\n",
       "       [0, 1],\n",
       "       [0, 1],\n",
       "       [0, 1],\n",
       "       [0, 1],\n",
       "       [0, 1],\n",
       "       [0, 1]], dtype=int32)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "9742f8e6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1],\n",
       "       [0, 1],\n",
       "       [1, 0],\n",
       "       [0, 1],\n",
       "       [1, 0],\n",
       "       [0, 1],\n",
       "       [0, 1],\n",
       "       [1, 0],\n",
       "       [0, 1],\n",
       "       [0, 1],\n",
       "       [0, 1],\n",
       "       [0, 1],\n",
       "       [0, 1],\n",
       "       [0, 1],\n",
       "       [0, 1],\n",
       "       [0, 1],\n",
       "       [0, 1],\n",
       "       [0, 1],\n",
       "       [0, 1],\n",
       "       [0, 1],\n",
       "       [0, 1],\n",
       "       [0, 1],\n",
       "       [0, 1],\n",
       "       [0, 1],\n",
       "       [0, 1],\n",
       "       [1, 0],\n",
       "       [0, 1],\n",
       "       [0, 1],\n",
       "       [0, 1],\n",
       "       [1, 0],\n",
       "       [0, 1],\n",
       "       [1, 0],\n",
       "       [0, 1],\n",
       "       [1, 0],\n",
       "       [0, 1],\n",
       "       [0, 1],\n",
       "       [1, 0],\n",
       "       [1, 0],\n",
       "       [0, 1],\n",
       "       [0, 1],\n",
       "       [0, 1],\n",
       "       [0, 1],\n",
       "       [0, 1],\n",
       "       [1, 0],\n",
       "       [1, 0],\n",
       "       [0, 1],\n",
       "       [0, 1],\n",
       "       [1, 0],\n",
       "       [1, 0],\n",
       "       [1, 0],\n",
       "       [0, 1],\n",
       "       [0, 1]], dtype=int32)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "2d331052",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 7ms/step - loss: 0.5982 - accuracy: 0.6923\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "cf_matrix = confusion_matrix(y_test_np.argmax(axis=1), y_c.argmax(axis=1)) \n",
    "#cf_matrix = confusion_matrix(y_test_np[:, 1], y_c[:, 1]) \n",
    "\n",
    "ax = sns.heatmap(cf_matrix, annot=True, cmap='Blues')\n",
    "\n",
    "ax.set_title('Seaborn Confusion Matrix with labels\\n\\n');\n",
    "ax.set_xlabel('\\nPredicted Values')\n",
    "ax.set_ylabel('Actual Values ');\n",
    "\n",
    "## Ticket labels - List must be in alphabetical order\n",
    "ax.xaxis.set_ticklabels(['Progressor','Non-Progressor'])\n",
    "ax.yaxis.set_ticklabels(['Progressor','Non-Progressor'])\n",
    "\n",
    "## Display the visualization of the Confusion Matrix.\n",
    "plt.savefig('M1_GRI_test.png')\n",
    "m1_eval_test = model_1.evaluate(X_test, y_test)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53d60a52",
   "metadata": {},
   "source": [
    "**In fact, we can use 'y_test_np[:,1]' instead of 'argmax'!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b99ec281",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0,\n",
       "       0, 1, 1, 0, 0, 0, 1, 1])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baitest_y = y_test_np.argmax(axis=1)\n",
    "baitest_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "10a9ddbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(baitest_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b8345d0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0,\n",
       "       0, 1, 1, 0, 0, 0, 1, 1], dtype=int32)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baitest = y_test_np[:,1]\n",
    "baitest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a766f3fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(baitest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "303cf03b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array_equal(baitest, baitest_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d729f90e",
   "metadata": {},
   "source": [
    "**HELP on argmax function.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "005fc195",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function argmax in module numpy:\n",
      "\n",
      "argmax(a, axis=None, out=None, *, keepdims=<no value>)\n",
      "    Returns the indices of the maximum values along an axis.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    a : array_like\n",
      "        Input array.\n",
      "    axis : int, optional\n",
      "        By default, the index is into the flattened array, otherwise\n",
      "        along the specified axis.\n",
      "    out : array, optional\n",
      "        If provided, the result will be inserted into this array. It should\n",
      "        be of the appropriate shape and dtype.\n",
      "    keepdims : bool, optional\n",
      "        If this is set to True, the axes which are reduced are left\n",
      "        in the result as dimensions with size one. With this option,\n",
      "        the result will broadcast correctly against the array.\n",
      "    \n",
      "        .. versionadded:: 1.22.0\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    index_array : ndarray of ints\n",
      "        Array of indices into the array. It has the same shape as `a.shape`\n",
      "        with the dimension along `axis` removed. If `keepdims` is set to True,\n",
      "        then the size of `axis` will be 1 with the resulting array having same\n",
      "        shape as `a.shape`.\n",
      "    \n",
      "    See Also\n",
      "    --------\n",
      "    ndarray.argmax, argmin\n",
      "    amax : The maximum value along a given axis.\n",
      "    unravel_index : Convert a flat index into an index tuple.\n",
      "    take_along_axis : Apply ``np.expand_dims(index_array, axis)``\n",
      "                      from argmax to an array as if by calling max.\n",
      "    \n",
      "    Notes\n",
      "    -----\n",
      "    In case of multiple occurrences of the maximum values, the indices\n",
      "    corresponding to the first occurrence are returned.\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    >>> a = np.arange(6).reshape(2,3) + 10\n",
      "    >>> a\n",
      "    array([[10, 11, 12],\n",
      "           [13, 14, 15]])\n",
      "    >>> np.argmax(a)\n",
      "    5\n",
      "    >>> np.argmax(a, axis=0)\n",
      "    array([1, 1, 1])\n",
      "    >>> np.argmax(a, axis=1)\n",
      "    array([2, 2])\n",
      "    \n",
      "    Indexes of the maximal elements of a N-dimensional array:\n",
      "    \n",
      "    >>> ind = np.unravel_index(np.argmax(a, axis=None), a.shape)\n",
      "    >>> ind\n",
      "    (1, 2)\n",
      "    >>> a[ind]\n",
      "    15\n",
      "    \n",
      "    >>> b = np.arange(6)\n",
      "    >>> b[1] = 5\n",
      "    >>> b\n",
      "    array([0, 5, 2, 3, 4, 5])\n",
      "    >>> np.argmax(b)  # Only the first occurrence is returned.\n",
      "    1\n",
      "    \n",
      "    >>> x = np.array([[4,2,3], [1,0,3]])\n",
      "    >>> index_array = np.argmax(x, axis=-1)\n",
      "    >>> # Same as np.amax(x, axis=-1, keepdims=True)\n",
      "    >>> np.take_along_axis(x, np.expand_dims(index_array, axis=-1), axis=-1)\n",
      "    array([[4],\n",
      "           [3]])\n",
      "    >>> # Same as np.amax(x, axis=-1)\n",
      "    >>> np.take_along_axis(x, np.expand_dims(index_array, axis=-1), axis=-1).squeeze(axis=-1)\n",
      "    array([4, 3])\n",
      "    \n",
      "    Setting `keepdims` to `True`,\n",
      "    \n",
      "    >>> x = np.arange(24).reshape((2, 3, 4))\n",
      "    >>> res = np.argmax(x, axis=1, keepdims=True)\n",
      "    >>> res.shape\n",
      "    (2, 1, 4)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(np.argmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "1f61dc2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[10, 20, 12],\n",
       "       [ 9, 14, 15]])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.arange(6).reshape(2,3) + 10\n",
    "a[0,1]=20\n",
    "a[1,0]=9\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "9825ac77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.argmax(axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "d888338f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.argmax(axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20bab5bd",
   "metadata": {},
   "source": [
    "so we can see 'argmax' is actually used to find out the index of largest element from each axis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0662ce3",
   "metadata": {},
   "source": [
    "**Help End**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "464a194c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dad2a39",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "81e7d0c1",
   "metadata": {},
   "source": [
    "### CNN model for resampled dataset (NEW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "296c64df",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv1d_1 (Conv1D)           (None, 766, 64)           256       \n",
      "                                                                 \n",
      " max_pooling1d_1 (MaxPooling  (None, 255, 64)          0         \n",
      " 1D)                                                             \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 255, 64)           0         \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 16320)             0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 64)                1044544   \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 16)                1040      \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 2)                 34        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,045,874\n",
      "Trainable params: 1,045,874\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#create model2\n",
    "model_2 = Sequential()\n",
    "\n",
    "#add layers\n",
    "model_2.add(Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(768,1)))\n",
    "model_2.add(MaxPooling1D(pool_size=3))\n",
    "# model_1.add(Conv1D(filters=32, kernel_size=3, activation='relu'))\n",
    "model_2.add(Dropout(0.5))\n",
    "# model_1.add(MaxPooling1D(pool_size=2))\n",
    "model_2.add(Flatten())\n",
    "model_2.add(Dense(64, activation='relu'))\n",
    "model_2.add(Dense(16, activation='relu'))\n",
    "model_2.add(Dense(2, activation='softmax'))\n",
    "model_2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "87ec2a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "early_stopping_monitor = EarlyStopping(\n",
    "    monitor='val_accuracy',\n",
    "    min_delta=0,\n",
    "    patience=100,\n",
    "    verbose=0,\n",
    "    mode='auto',\n",
    "    baseline=None,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "opt1 = keras.optimizers.Adam(learning_rate = 1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "64f64a26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "21/21 [==============================] - 1s 34ms/step - loss: 0.6948 - accuracy: 0.4940 - val_loss: 0.6651 - val_accuracy: 0.6909\n",
      "Epoch 2/500\n",
      "21/21 [==============================] - 1s 26ms/step - loss: 0.6923 - accuracy: 0.5000 - val_loss: 0.6725 - val_accuracy: 0.6909\n",
      "Epoch 3/500\n",
      "21/21 [==============================] - 1s 27ms/step - loss: 0.6899 - accuracy: 0.5015 - val_loss: 0.6695 - val_accuracy: 0.6909\n",
      "Epoch 4/500\n",
      "21/21 [==============================] - 1s 27ms/step - loss: 0.6891 - accuracy: 0.5589 - val_loss: 0.6637 - val_accuracy: 0.6909\n",
      "Epoch 5/500\n",
      "21/21 [==============================] - 1s 27ms/step - loss: 0.6859 - accuracy: 0.5181 - val_loss: 0.6699 - val_accuracy: 0.7091\n",
      "Epoch 6/500\n",
      "21/21 [==============================] - 1s 26ms/step - loss: 0.6826 - accuracy: 0.5604 - val_loss: 0.6769 - val_accuracy: 0.5818\n",
      "Epoch 7/500\n",
      "21/21 [==============================] - 1s 28ms/step - loss: 0.6779 - accuracy: 0.5861 - val_loss: 0.6667 - val_accuracy: 0.6182\n",
      "Epoch 8/500\n",
      "21/21 [==============================] - 1s 26ms/step - loss: 0.6733 - accuracy: 0.6344 - val_loss: 0.6505 - val_accuracy: 0.6364\n",
      "Epoch 9/500\n",
      "21/21 [==============================] - 1s 27ms/step - loss: 0.6667 - accuracy: 0.5906 - val_loss: 0.6748 - val_accuracy: 0.5818\n",
      "Epoch 10/500\n",
      "21/21 [==============================] - 1s 28ms/step - loss: 0.6608 - accuracy: 0.6254 - val_loss: 0.6197 - val_accuracy: 0.6727\n",
      "Epoch 11/500\n",
      "21/21 [==============================] - 1s 26ms/step - loss: 0.6574 - accuracy: 0.6224 - val_loss: 0.6598 - val_accuracy: 0.6000\n",
      "Epoch 12/500\n",
      "21/21 [==============================] - 1s 27ms/step - loss: 0.6569 - accuracy: 0.6254 - val_loss: 0.6739 - val_accuracy: 0.5636\n",
      "Epoch 13/500\n",
      "21/21 [==============================] - 1s 27ms/step - loss: 0.6452 - accuracy: 0.6511 - val_loss: 0.6759 - val_accuracy: 0.5273\n",
      "Epoch 14/500\n",
      "21/21 [==============================] - 1s 29ms/step - loss: 0.6381 - accuracy: 0.6405 - val_loss: 0.6166 - val_accuracy: 0.6364\n",
      "Epoch 15/500\n",
      "21/21 [==============================] - 1s 28ms/step - loss: 0.6406 - accuracy: 0.6375 - val_loss: 0.6659 - val_accuracy: 0.5455\n",
      "Epoch 16/500\n",
      "21/21 [==============================] - 1s 28ms/step - loss: 0.6342 - accuracy: 0.6586 - val_loss: 0.6470 - val_accuracy: 0.6000\n",
      "Epoch 17/500\n",
      "21/21 [==============================] - 1s 28ms/step - loss: 0.6292 - accuracy: 0.6601 - val_loss: 0.6211 - val_accuracy: 0.6182\n",
      "Epoch 18/500\n",
      "21/21 [==============================] - 1s 30ms/step - loss: 0.6203 - accuracy: 0.6647 - val_loss: 0.6665 - val_accuracy: 0.5818\n",
      "Epoch 19/500\n",
      "21/21 [==============================] - 1s 32ms/step - loss: 0.6250 - accuracy: 0.6511 - val_loss: 0.6385 - val_accuracy: 0.6000\n",
      "Epoch 20/500\n",
      "21/21 [==============================] - 1s 28ms/step - loss: 0.6156 - accuracy: 0.6782 - val_loss: 0.6685 - val_accuracy: 0.6000\n",
      "Epoch 21/500\n",
      "21/21 [==============================] - 1s 27ms/step - loss: 0.6111 - accuracy: 0.6798 - val_loss: 0.6195 - val_accuracy: 0.6182\n",
      "Epoch 22/500\n",
      "21/21 [==============================] - 1s 31ms/step - loss: 0.6067 - accuracy: 0.6858 - val_loss: 0.6337 - val_accuracy: 0.6000\n",
      "Epoch 23/500\n",
      "21/21 [==============================] - 1s 30ms/step - loss: 0.6059 - accuracy: 0.6722 - val_loss: 0.6476 - val_accuracy: 0.6000\n",
      "Epoch 24/500\n",
      "21/21 [==============================] - 1s 29ms/step - loss: 0.6039 - accuracy: 0.6662 - val_loss: 0.6203 - val_accuracy: 0.6182\n",
      "Epoch 25/500\n",
      "21/21 [==============================] - 1s 28ms/step - loss: 0.5988 - accuracy: 0.6722 - val_loss: 0.7067 - val_accuracy: 0.5091\n",
      "Epoch 26/500\n",
      "21/21 [==============================] - 1s 29ms/step - loss: 0.5969 - accuracy: 0.6828 - val_loss: 0.6134 - val_accuracy: 0.6364\n",
      "Epoch 27/500\n",
      "21/21 [==============================] - 1s 31ms/step - loss: 0.5868 - accuracy: 0.6964 - val_loss: 0.6328 - val_accuracy: 0.6000\n",
      "Epoch 28/500\n",
      "21/21 [==============================] - 1s 28ms/step - loss: 0.5797 - accuracy: 0.7024 - val_loss: 0.6450 - val_accuracy: 0.6000\n",
      "Epoch 29/500\n",
      "21/21 [==============================] - 1s 29ms/step - loss: 0.5827 - accuracy: 0.6934 - val_loss: 0.6486 - val_accuracy: 0.5818\n",
      "Epoch 30/500\n",
      "21/21 [==============================] - 1s 38ms/step - loss: 0.5727 - accuracy: 0.6979 - val_loss: 0.6319 - val_accuracy: 0.6000\n",
      "Epoch 31/500\n",
      "21/21 [==============================] - 1s 30ms/step - loss: 0.5662 - accuracy: 0.7085 - val_loss: 0.6339 - val_accuracy: 0.6182\n",
      "Epoch 32/500\n",
      "21/21 [==============================] - 1s 29ms/step - loss: 0.5754 - accuracy: 0.7039 - val_loss: 0.6694 - val_accuracy: 0.6364\n",
      "Epoch 33/500\n",
      "21/21 [==============================] - 1s 27ms/step - loss: 0.5671 - accuracy: 0.7100 - val_loss: 0.5966 - val_accuracy: 0.6727\n",
      "Epoch 34/500\n",
      "21/21 [==============================] - 1s 29ms/step - loss: 0.5652 - accuracy: 0.7100 - val_loss: 0.6002 - val_accuracy: 0.6727\n",
      "Epoch 35/500\n",
      "21/21 [==============================] - 1s 34ms/step - loss: 0.5624 - accuracy: 0.7251 - val_loss: 0.6463 - val_accuracy: 0.6182\n",
      "Epoch 36/500\n",
      "21/21 [==============================] - 1s 33ms/step - loss: 0.5579 - accuracy: 0.7069 - val_loss: 0.6231 - val_accuracy: 0.6000\n",
      "Epoch 37/500\n",
      "21/21 [==============================] - 1s 30ms/step - loss: 0.5521 - accuracy: 0.7205 - val_loss: 0.6616 - val_accuracy: 0.6364\n",
      "Epoch 38/500\n",
      "21/21 [==============================] - 1s 31ms/step - loss: 0.5446 - accuracy: 0.7402 - val_loss: 0.6207 - val_accuracy: 0.6182\n",
      "Epoch 39/500\n",
      "21/21 [==============================] - 1s 33ms/step - loss: 0.5419 - accuracy: 0.7296 - val_loss: 0.6095 - val_accuracy: 0.6182\n",
      "Epoch 40/500\n",
      "21/21 [==============================] - 1s 32ms/step - loss: 0.5403 - accuracy: 0.7432 - val_loss: 0.6224 - val_accuracy: 0.6182\n",
      "Epoch 41/500\n",
      "21/21 [==============================] - 1s 35ms/step - loss: 0.5364 - accuracy: 0.7281 - val_loss: 0.6418 - val_accuracy: 0.5636\n",
      "Epoch 42/500\n",
      "21/21 [==============================] - 1s 33ms/step - loss: 0.5315 - accuracy: 0.7311 - val_loss: 0.6083 - val_accuracy: 0.6545\n",
      "Epoch 43/500\n",
      "21/21 [==============================] - 1s 35ms/step - loss: 0.5319 - accuracy: 0.7462 - val_loss: 0.6317 - val_accuracy: 0.6000\n",
      "Epoch 44/500\n",
      "21/21 [==============================] - 1s 33ms/step - loss: 0.5399 - accuracy: 0.7311 - val_loss: 0.6578 - val_accuracy: 0.6182\n",
      "Epoch 45/500\n",
      "21/21 [==============================] - 1s 35ms/step - loss: 0.5241 - accuracy: 0.7417 - val_loss: 0.6475 - val_accuracy: 0.5818\n",
      "Epoch 46/500\n",
      "21/21 [==============================] - 1s 39ms/step - loss: 0.5228 - accuracy: 0.7477 - val_loss: 0.6223 - val_accuracy: 0.6545\n",
      "Epoch 47/500\n",
      "21/21 [==============================] - 1s 34ms/step - loss: 0.5194 - accuracy: 0.7462 - val_loss: 0.6282 - val_accuracy: 0.6182\n",
      "Epoch 48/500\n",
      "21/21 [==============================] - 1s 31ms/step - loss: 0.5116 - accuracy: 0.7553 - val_loss: 0.6299 - val_accuracy: 0.6182\n",
      "Epoch 49/500\n",
      "21/21 [==============================] - 1s 31ms/step - loss: 0.5112 - accuracy: 0.7583 - val_loss: 0.6181 - val_accuracy: 0.6545\n",
      "Epoch 50/500\n",
      "21/21 [==============================] - 1s 34ms/step - loss: 0.5063 - accuracy: 0.7583 - val_loss: 0.6176 - val_accuracy: 0.6909\n",
      "Epoch 51/500\n",
      "21/21 [==============================] - 1s 33ms/step - loss: 0.5121 - accuracy: 0.7447 - val_loss: 0.6100 - val_accuracy: 0.6909\n",
      "Epoch 52/500\n",
      "21/21 [==============================] - 1s 34ms/step - loss: 0.5040 - accuracy: 0.7613 - val_loss: 0.6182 - val_accuracy: 0.6545\n",
      "Epoch 53/500\n",
      "21/21 [==============================] - 1s 35ms/step - loss: 0.4965 - accuracy: 0.7734 - val_loss: 0.6123 - val_accuracy: 0.6909\n",
      "Epoch 54/500\n",
      "21/21 [==============================] - 1s 34ms/step - loss: 0.4939 - accuracy: 0.7628 - val_loss: 0.6224 - val_accuracy: 0.6545\n",
      "Epoch 55/500\n",
      "21/21 [==============================] - 1s 33ms/step - loss: 0.4921 - accuracy: 0.7644 - val_loss: 0.6485 - val_accuracy: 0.6000\n",
      "Epoch 56/500\n",
      "21/21 [==============================] - 1s 33ms/step - loss: 0.4817 - accuracy: 0.7704 - val_loss: 0.6609 - val_accuracy: 0.5818\n",
      "Epoch 57/500\n",
      "21/21 [==============================] - 1s 32ms/step - loss: 0.4898 - accuracy: 0.7674 - val_loss: 0.7001 - val_accuracy: 0.5455\n",
      "Epoch 58/500\n",
      "21/21 [==============================] - 1s 34ms/step - loss: 0.4855 - accuracy: 0.7644 - val_loss: 0.6147 - val_accuracy: 0.6909\n",
      "Epoch 59/500\n",
      "21/21 [==============================] - 1s 31ms/step - loss: 0.4789 - accuracy: 0.7749 - val_loss: 0.6390 - val_accuracy: 0.6364\n",
      "Epoch 60/500\n",
      "21/21 [==============================] - 1s 34ms/step - loss: 0.4799 - accuracy: 0.7613 - val_loss: 0.6159 - val_accuracy: 0.6909\n",
      "Epoch 61/500\n",
      "21/21 [==============================] - 1s 33ms/step - loss: 0.4742 - accuracy: 0.7689 - val_loss: 0.6239 - val_accuracy: 0.6909\n",
      "Epoch 62/500\n",
      "21/21 [==============================] - 1s 38ms/step - loss: 0.4680 - accuracy: 0.7779 - val_loss: 0.6232 - val_accuracy: 0.6909\n",
      "Epoch 63/500\n",
      "21/21 [==============================] - 1s 33ms/step - loss: 0.4671 - accuracy: 0.7734 - val_loss: 0.6459 - val_accuracy: 0.6545\n",
      "Epoch 64/500\n",
      "21/21 [==============================] - 1s 31ms/step - loss: 0.4703 - accuracy: 0.7749 - val_loss: 0.7077 - val_accuracy: 0.5455\n",
      "Epoch 65/500\n",
      "21/21 [==============================] - 1s 43ms/step - loss: 0.4552 - accuracy: 0.7870 - val_loss: 0.6702 - val_accuracy: 0.6182\n",
      "Epoch 66/500\n",
      "21/21 [==============================] - 1s 36ms/step - loss: 0.4679 - accuracy: 0.7644 - val_loss: 0.6554 - val_accuracy: 0.6364\n",
      "Epoch 67/500\n",
      "21/21 [==============================] - 1s 31ms/step - loss: 0.4572 - accuracy: 0.7749 - val_loss: 0.6681 - val_accuracy: 0.6364\n",
      "Epoch 68/500\n",
      "21/21 [==============================] - 1s 34ms/step - loss: 0.4458 - accuracy: 0.8127 - val_loss: 0.6453 - val_accuracy: 0.6364\n",
      "Epoch 69/500\n",
      "21/21 [==============================] - 1s 37ms/step - loss: 0.4535 - accuracy: 0.7931 - val_loss: 0.6480 - val_accuracy: 0.6364\n",
      "Epoch 70/500\n",
      "21/21 [==============================] - 1s 32ms/step - loss: 0.4417 - accuracy: 0.8051 - val_loss: 0.6336 - val_accuracy: 0.6909\n",
      "Epoch 71/500\n",
      "21/21 [==============================] - 1s 32ms/step - loss: 0.4348 - accuracy: 0.8036 - val_loss: 0.6535 - val_accuracy: 0.6545\n",
      "Epoch 72/500\n",
      "21/21 [==============================] - 1s 31ms/step - loss: 0.4411 - accuracy: 0.8051 - val_loss: 0.6502 - val_accuracy: 0.6727\n",
      "Epoch 73/500\n",
      "21/21 [==============================] - 1s 31ms/step - loss: 0.4386 - accuracy: 0.8021 - val_loss: 0.6374 - val_accuracy: 0.6909\n",
      "Epoch 74/500\n",
      "21/21 [==============================] - 1s 32ms/step - loss: 0.4322 - accuracy: 0.8036 - val_loss: 0.6558 - val_accuracy: 0.6364\n",
      "Epoch 75/500\n",
      "21/21 [==============================] - 1s 35ms/step - loss: 0.4247 - accuracy: 0.8157 - val_loss: 0.6672 - val_accuracy: 0.6364\n",
      "Epoch 76/500\n",
      "21/21 [==============================] - 1s 33ms/step - loss: 0.4303 - accuracy: 0.8097 - val_loss: 0.6406 - val_accuracy: 0.6909\n",
      "Epoch 77/500\n",
      "21/21 [==============================] - 1s 38ms/step - loss: 0.4330 - accuracy: 0.7991 - val_loss: 0.6566 - val_accuracy: 0.6727\n",
      "Epoch 78/500\n",
      "21/21 [==============================] - 1s 40ms/step - loss: 0.4307 - accuracy: 0.8127 - val_loss: 0.6585 - val_accuracy: 0.6364\n",
      "Epoch 79/500\n",
      "21/21 [==============================] - 1s 44ms/step - loss: 0.4260 - accuracy: 0.8112 - val_loss: 0.6805 - val_accuracy: 0.6364\n",
      "Epoch 80/500\n",
      "21/21 [==============================] - 1s 38ms/step - loss: 0.4166 - accuracy: 0.8157 - val_loss: 0.6619 - val_accuracy: 0.6909\n",
      "Epoch 81/500\n",
      "21/21 [==============================] - 1s 37ms/step - loss: 0.4102 - accuracy: 0.8172 - val_loss: 0.6662 - val_accuracy: 0.6364\n",
      "Epoch 82/500\n",
      "21/21 [==============================] - 1s 33ms/step - loss: 0.4153 - accuracy: 0.8233 - val_loss: 0.6532 - val_accuracy: 0.6545\n",
      "Epoch 83/500\n",
      "21/21 [==============================] - 1s 38ms/step - loss: 0.4082 - accuracy: 0.8142 - val_loss: 0.6793 - val_accuracy: 0.6909\n",
      "Epoch 84/500\n",
      "21/21 [==============================] - 1s 49ms/step - loss: 0.4070 - accuracy: 0.8233 - val_loss: 0.6672 - val_accuracy: 0.6364\n",
      "Epoch 85/500\n",
      "21/21 [==============================] - 1s 41ms/step - loss: 0.4119 - accuracy: 0.8082 - val_loss: 0.6903 - val_accuracy: 0.6545\n",
      "Epoch 86/500\n",
      "21/21 [==============================] - 1s 48ms/step - loss: 0.4062 - accuracy: 0.8112 - val_loss: 0.6772 - val_accuracy: 0.6545\n",
      "Epoch 87/500\n",
      "21/21 [==============================] - 1s 45ms/step - loss: 0.3986 - accuracy: 0.8278 - val_loss: 0.6903 - val_accuracy: 0.6545\n",
      "Epoch 88/500\n",
      "21/21 [==============================] - 1s 34ms/step - loss: 0.4029 - accuracy: 0.8202 - val_loss: 0.6658 - val_accuracy: 0.6727\n",
      "Epoch 89/500\n",
      "21/21 [==============================] - 1s 33ms/step - loss: 0.3891 - accuracy: 0.8293 - val_loss: 0.6682 - val_accuracy: 0.6727\n",
      "Epoch 90/500\n",
      "21/21 [==============================] - 1s 36ms/step - loss: 0.3886 - accuracy: 0.8384 - val_loss: 0.6636 - val_accuracy: 0.6727\n",
      "Epoch 91/500\n",
      "21/21 [==============================] - 1s 31ms/step - loss: 0.3825 - accuracy: 0.8353 - val_loss: 0.6866 - val_accuracy: 0.6364\n",
      "Epoch 92/500\n",
      "21/21 [==============================] - 1s 39ms/step - loss: 0.3856 - accuracy: 0.8384 - val_loss: 0.6894 - val_accuracy: 0.6364\n",
      "Epoch 93/500\n",
      "21/21 [==============================] - 1s 48ms/step - loss: 0.3793 - accuracy: 0.8459 - val_loss: 0.6713 - val_accuracy: 0.6909\n",
      "Epoch 94/500\n",
      "21/21 [==============================] - 1s 31ms/step - loss: 0.3771 - accuracy: 0.8399 - val_loss: 0.6710 - val_accuracy: 0.6727\n",
      "Epoch 95/500\n",
      "21/21 [==============================] - 1s 43ms/step - loss: 0.3728 - accuracy: 0.8414 - val_loss: 0.6771 - val_accuracy: 0.6909\n",
      "Epoch 96/500\n",
      "21/21 [==============================] - 1s 36ms/step - loss: 0.3791 - accuracy: 0.8353 - val_loss: 0.6687 - val_accuracy: 0.6727\n",
      "Epoch 97/500\n",
      "21/21 [==============================] - 1s 33ms/step - loss: 0.3831 - accuracy: 0.8399 - val_loss: 0.7030 - val_accuracy: 0.6364\n",
      "Epoch 98/500\n",
      "21/21 [==============================] - 1s 32ms/step - loss: 0.3573 - accuracy: 0.8550 - val_loss: 0.6949 - val_accuracy: 0.6727\n",
      "Epoch 99/500\n",
      "21/21 [==============================] - 1s 31ms/step - loss: 0.3648 - accuracy: 0.8489 - val_loss: 0.6961 - val_accuracy: 0.6364\n",
      "Epoch 100/500\n",
      "21/21 [==============================] - 1s 33ms/step - loss: 0.3702 - accuracy: 0.8399 - val_loss: 0.6893 - val_accuracy: 0.6727\n",
      "Epoch 101/500\n",
      "21/21 [==============================] - 1s 30ms/step - loss: 0.3604 - accuracy: 0.8248 - val_loss: 0.6954 - val_accuracy: 0.6364\n",
      "Epoch 102/500\n",
      "21/21 [==============================] - 1s 31ms/step - loss: 0.3612 - accuracy: 0.8565 - val_loss: 0.7203 - val_accuracy: 0.6364\n",
      "Epoch 103/500\n",
      "21/21 [==============================] - 1s 36ms/step - loss: 0.3511 - accuracy: 0.8520 - val_loss: 0.7115 - val_accuracy: 0.6364\n",
      "Epoch 104/500\n",
      "21/21 [==============================] - 1s 27ms/step - loss: 0.3556 - accuracy: 0.8414 - val_loss: 0.7079 - val_accuracy: 0.6545\n",
      "Epoch 105/500\n",
      "21/21 [==============================] - 1s 31ms/step - loss: 0.3475 - accuracy: 0.8580 - val_loss: 0.7005 - val_accuracy: 0.6727\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fa4324153a0>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_2.compile(optimizer=opt1, \n",
    "                loss='categorical_crossentropy', \n",
    "                metrics=['accuracy'])\n",
    "#Here we use cross-entropy as the criteria for loss.\n",
    "model_2.fit(X_train_over, y_train_over, \n",
    "            validation_data=(X_val, y_val), \n",
    "            epochs=500, verbose=True, \n",
    "            callbacks=[early_stopping_monitor])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "25d8d4a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 8ms/step - loss: 0.6646 - accuracy: 0.7308\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.6699 - accuracy: 0.7091\n"
     ]
    }
   ],
   "source": [
    "m2_eval_test = model_2.evaluate(X_test, y_test)\n",
    "m2_eval_val = model_2.evaluate(X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "4d319421",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 11ms/step\n",
      "roc auc score:  0.693609022556391\n",
      "average precision score:  0.6329859294914718\n"
     ]
    }
   ],
   "source": [
    "pred = model_2.predict(X_test)\n",
    "roc_value = roc_auc_score(y_test, pred)\n",
    "ap_score = average_precision_score(y_test, pred)\n",
    "print('roc auc score: ', roc_value)\n",
    "print('average precision score: ', ap_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "af3f0215",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.4585839 , 0.54141617],\n",
       "       [0.4597717 , 0.5402283 ],\n",
       "       [0.48812565, 0.5118743 ],\n",
       "       [0.49960154, 0.5003984 ],\n",
       "       [0.50587004, 0.49412993],\n",
       "       [0.50030255, 0.49969742],\n",
       "       [0.47180012, 0.52819985],\n",
       "       [0.48705915, 0.5129408 ],\n",
       "       [0.4740945 , 0.52590543],\n",
       "       [0.48035535, 0.5196447 ],\n",
       "       [0.46373266, 0.5362673 ],\n",
       "       [0.46198708, 0.5380129 ],\n",
       "       [0.47393823, 0.52606183],\n",
       "       [0.45065898, 0.549341  ],\n",
       "       [0.49447337, 0.50552666],\n",
       "       [0.45953923, 0.5404608 ],\n",
       "       [0.45812416, 0.54187584],\n",
       "       [0.4655211 , 0.5344789 ],\n",
       "       [0.4940805 , 0.5059195 ],\n",
       "       [0.49092   , 0.50908   ],\n",
       "       [0.4870517 , 0.51294833],\n",
       "       [0.47131115, 0.5286888 ],\n",
       "       [0.48135763, 0.51864237],\n",
       "       [0.47642735, 0.5235727 ],\n",
       "       [0.4589986 , 0.5410014 ],\n",
       "       [0.4930833 , 0.5069167 ],\n",
       "       [0.46103075, 0.5389693 ],\n",
       "       [0.44115525, 0.5588448 ],\n",
       "       [0.44548094, 0.554519  ],\n",
       "       [0.50830907, 0.491691  ],\n",
       "       [0.51448774, 0.48551226],\n",
       "       [0.50718385, 0.49281618],\n",
       "       [0.50475866, 0.4952414 ],\n",
       "       [0.4751844 , 0.5248156 ],\n",
       "       [0.44472656, 0.5552734 ],\n",
       "       [0.44982776, 0.5501723 ],\n",
       "       [0.47712085, 0.5228791 ],\n",
       "       [0.483452  , 0.51654804],\n",
       "       [0.4882668 , 0.5117333 ],\n",
       "       [0.47596633, 0.52403367],\n",
       "       [0.463315  , 0.53668493],\n",
       "       [0.4539893 , 0.5460107 ],\n",
       "       [0.5103113 , 0.4896887 ],\n",
       "       [0.50135314, 0.49864683],\n",
       "       [0.49578965, 0.50421035],\n",
       "       [0.48815534, 0.5118447 ],\n",
       "       [0.4866693 , 0.5133307 ],\n",
       "       [0.476721  , 0.5232791 ],\n",
       "       [0.46856162, 0.53143835],\n",
       "       [0.43584538, 0.5641546 ],\n",
       "       [0.47857937, 0.5214206 ],\n",
       "       [0.4776929 , 0.52230716]], dtype=float32)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "5f65c213",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = pred\n",
    "y_c = (y_pred > 0.5).astype(\"int32\")\n",
    "y_test_np = y_test.to_numpy()\n",
    "y_test_np = y_test_np.astype('int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "9a937293",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1],\n",
       "       [0, 1],\n",
       "       [0, 1],\n",
       "       [0, 1],\n",
       "       [1, 0],\n",
       "       [1, 0],\n",
       "       [0, 1],\n",
       "       [0, 1],\n",
       "       [0, 1],\n",
       "       [0, 1],\n",
       "       [0, 1],\n",
       "       [0, 1],\n",
       "       [0, 1],\n",
       "       [0, 1],\n",
       "       [0, 1],\n",
       "       [0, 1],\n",
       "       [0, 1],\n",
       "       [0, 1],\n",
       "       [0, 1],\n",
       "       [0, 1],\n",
       "       [0, 1],\n",
       "       [0, 1],\n",
       "       [0, 1],\n",
       "       [0, 1],\n",
       "       [0, 1],\n",
       "       [0, 1],\n",
       "       [0, 1],\n",
       "       [0, 1],\n",
       "       [0, 1],\n",
       "       [1, 0],\n",
       "       [1, 0],\n",
       "       [1, 0],\n",
       "       [1, 0],\n",
       "       [0, 1],\n",
       "       [0, 1],\n",
       "       [0, 1],\n",
       "       [0, 1],\n",
       "       [0, 1],\n",
       "       [0, 1],\n",
       "       [0, 1],\n",
       "       [0, 1],\n",
       "       [0, 1],\n",
       "       [1, 0],\n",
       "       [1, 0],\n",
       "       [0, 1],\n",
       "       [0, 1],\n",
       "       [0, 1],\n",
       "       [0, 1],\n",
       "       [0, 1],\n",
       "       [0, 1],\n",
       "       [0, 1],\n",
       "       [0, 1]], dtype=int32)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "b25fac7e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1],\n",
       "       [0, 1],\n",
       "       [1, 0],\n",
       "       [0, 1],\n",
       "       [1, 0],\n",
       "       [0, 1],\n",
       "       [0, 1],\n",
       "       [1, 0],\n",
       "       [0, 1],\n",
       "       [0, 1],\n",
       "       [0, 1],\n",
       "       [0, 1],\n",
       "       [0, 1],\n",
       "       [0, 1],\n",
       "       [0, 1],\n",
       "       [0, 1],\n",
       "       [0, 1],\n",
       "       [0, 1],\n",
       "       [0, 1],\n",
       "       [0, 1],\n",
       "       [0, 1],\n",
       "       [0, 1],\n",
       "       [0, 1],\n",
       "       [0, 1],\n",
       "       [0, 1],\n",
       "       [1, 0],\n",
       "       [0, 1],\n",
       "       [0, 1],\n",
       "       [0, 1],\n",
       "       [1, 0],\n",
       "       [0, 1],\n",
       "       [1, 0],\n",
       "       [0, 1],\n",
       "       [1, 0],\n",
       "       [0, 1],\n",
       "       [0, 1],\n",
       "       [1, 0],\n",
       "       [1, 0],\n",
       "       [0, 1],\n",
       "       [0, 1],\n",
       "       [0, 1],\n",
       "       [0, 1],\n",
       "       [0, 1],\n",
       "       [1, 0],\n",
       "       [1, 0],\n",
       "       [0, 1],\n",
       "       [0, 1],\n",
       "       [1, 0],\n",
       "       [1, 0],\n",
       "       [1, 0],\n",
       "       [0, 1],\n",
       "       [0, 1]], dtype=int32)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "8dd71bd6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 6ms/step - loss: 0.6646 - accuracy: 0.7308\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "cf_matrix = confusion_matrix(y_test_np.argmax(axis=1), y_c.argmax(axis=1)) \n",
    "#cf_matrix = confusion_matrix(y_test_np[:, 1], y_c[:, 1]) \n",
    "\n",
    "ax = sns.heatmap(cf_matrix, annot=True, cmap='Blues')\n",
    "\n",
    "ax.set_title('Seaborn Confusion Matrix with labels\\n\\n');\n",
    "ax.set_xlabel('\\nPredicted Values')\n",
    "ax.set_ylabel('Actual Values ');\n",
    "\n",
    "## Ticket labels - List must be in alphabetical order\n",
    "ax.xaxis.set_ticklabels(['Progressor','Non-Progressor'])\n",
    "ax.yaxis.set_ticklabels(['Progressor','Non-Progressor'])\n",
    "\n",
    "## Display the visualization of the Confusion Matrix.\n",
    "plt.savefig('M1_GRI_test.png')\n",
    "m2_eval_test = model_2.evaluate(X_test, y_test)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "25b61429",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 6ms/step\n"
     ]
    }
   ],
   "source": [
    "pred = model_2.predict(X_val)\n",
    "y_pred = pred\n",
    "y_c = (y_pred > 0.47).astype(\"int32\")\n",
    "y_val_np = y_val.to_numpy()\n",
    "y_val_np = y_val_np.astype('int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "8c408f67",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.4855748 , 0.51442516],\n",
       "       [0.48473492, 0.51526505],\n",
       "       [0.4570685 , 0.5429315 ],\n",
       "       [0.44602183, 0.5539781 ],\n",
       "       [0.46082532, 0.5391747 ],\n",
       "       [0.4573168 , 0.5426832 ],\n",
       "       [0.47599363, 0.5240064 ],\n",
       "       [0.48635066, 0.5136493 ],\n",
       "       [0.46901828, 0.5309818 ],\n",
       "       [0.48618758, 0.5138125 ],\n",
       "       [0.49585724, 0.50414276],\n",
       "       [0.49576947, 0.50423044],\n",
       "       [0.4510881 , 0.5489119 ],\n",
       "       [0.46352404, 0.53647596],\n",
       "       [0.48213178, 0.5178682 ],\n",
       "       [0.47806114, 0.52193886],\n",
       "       [0.49470532, 0.50529474],\n",
       "       [0.49191302, 0.50808704],\n",
       "       [0.4934227 , 0.5065773 ],\n",
       "       [0.48249945, 0.5175005 ],\n",
       "       [0.48429966, 0.5157003 ],\n",
       "       [0.4609119 , 0.53908813],\n",
       "       [0.4499902 , 0.55000985],\n",
       "       [0.48766562, 0.51233435],\n",
       "       [0.49585798, 0.50414205],\n",
       "       [0.47796986, 0.5220302 ],\n",
       "       [0.4973011 , 0.5026989 ],\n",
       "       [0.49294904, 0.507051  ],\n",
       "       [0.49919853, 0.5008015 ],\n",
       "       [0.49294904, 0.507051  ],\n",
       "       [0.49919853, 0.5008015 ],\n",
       "       [0.47859144, 0.52140856],\n",
       "       [0.45780388, 0.54219615],\n",
       "       [0.49179465, 0.5082054 ],\n",
       "       [0.49767977, 0.5023203 ],\n",
       "       [0.47603902, 0.5239609 ],\n",
       "       [0.49830574, 0.50169426],\n",
       "       [0.49880815, 0.50119185],\n",
       "       [0.4753631 , 0.5246369 ],\n",
       "       [0.47858414, 0.5214159 ],\n",
       "       [0.4459833 , 0.55401677],\n",
       "       [0.47614682, 0.5238532 ],\n",
       "       [0.4468961 , 0.55310386],\n",
       "       [0.446636  , 0.55336404],\n",
       "       [0.45710748, 0.54289246],\n",
       "       [0.51128817, 0.48871186],\n",
       "       [0.47120133, 0.5287987 ],\n",
       "       [0.46874505, 0.531255  ],\n",
       "       [0.4624785 , 0.5375215 ],\n",
       "       [0.4589058 , 0.5410942 ],\n",
       "       [0.4697252 , 0.5302748 ],\n",
       "       [0.4516209 , 0.54837906],\n",
       "       [0.49018505, 0.5098149 ],\n",
       "       [0.48165402, 0.518346  ],\n",
       "       [0.46491638, 0.53508365]], dtype=float32)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "7b005b4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "roc auc score:  0.6393188854489165\n",
      "average precision score:  0.6311604100163684\n"
     ]
    }
   ],
   "source": [
    "roc_value = roc_auc_score(y_val, pred)\n",
    "ap_score = average_precision_score(y_val, pred)\n",
    "print('roc auc score: ', roc_value)\n",
    "print('average precision score: ', ap_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "c418a360",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 4ms/step - loss: 0.6699 - accuracy: 0.7091\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "cf_matrix = confusion_matrix(y_val_np.argmax(axis=1), y_c.argmax(axis=1)) \n",
    "#cf_matrix = confusion_matrix(y_test_np[:, 1], y_c[:, 1]) \n",
    "\n",
    "ax = sns.heatmap(cf_matrix, annot=True, cmap='Blues')\n",
    "\n",
    "ax.set_title('Seaborn Confusion Matrix with labels\\n\\n');\n",
    "ax.set_xlabel('\\nPredicted Values')\n",
    "ax.set_ylabel('Actual Values ');\n",
    "\n",
    "## Ticket labels - List must be in alphabetical order\n",
    "ax.xaxis.set_ticklabels(['Progressor','Non-Progressor'])\n",
    "ax.yaxis.set_ticklabels(['Progressor','Non-Progressor'])\n",
    "\n",
    "## Display the visualization of the Confusion Matrix.\n",
    "plt.savefig('M1_GRI_test.png')\n",
    "m2_eval_test = model_2.evaluate(X_val, y_val)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb71b14",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "139f455d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ec0ea489",
   "metadata": {},
   "source": [
    "## Using k-fold validation to train CNN (NEW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "170e8718",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "early_stopping_monitor = EarlyStopping(\n",
    "    monitor='val_accuracy',\n",
    "    min_delta=0,\n",
    "    patience=100,\n",
    "    verbose=0,\n",
    "    mode='auto',\n",
    "    baseline=None,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "opt1 = keras.optimizers.Adam(learning_rate = 1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "ebc6a836",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define a function for getting new CNN models.\n",
    "def get_model():\n",
    "    #create model\n",
    "    model = Sequential()\n",
    "\n",
    "    #add layers\n",
    "    model.add(Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(768,1)))\n",
    "    model.add(MaxPooling1D(pool_size=3))\n",
    "    # model.add(Conv1D(filters=32, kernel_size=3, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    # model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dense(16, activation='relu'))\n",
    "    model.add(Dense(2, activation='softmax'))\n",
    "    model.compile(optimizer=opt1, \n",
    "                loss='categorical_crossentropy', \n",
    "                metrics=['accuracy'])\n",
    "    #Here we use cross-entropy as the criteria for loss.\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "fa1b429a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#k-fold\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "kfold = StratifiedKFold(n_splits=5,shuffle=True,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "94a33074",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(523, 768, 1)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Join the training set and validation set together.\n",
    "X_trainval = np.concatenate((X_train, X_val), axis=0)\n",
    "X_trainval.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "59ca1f7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(523, 2)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_trainval = np.concatenate((y_train, y_val), axis = 0)\n",
    "y_trainval.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "5503fb42",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_trainval = y_trainval[:, 0] #Only keep the progressor column"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeaebcb1",
   "metadata": {},
   "source": [
    "Testing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "0bc80842",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "baitest = kfold.split(X_trainval, y_trainval)\n",
    "bailist = list(baitest)\n",
    "for i in range(5):\n",
    "    train_idx, val_idx = bailist[i]\n",
    "    print(type(train_idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "c21ae3f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(418, 768, 1)\n",
      "(418,)\n",
      "(105, 768, 1)\n",
      "(105,)\n"
     ]
    }
   ],
   "source": [
    "train_idx, val_idx = bailist[0]\n",
    "X_train_kfold = X_trainval[train_idx]\n",
    "y_train_kfold = y_trainval[train_idx]\n",
    "X_val_kfold = X_trainval[val_idx]\n",
    "y_val_kfold = y_trainval[val_idx]\n",
    "print(X_train_kfold.shape)\n",
    "print(y_train_kfold.shape)\n",
    "print(X_val_kfold.shape)\n",
    "print(y_val_kfold.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "672e836a",
   "metadata": {},
   "source": [
    "Testing end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "7d69d18e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "14/14 [==============================] - 1s 31ms/step - loss: 0.6281 - accuracy: 0.6938 - val_loss: 0.6038 - val_accuracy: 0.7048\n",
      "Epoch 2/500\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 0.5965 - accuracy: 0.7057 - val_loss: 0.6090 - val_accuracy: 0.7048\n",
      "Epoch 3/500\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 0.5895 - accuracy: 0.7057 - val_loss: 0.6014 - val_accuracy: 0.7048\n",
      "Epoch 4/500\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.5878 - accuracy: 0.7057 - val_loss: 0.6035 - val_accuracy: 0.7048\n",
      "Epoch 5/500\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.5875 - accuracy: 0.7057 - val_loss: 0.6004 - val_accuracy: 0.7048\n",
      "Epoch 6/500\n",
      "14/14 [==============================] - 0s 31ms/step - loss: 0.5876 - accuracy: 0.7057 - val_loss: 0.6036 - val_accuracy: 0.7048\n",
      "Epoch 7/500\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 0.5823 - accuracy: 0.7057 - val_loss: 0.5998 - val_accuracy: 0.7048\n",
      "Epoch 8/500\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 0.5786 - accuracy: 0.7057 - val_loss: 0.6057 - val_accuracy: 0.7048\n",
      "Epoch 9/500\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 0.5852 - accuracy: 0.7057 - val_loss: 0.5983 - val_accuracy: 0.7048\n",
      "Epoch 10/500\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 0.5819 - accuracy: 0.7057 - val_loss: 0.5957 - val_accuracy: 0.7048\n",
      "Epoch 11/500\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 0.5839 - accuracy: 0.7057 - val_loss: 0.5939 - val_accuracy: 0.7048\n",
      "Epoch 12/500\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 0.5769 - accuracy: 0.7057 - val_loss: 0.5956 - val_accuracy: 0.7048\n",
      "Epoch 13/500\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 0.5759 - accuracy: 0.7057 - val_loss: 0.6082 - val_accuracy: 0.7048\n",
      "Epoch 14/500\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 0.5772 - accuracy: 0.7057 - val_loss: 0.5960 - val_accuracy: 0.7048\n",
      "Epoch 15/500\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.5717 - accuracy: 0.7057 - val_loss: 0.5912 - val_accuracy: 0.7048\n",
      "Epoch 16/500\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.5754 - accuracy: 0.7057 - val_loss: 0.5906 - val_accuracy: 0.7048\n",
      "Epoch 17/500\n",
      "14/14 [==============================] - 0s 30ms/step - loss: 0.5728 - accuracy: 0.7057 - val_loss: 0.5920 - val_accuracy: 0.7048\n",
      "Epoch 18/500\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 0.5697 - accuracy: 0.7057 - val_loss: 0.5994 - val_accuracy: 0.7048\n",
      "Epoch 19/500\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 0.5688 - accuracy: 0.7057 - val_loss: 0.5926 - val_accuracy: 0.7048\n",
      "Epoch 20/500\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.5679 - accuracy: 0.7057 - val_loss: 0.5895 - val_accuracy: 0.7048\n",
      "Epoch 21/500\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.5671 - accuracy: 0.7057 - val_loss: 0.5969 - val_accuracy: 0.7048\n",
      "Epoch 22/500\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 0.5675 - accuracy: 0.7057 - val_loss: 0.5927 - val_accuracy: 0.7048\n",
      "Epoch 23/500\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 0.5726 - accuracy: 0.7057 - val_loss: 0.5887 - val_accuracy: 0.7048\n",
      "Epoch 24/500\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.5731 - accuracy: 0.7057 - val_loss: 0.5890 - val_accuracy: 0.7048\n",
      "Epoch 25/500\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.5654 - accuracy: 0.7081 - val_loss: 0.5949 - val_accuracy: 0.7048\n",
      "Epoch 26/500\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 0.5625 - accuracy: 0.7081 - val_loss: 0.5879 - val_accuracy: 0.7048\n",
      "Epoch 27/500\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 0.5629 - accuracy: 0.7081 - val_loss: 0.5859 - val_accuracy: 0.7048\n",
      "Epoch 28/500\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 0.5617 - accuracy: 0.7105 - val_loss: 0.5915 - val_accuracy: 0.7048\n",
      "Epoch 29/500\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 0.5603 - accuracy: 0.7105 - val_loss: 0.5877 - val_accuracy: 0.7048\n",
      "Epoch 30/500\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.5622 - accuracy: 0.7081 - val_loss: 0.5878 - val_accuracy: 0.7048\n",
      "Epoch 31/500\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 0.5605 - accuracy: 0.7105 - val_loss: 0.5871 - val_accuracy: 0.7048\n",
      "Epoch 32/500\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 0.5590 - accuracy: 0.7105 - val_loss: 0.5963 - val_accuracy: 0.7048\n",
      "Epoch 33/500\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.5616 - accuracy: 0.7105 - val_loss: 0.6008 - val_accuracy: 0.7048\n",
      "Epoch 34/500\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 0.5587 - accuracy: 0.7105 - val_loss: 0.5988 - val_accuracy: 0.7048\n",
      "Epoch 35/500\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 0.5565 - accuracy: 0.7129 - val_loss: 0.5903 - val_accuracy: 0.7048\n",
      "Epoch 36/500\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 0.5641 - accuracy: 0.7153 - val_loss: 0.5894 - val_accuracy: 0.6952\n",
      "Epoch 37/500\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.5591 - accuracy: 0.7177 - val_loss: 0.5903 - val_accuracy: 0.7048\n",
      "Epoch 38/500\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 0.5546 - accuracy: 0.7177 - val_loss: 0.5912 - val_accuracy: 0.6857\n",
      "Epoch 39/500\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 0.5544 - accuracy: 0.7249 - val_loss: 0.5968 - val_accuracy: 0.6857\n",
      "Epoch 40/500\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 0.5501 - accuracy: 0.7129 - val_loss: 0.5946 - val_accuracy: 0.7048\n",
      "Epoch 41/500\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.5468 - accuracy: 0.7153 - val_loss: 0.6033 - val_accuracy: 0.7048\n",
      "Epoch 42/500\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.5415 - accuracy: 0.7153 - val_loss: 0.5962 - val_accuracy: 0.7143\n",
      "Epoch 43/500\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 0.5437 - accuracy: 0.7249 - val_loss: 0.5993 - val_accuracy: 0.7048\n",
      "Epoch 44/500\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 0.5396 - accuracy: 0.7249 - val_loss: 0.6011 - val_accuracy: 0.7143\n",
      "Epoch 45/500\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 0.5395 - accuracy: 0.7225 - val_loss: 0.5995 - val_accuracy: 0.7238\n",
      "Epoch 46/500\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 0.5345 - accuracy: 0.7321 - val_loss: 0.6066 - val_accuracy: 0.7143\n",
      "Epoch 47/500\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 0.5325 - accuracy: 0.7225 - val_loss: 0.6063 - val_accuracy: 0.7048\n",
      "Epoch 48/500\n",
      "14/14 [==============================] - 0s 20ms/step - loss: 0.5358 - accuracy: 0.7225 - val_loss: 0.5949 - val_accuracy: 0.7048\n",
      "Epoch 49/500\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 0.5372 - accuracy: 0.7201 - val_loss: 0.5942 - val_accuracy: 0.7333\n",
      "Epoch 50/500\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 0.5369 - accuracy: 0.7249 - val_loss: 0.5955 - val_accuracy: 0.7238\n",
      "Epoch 51/500\n",
      "14/14 [==============================] - 0s 20ms/step - loss: 0.5313 - accuracy: 0.7297 - val_loss: 0.5979 - val_accuracy: 0.7238\n",
      "Epoch 52/500\n",
      "14/14 [==============================] - 0s 19ms/step - loss: 0.5368 - accuracy: 0.7321 - val_loss: 0.5966 - val_accuracy: 0.7238\n",
      "Epoch 53/500\n",
      "14/14 [==============================] - 0s 20ms/step - loss: 0.5267 - accuracy: 0.7273 - val_loss: 0.6099 - val_accuracy: 0.7238\n",
      "Epoch 54/500\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 0.5296 - accuracy: 0.7297 - val_loss: 0.6046 - val_accuracy: 0.7143\n",
      "Epoch 55/500\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 0.5273 - accuracy: 0.7273 - val_loss: 0.6016 - val_accuracy: 0.7143\n",
      "Epoch 56/500\n",
      "14/14 [==============================] - 0s 20ms/step - loss: 0.5271 - accuracy: 0.7392 - val_loss: 0.5994 - val_accuracy: 0.7143\n",
      "Epoch 57/500\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 0.5244 - accuracy: 0.7464 - val_loss: 0.6081 - val_accuracy: 0.7143\n",
      "Epoch 58/500\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 0.5240 - accuracy: 0.7440 - val_loss: 0.5970 - val_accuracy: 0.7333\n",
      "Epoch 59/500\n",
      "14/14 [==============================] - 0s 20ms/step - loss: 0.5307 - accuracy: 0.7344 - val_loss: 0.5980 - val_accuracy: 0.7143\n",
      "Epoch 60/500\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 0.5228 - accuracy: 0.7392 - val_loss: 0.6100 - val_accuracy: 0.7143\n",
      "Epoch 61/500\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 0.5169 - accuracy: 0.7368 - val_loss: 0.6040 - val_accuracy: 0.7238\n",
      "Epoch 62/500\n",
      "14/14 [==============================] - 0s 20ms/step - loss: 0.5191 - accuracy: 0.7440 - val_loss: 0.6055 - val_accuracy: 0.7238\n",
      "Epoch 63/500\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 0.5172 - accuracy: 0.7488 - val_loss: 0.5976 - val_accuracy: 0.7238\n",
      "Epoch 64/500\n",
      "14/14 [==============================] - 0s 20ms/step - loss: 0.5141 - accuracy: 0.7440 - val_loss: 0.6026 - val_accuracy: 0.7238\n",
      "Epoch 65/500\n",
      "14/14 [==============================] - 0s 20ms/step - loss: 0.5172 - accuracy: 0.7392 - val_loss: 0.6029 - val_accuracy: 0.7143\n",
      "Epoch 66/500\n",
      "14/14 [==============================] - 0s 19ms/step - loss: 0.5213 - accuracy: 0.7416 - val_loss: 0.5917 - val_accuracy: 0.7238\n",
      "Epoch 67/500\n",
      "14/14 [==============================] - 0s 19ms/step - loss: 0.5322 - accuracy: 0.7273 - val_loss: 0.5897 - val_accuracy: 0.7333\n",
      "Epoch 68/500\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 0.5111 - accuracy: 0.7392 - val_loss: 0.6146 - val_accuracy: 0.7143\n",
      "Epoch 69/500\n",
      "14/14 [==============================] - 0s 20ms/step - loss: 0.5171 - accuracy: 0.7321 - val_loss: 0.6076 - val_accuracy: 0.7143\n",
      "Epoch 70/500\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 0.5153 - accuracy: 0.7464 - val_loss: 0.5994 - val_accuracy: 0.7238\n",
      "Epoch 71/500\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 0.5037 - accuracy: 0.7416 - val_loss: 0.5947 - val_accuracy: 0.7333\n",
      "Epoch 72/500\n",
      "14/14 [==============================] - 0s 20ms/step - loss: 0.5102 - accuracy: 0.7632 - val_loss: 0.5945 - val_accuracy: 0.7333\n",
      "Epoch 73/500\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 0.5200 - accuracy: 0.7344 - val_loss: 0.5874 - val_accuracy: 0.7333\n",
      "Epoch 74/500\n",
      "14/14 [==============================] - 0s 19ms/step - loss: 0.5094 - accuracy: 0.7488 - val_loss: 0.5915 - val_accuracy: 0.7238\n",
      "Epoch 75/500\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 0.5111 - accuracy: 0.7584 - val_loss: 0.5896 - val_accuracy: 0.7429\n",
      "Epoch 76/500\n",
      "14/14 [==============================] - 0s 20ms/step - loss: 0.5088 - accuracy: 0.7464 - val_loss: 0.6042 - val_accuracy: 0.7143\n",
      "Epoch 77/500\n",
      "14/14 [==============================] - 0s 20ms/step - loss: 0.5100 - accuracy: 0.7560 - val_loss: 0.5953 - val_accuracy: 0.7238\n",
      "Epoch 78/500\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 0.5052 - accuracy: 0.7584 - val_loss: 0.5988 - val_accuracy: 0.7143\n",
      "Epoch 79/500\n",
      "14/14 [==============================] - 0s 20ms/step - loss: 0.5060 - accuracy: 0.7608 - val_loss: 0.5975 - val_accuracy: 0.7333\n",
      "Epoch 80/500\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 0.5002 - accuracy: 0.7775 - val_loss: 0.6104 - val_accuracy: 0.7143\n",
      "Epoch 81/500\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 0.5040 - accuracy: 0.7679 - val_loss: 0.5985 - val_accuracy: 0.7429\n",
      "Epoch 82/500\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 0.5012 - accuracy: 0.7536 - val_loss: 0.6068 - val_accuracy: 0.7143\n",
      "Epoch 83/500\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 0.5023 - accuracy: 0.7608 - val_loss: 0.5984 - val_accuracy: 0.7333\n",
      "Epoch 84/500\n",
      "14/14 [==============================] - 0s 20ms/step - loss: 0.4990 - accuracy: 0.7608 - val_loss: 0.6025 - val_accuracy: 0.7238\n",
      "Epoch 85/500\n",
      "14/14 [==============================] - 0s 20ms/step - loss: 0.5029 - accuracy: 0.7512 - val_loss: 0.6081 - val_accuracy: 0.7143\n",
      "Epoch 86/500\n",
      "14/14 [==============================] - 0s 20ms/step - loss: 0.4964 - accuracy: 0.7751 - val_loss: 0.5974 - val_accuracy: 0.7429\n",
      "Epoch 87/500\n",
      "14/14 [==============================] - 0s 19ms/step - loss: 0.4978 - accuracy: 0.7751 - val_loss: 0.6107 - val_accuracy: 0.7143\n",
      "Epoch 88/500\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 0.5033 - accuracy: 0.7512 - val_loss: 0.6069 - val_accuracy: 0.7143\n",
      "Epoch 89/500\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 0.4987 - accuracy: 0.7751 - val_loss: 0.6035 - val_accuracy: 0.7143\n",
      "Epoch 90/500\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 0.4972 - accuracy: 0.7608 - val_loss: 0.6102 - val_accuracy: 0.7143\n",
      "Epoch 91/500\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 0.5032 - accuracy: 0.7440 - val_loss: 0.5972 - val_accuracy: 0.7048\n",
      "Epoch 92/500\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 0.4935 - accuracy: 0.7727 - val_loss: 0.6073 - val_accuracy: 0.7143\n",
      "Epoch 93/500\n",
      "14/14 [==============================] - 0s 19ms/step - loss: 0.4907 - accuracy: 0.7560 - val_loss: 0.6107 - val_accuracy: 0.7143\n",
      "Epoch 94/500\n",
      "14/14 [==============================] - 0s 20ms/step - loss: 0.4901 - accuracy: 0.7536 - val_loss: 0.6115 - val_accuracy: 0.7238\n",
      "Epoch 95/500\n",
      "14/14 [==============================] - 0s 19ms/step - loss: 0.4867 - accuracy: 0.7584 - val_loss: 0.6117 - val_accuracy: 0.7238\n",
      "Epoch 96/500\n",
      "14/14 [==============================] - 0s 20ms/step - loss: 0.4967 - accuracy: 0.7608 - val_loss: 0.6076 - val_accuracy: 0.7333\n",
      "Epoch 97/500\n",
      "14/14 [==============================] - 0s 20ms/step - loss: 0.4863 - accuracy: 0.7560 - val_loss: 0.6127 - val_accuracy: 0.7238\n",
      "Epoch 98/500\n",
      "14/14 [==============================] - 0s 19ms/step - loss: 0.4881 - accuracy: 0.7679 - val_loss: 0.6025 - val_accuracy: 0.7333\n",
      "Epoch 99/500\n",
      "14/14 [==============================] - 0s 19ms/step - loss: 0.4881 - accuracy: 0.7536 - val_loss: 0.6053 - val_accuracy: 0.7238\n",
      "Epoch 100/500\n",
      "14/14 [==============================] - 0s 19ms/step - loss: 0.4854 - accuracy: 0.7632 - val_loss: 0.6136 - val_accuracy: 0.7333\n",
      "Epoch 101/500\n",
      "14/14 [==============================] - 0s 20ms/step - loss: 0.4864 - accuracy: 0.7584 - val_loss: 0.6174 - val_accuracy: 0.7238\n",
      "Epoch 102/500\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 0.4862 - accuracy: 0.7560 - val_loss: 0.6107 - val_accuracy: 0.7333\n",
      "Epoch 103/500\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 0.4826 - accuracy: 0.7656 - val_loss: 0.6163 - val_accuracy: 0.7238\n",
      "Epoch 104/500\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 0.4829 - accuracy: 0.7608 - val_loss: 0.6058 - val_accuracy: 0.7238\n",
      "Epoch 105/500\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 0.4837 - accuracy: 0.7560 - val_loss: 0.6171 - val_accuracy: 0.7238\n",
      "Epoch 106/500\n",
      "14/14 [==============================] - 0s 20ms/step - loss: 0.4854 - accuracy: 0.7727 - val_loss: 0.6208 - val_accuracy: 0.7143\n",
      "Epoch 107/500\n",
      "14/14 [==============================] - 0s 20ms/step - loss: 0.4759 - accuracy: 0.7608 - val_loss: 0.6110 - val_accuracy: 0.7333\n",
      "Epoch 108/500\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 0.4815 - accuracy: 0.7775 - val_loss: 0.6064 - val_accuracy: 0.7143\n",
      "Epoch 109/500\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 0.4859 - accuracy: 0.7775 - val_loss: 0.6019 - val_accuracy: 0.7333\n",
      "Epoch 110/500\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 0.4760 - accuracy: 0.7608 - val_loss: 0.6140 - val_accuracy: 0.7143\n",
      "Epoch 111/500\n",
      "14/14 [==============================] - 0s 20ms/step - loss: 0.4815 - accuracy: 0.7464 - val_loss: 0.6101 - val_accuracy: 0.7333\n",
      "Epoch 112/500\n",
      "14/14 [==============================] - 0s 20ms/step - loss: 0.4737 - accuracy: 0.7608 - val_loss: 0.6213 - val_accuracy: 0.7238\n",
      "Epoch 113/500\n",
      "14/14 [==============================] - 0s 18ms/step - loss: 0.4811 - accuracy: 0.7632 - val_loss: 0.6144 - val_accuracy: 0.7333\n",
      "Epoch 114/500\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 0.4681 - accuracy: 0.7895 - val_loss: 0.6157 - val_accuracy: 0.7429\n",
      "Epoch 115/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14/14 [==============================] - 0s 20ms/step - loss: 0.4683 - accuracy: 0.7703 - val_loss: 0.6240 - val_accuracy: 0.7238\n",
      "Epoch 116/500\n",
      "14/14 [==============================] - 0s 20ms/step - loss: 0.4713 - accuracy: 0.7703 - val_loss: 0.6209 - val_accuracy: 0.7238\n",
      "Epoch 117/500\n",
      "14/14 [==============================] - 0s 20ms/step - loss: 0.4688 - accuracy: 0.7632 - val_loss: 0.6203 - val_accuracy: 0.7333\n",
      "Epoch 118/500\n",
      "14/14 [==============================] - 0s 20ms/step - loss: 0.4710 - accuracy: 0.7679 - val_loss: 0.6137 - val_accuracy: 0.7333\n",
      "Epoch 119/500\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 0.4724 - accuracy: 0.7703 - val_loss: 0.6283 - val_accuracy: 0.7238\n",
      "Epoch 120/500\n",
      "14/14 [==============================] - 0s 19ms/step - loss: 0.4683 - accuracy: 0.7727 - val_loss: 0.6129 - val_accuracy: 0.7333\n",
      "Epoch 121/500\n",
      "14/14 [==============================] - 0s 18ms/step - loss: 0.4639 - accuracy: 0.7656 - val_loss: 0.6191 - val_accuracy: 0.7333\n",
      "Epoch 122/500\n",
      "14/14 [==============================] - 0s 20ms/step - loss: 0.4708 - accuracy: 0.7679 - val_loss: 0.6120 - val_accuracy: 0.7429\n",
      "Epoch 123/500\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 0.4636 - accuracy: 0.7727 - val_loss: 0.6290 - val_accuracy: 0.7238\n",
      "Epoch 124/500\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 0.4594 - accuracy: 0.7823 - val_loss: 0.6203 - val_accuracy: 0.7429\n",
      "Epoch 125/500\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 0.4626 - accuracy: 0.7775 - val_loss: 0.6213 - val_accuracy: 0.7429\n",
      "Epoch 126/500\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 0.4654 - accuracy: 0.7727 - val_loss: 0.6249 - val_accuracy: 0.7333\n",
      "Epoch 127/500\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 0.4616 - accuracy: 0.7799 - val_loss: 0.6143 - val_accuracy: 0.7429\n",
      "Epoch 128/500\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 0.4606 - accuracy: 0.7799 - val_loss: 0.6222 - val_accuracy: 0.7333\n",
      "Epoch 129/500\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 0.4702 - accuracy: 0.7608 - val_loss: 0.6179 - val_accuracy: 0.7238\n",
      "Epoch 130/500\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 0.4602 - accuracy: 0.7608 - val_loss: 0.6369 - val_accuracy: 0.7238\n",
      "Epoch 131/500\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 0.4591 - accuracy: 0.7751 - val_loss: 0.6150 - val_accuracy: 0.7429\n",
      "Epoch 132/500\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 0.4615 - accuracy: 0.7703 - val_loss: 0.6148 - val_accuracy: 0.7429\n",
      "Epoch 133/500\n",
      "14/14 [==============================] - 0s 20ms/step - loss: 0.4623 - accuracy: 0.7679 - val_loss: 0.6175 - val_accuracy: 0.7333\n",
      "Epoch 134/500\n",
      "14/14 [==============================] - 0s 20ms/step - loss: 0.4637 - accuracy: 0.7751 - val_loss: 0.6114 - val_accuracy: 0.7238\n",
      "Epoch 135/500\n",
      "14/14 [==============================] - 0s 20ms/step - loss: 0.4528 - accuracy: 0.7847 - val_loss: 0.6175 - val_accuracy: 0.7333\n",
      "Epoch 136/500\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 0.4587 - accuracy: 0.7679 - val_loss: 0.6222 - val_accuracy: 0.7429\n",
      "Epoch 137/500\n",
      "14/14 [==============================] - 0s 20ms/step - loss: 0.4550 - accuracy: 0.7799 - val_loss: 0.6092 - val_accuracy: 0.7429\n",
      "Epoch 138/500\n",
      "14/14 [==============================] - 0s 19ms/step - loss: 0.4496 - accuracy: 0.7727 - val_loss: 0.6284 - val_accuracy: 0.7333\n",
      "Epoch 139/500\n",
      "14/14 [==============================] - 0s 19ms/step - loss: 0.4561 - accuracy: 0.7799 - val_loss: 0.6280 - val_accuracy: 0.7333\n",
      "Epoch 140/500\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 0.4496 - accuracy: 0.7871 - val_loss: 0.6143 - val_accuracy: 0.7238\n",
      "Epoch 141/500\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 0.4481 - accuracy: 0.7703 - val_loss: 0.6278 - val_accuracy: 0.7524\n",
      "Epoch 142/500\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 0.4527 - accuracy: 0.7656 - val_loss: 0.6422 - val_accuracy: 0.7143\n",
      "Epoch 143/500\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.4486 - accuracy: 0.7679 - val_loss: 0.6270 - val_accuracy: 0.7429\n",
      "Epoch 144/500\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.4479 - accuracy: 0.7751 - val_loss: 0.6267 - val_accuracy: 0.7429\n",
      "Epoch 145/500\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 0.4500 - accuracy: 0.7727 - val_loss: 0.6193 - val_accuracy: 0.7333\n",
      "Epoch 146/500\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 0.4486 - accuracy: 0.7775 - val_loss: 0.6362 - val_accuracy: 0.7238\n",
      "Epoch 147/500\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 0.4451 - accuracy: 0.7847 - val_loss: 0.6267 - val_accuracy: 0.7333\n",
      "Epoch 148/500\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 0.4468 - accuracy: 0.7967 - val_loss: 0.6202 - val_accuracy: 0.7333\n",
      "Epoch 149/500\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 0.4386 - accuracy: 0.7823 - val_loss: 0.6257 - val_accuracy: 0.7333\n",
      "Epoch 150/500\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 0.4484 - accuracy: 0.7823 - val_loss: 0.6336 - val_accuracy: 0.7429\n",
      "Epoch 151/500\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 0.4468 - accuracy: 0.7799 - val_loss: 0.6272 - val_accuracy: 0.7333\n",
      "Epoch 152/500\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 0.4354 - accuracy: 0.7990 - val_loss: 0.6253 - val_accuracy: 0.7429\n",
      "Epoch 153/500\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 0.4494 - accuracy: 0.7775 - val_loss: 0.6261 - val_accuracy: 0.7333\n",
      "Epoch 154/500\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 0.4345 - accuracy: 0.7656 - val_loss: 0.6378 - val_accuracy: 0.7619\n",
      "Epoch 155/500\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 0.4355 - accuracy: 0.7871 - val_loss: 0.6426 - val_accuracy: 0.7524\n",
      "Epoch 156/500\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 0.4386 - accuracy: 0.7775 - val_loss: 0.6331 - val_accuracy: 0.7524\n",
      "Epoch 157/500\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 0.4374 - accuracy: 0.7823 - val_loss: 0.6351 - val_accuracy: 0.7429\n",
      "Epoch 158/500\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 0.4311 - accuracy: 0.7990 - val_loss: 0.6323 - val_accuracy: 0.7333\n",
      "Epoch 159/500\n",
      "14/14 [==============================] - 0s 30ms/step - loss: 0.4272 - accuracy: 0.7823 - val_loss: 0.6388 - val_accuracy: 0.7524\n",
      "Epoch 160/500\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 0.4279 - accuracy: 0.7943 - val_loss: 0.6273 - val_accuracy: 0.7524\n",
      "Epoch 161/500\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 0.4316 - accuracy: 0.7943 - val_loss: 0.6351 - val_accuracy: 0.7429\n",
      "Epoch 162/500\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 0.4349 - accuracy: 0.7919 - val_loss: 0.6490 - val_accuracy: 0.7333\n",
      "Epoch 163/500\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 0.4286 - accuracy: 0.8014 - val_loss: 0.6356 - val_accuracy: 0.7524\n",
      "Epoch 164/500\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 0.4285 - accuracy: 0.7871 - val_loss: 0.6344 - val_accuracy: 0.7429\n",
      "Epoch 165/500\n",
      "14/14 [==============================] - 0s 20ms/step - loss: 0.4297 - accuracy: 0.7823 - val_loss: 0.6408 - val_accuracy: 0.7429\n",
      "Epoch 166/500\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 0.4328 - accuracy: 0.8110 - val_loss: 0.6350 - val_accuracy: 0.7524\n",
      "Epoch 167/500\n",
      "14/14 [==============================] - 0s 19ms/step - loss: 0.4331 - accuracy: 0.7751 - val_loss: 0.6465 - val_accuracy: 0.7524\n",
      "Epoch 168/500\n",
      "14/14 [==============================] - 0s 20ms/step - loss: 0.4244 - accuracy: 0.7967 - val_loss: 0.6365 - val_accuracy: 0.7524\n",
      "Epoch 169/500\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 0.4394 - accuracy: 0.7895 - val_loss: 0.6524 - val_accuracy: 0.7333\n",
      "Epoch 170/500\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 0.4247 - accuracy: 0.7895 - val_loss: 0.6302 - val_accuracy: 0.7048\n",
      "Epoch 171/500\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 0.4256 - accuracy: 0.7895 - val_loss: 0.6341 - val_accuracy: 0.7429\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 172/500\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 0.4299 - accuracy: 0.8086 - val_loss: 0.6534 - val_accuracy: 0.7524\n",
      "Epoch 173/500\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 0.4240 - accuracy: 0.7990 - val_loss: 0.6419 - val_accuracy: 0.7429\n",
      "Epoch 174/500\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 0.4257 - accuracy: 0.8086 - val_loss: 0.6331 - val_accuracy: 0.7429\n",
      "Epoch 175/500\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 0.4156 - accuracy: 0.8062 - val_loss: 0.6396 - val_accuracy: 0.7333\n",
      "Epoch 176/500\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 0.4237 - accuracy: 0.7919 - val_loss: 0.6638 - val_accuracy: 0.7429\n",
      "Epoch 177/500\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 0.4201 - accuracy: 0.8014 - val_loss: 0.6437 - val_accuracy: 0.7524\n",
      "Epoch 178/500\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 0.4256 - accuracy: 0.8062 - val_loss: 0.6422 - val_accuracy: 0.7524\n",
      "Epoch 179/500\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.4177 - accuracy: 0.8038 - val_loss: 0.6447 - val_accuracy: 0.7429\n",
      "Epoch 180/500\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.4187 - accuracy: 0.8062 - val_loss: 0.6409 - val_accuracy: 0.7238\n",
      "Epoch 181/500\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 0.4305 - accuracy: 0.7727 - val_loss: 0.6644 - val_accuracy: 0.7524\n",
      "Epoch 182/500\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 0.4093 - accuracy: 0.8134 - val_loss: 0.6405 - val_accuracy: 0.7143\n",
      "Epoch 183/500\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 0.4120 - accuracy: 0.8182 - val_loss: 0.6492 - val_accuracy: 0.7333\n",
      "Epoch 184/500\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 0.4139 - accuracy: 0.8038 - val_loss: 0.6505 - val_accuracy: 0.7333\n",
      "Epoch 185/500\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.4065 - accuracy: 0.8134 - val_loss: 0.6606 - val_accuracy: 0.7524\n",
      "Epoch 186/500\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 0.4099 - accuracy: 0.8134 - val_loss: 0.6520 - val_accuracy: 0.7429\n",
      "Epoch 187/500\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.4137 - accuracy: 0.8134 - val_loss: 0.6527 - val_accuracy: 0.7143\n",
      "Epoch 188/500\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 0.4117 - accuracy: 0.8086 - val_loss: 0.6466 - val_accuracy: 0.7333\n",
      "Epoch 189/500\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 0.4038 - accuracy: 0.8182 - val_loss: 0.6494 - val_accuracy: 0.7429\n",
      "Epoch 190/500\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 0.4072 - accuracy: 0.8182 - val_loss: 0.6474 - val_accuracy: 0.7333\n",
      "Epoch 191/500\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 0.4151 - accuracy: 0.8158 - val_loss: 0.6647 - val_accuracy: 0.7524\n",
      "Epoch 192/500\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 0.4034 - accuracy: 0.8206 - val_loss: 0.6541 - val_accuracy: 0.7333\n",
      "Epoch 193/500\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 0.4035 - accuracy: 0.8134 - val_loss: 0.6544 - val_accuracy: 0.7429\n",
      "Epoch 194/500\n",
      "14/14 [==============================] - 0s 20ms/step - loss: 0.4087 - accuracy: 0.8038 - val_loss: 0.6378 - val_accuracy: 0.7048\n",
      "Epoch 195/500\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 0.4086 - accuracy: 0.8158 - val_loss: 0.6369 - val_accuracy: 0.6952\n",
      "Epoch 196/500\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 0.4123 - accuracy: 0.8158 - val_loss: 0.6644 - val_accuracy: 0.7333\n",
      "Epoch 197/500\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 0.4095 - accuracy: 0.8230 - val_loss: 0.6640 - val_accuracy: 0.7429\n",
      "Epoch 198/500\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 0.4044 - accuracy: 0.8062 - val_loss: 0.6548 - val_accuracy: 0.7429\n",
      "Epoch 199/500\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 0.4044 - accuracy: 0.8134 - val_loss: 0.6538 - val_accuracy: 0.6952\n",
      "Epoch 200/500\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 0.4077 - accuracy: 0.8230 - val_loss: 0.6634 - val_accuracy: 0.7048\n",
      "Epoch 201/500\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 0.4070 - accuracy: 0.8230 - val_loss: 0.6764 - val_accuracy: 0.7333\n",
      "Epoch 202/500\n",
      "14/14 [==============================] - 0s 19ms/step - loss: 0.3977 - accuracy: 0.8206 - val_loss: 0.6732 - val_accuracy: 0.7333\n",
      "Epoch 203/500\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 0.3943 - accuracy: 0.8038 - val_loss: 0.6706 - val_accuracy: 0.7524\n",
      "Epoch 204/500\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 0.4004 - accuracy: 0.8158 - val_loss: 0.6535 - val_accuracy: 0.7238\n",
      "Epoch 205/500\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 0.4029 - accuracy: 0.8110 - val_loss: 0.6691 - val_accuracy: 0.7619\n",
      "Epoch 206/500\n",
      "14/14 [==============================] - 0s 20ms/step - loss: 0.3921 - accuracy: 0.8230 - val_loss: 0.6632 - val_accuracy: 0.7429\n",
      "Epoch 207/500\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 0.3905 - accuracy: 0.8254 - val_loss: 0.6602 - val_accuracy: 0.7238\n",
      "Epoch 208/500\n",
      "14/14 [==============================] - 0s 20ms/step - loss: 0.3907 - accuracy: 0.8445 - val_loss: 0.6568 - val_accuracy: 0.6952\n",
      "Epoch 209/500\n",
      "14/14 [==============================] - 0s 19ms/step - loss: 0.3909 - accuracy: 0.8254 - val_loss: 0.6791 - val_accuracy: 0.7524\n",
      "Epoch 210/500\n",
      "14/14 [==============================] - 0s 20ms/step - loss: 0.3988 - accuracy: 0.8158 - val_loss: 0.6692 - val_accuracy: 0.7238\n",
      "Epoch 211/500\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 0.3834 - accuracy: 0.8278 - val_loss: 0.6752 - val_accuracy: 0.7429\n",
      "Epoch 212/500\n",
      "14/14 [==============================] - 0s 20ms/step - loss: 0.3841 - accuracy: 0.8325 - val_loss: 0.6664 - val_accuracy: 0.7143\n",
      "Epoch 213/500\n",
      "14/14 [==============================] - 0s 19ms/step - loss: 0.3985 - accuracy: 0.8325 - val_loss: 0.6602 - val_accuracy: 0.6952\n",
      "Epoch 214/500\n",
      "14/14 [==============================] - 0s 19ms/step - loss: 0.3918 - accuracy: 0.8254 - val_loss: 0.6878 - val_accuracy: 0.7333\n",
      "Epoch 215/500\n",
      "14/14 [==============================] - 0s 19ms/step - loss: 0.3940 - accuracy: 0.8062 - val_loss: 0.6683 - val_accuracy: 0.7238\n",
      "Epoch 216/500\n",
      "14/14 [==============================] - 0s 19ms/step - loss: 0.3926 - accuracy: 0.8445 - val_loss: 0.6521 - val_accuracy: 0.6952\n",
      "Epoch 217/500\n",
      "14/14 [==============================] - 0s 16ms/step - loss: 0.3878 - accuracy: 0.8349 - val_loss: 0.6670 - val_accuracy: 0.7429\n",
      "Epoch 218/500\n",
      "14/14 [==============================] - 0s 18ms/step - loss: 0.3764 - accuracy: 0.8373 - val_loss: 0.6649 - val_accuracy: 0.7333\n",
      "Epoch 219/500\n",
      "14/14 [==============================] - 0s 18ms/step - loss: 0.3732 - accuracy: 0.8445 - val_loss: 0.6728 - val_accuracy: 0.7429\n",
      "Epoch 220/500\n",
      "14/14 [==============================] - 0s 17ms/step - loss: 0.3841 - accuracy: 0.8301 - val_loss: 0.6611 - val_accuracy: 0.7143\n",
      "Epoch 221/500\n",
      "14/14 [==============================] - 0s 18ms/step - loss: 0.3860 - accuracy: 0.8278 - val_loss: 0.6685 - val_accuracy: 0.7524\n",
      "Epoch 222/500\n",
      "14/14 [==============================] - 0s 19ms/step - loss: 0.3815 - accuracy: 0.8349 - val_loss: 0.6666 - val_accuracy: 0.7143\n",
      "Epoch 223/500\n",
      "14/14 [==============================] - 0s 19ms/step - loss: 0.3852 - accuracy: 0.8325 - val_loss: 0.6654 - val_accuracy: 0.7143\n",
      "Epoch 224/500\n",
      "14/14 [==============================] - 0s 19ms/step - loss: 0.3815 - accuracy: 0.8278 - val_loss: 0.6875 - val_accuracy: 0.7524\n",
      "Epoch 225/500\n",
      "14/14 [==============================] - 0s 17ms/step - loss: 0.3760 - accuracy: 0.8349 - val_loss: 0.6654 - val_accuracy: 0.6952\n",
      "Epoch 226/500\n",
      "14/14 [==============================] - 0s 18ms/step - loss: 0.3787 - accuracy: 0.8373 - val_loss: 0.6644 - val_accuracy: 0.6857\n",
      "Epoch 227/500\n",
      "14/14 [==============================] - 0s 19ms/step - loss: 0.3838 - accuracy: 0.8565 - val_loss: 0.6742 - val_accuracy: 0.7143\n",
      "Epoch 228/500\n",
      "14/14 [==============================] - 0s 20ms/step - loss: 0.3892 - accuracy: 0.8158 - val_loss: 0.6921 - val_accuracy: 0.7524\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 229/500\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 0.3784 - accuracy: 0.8325 - val_loss: 0.6625 - val_accuracy: 0.6857\n",
      "Epoch 230/500\n",
      "14/14 [==============================] - 0s 20ms/step - loss: 0.3731 - accuracy: 0.8349 - val_loss: 0.6872 - val_accuracy: 0.7333\n",
      "Epoch 231/500\n",
      "14/14 [==============================] - 0s 19ms/step - loss: 0.3758 - accuracy: 0.8278 - val_loss: 0.6896 - val_accuracy: 0.7238\n",
      "Epoch 232/500\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 0.3764 - accuracy: 0.8397 - val_loss: 0.6732 - val_accuracy: 0.6952\n",
      "Epoch 233/500\n",
      "14/14 [==============================] - 0s 19ms/step - loss: 0.3692 - accuracy: 0.8493 - val_loss: 0.6704 - val_accuracy: 0.6857\n",
      "Epoch 234/500\n",
      "14/14 [==============================] - 0s 20ms/step - loss: 0.3642 - accuracy: 0.8469 - val_loss: 0.6827 - val_accuracy: 0.6952\n",
      "Epoch 235/500\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 0.3786 - accuracy: 0.8158 - val_loss: 0.7155 - val_accuracy: 0.7333\n",
      "Epoch 236/500\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 0.3725 - accuracy: 0.8230 - val_loss: 0.6818 - val_accuracy: 0.7143\n",
      "Epoch 237/500\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 0.3744 - accuracy: 0.8493 - val_loss: 0.6750 - val_accuracy: 0.6952\n",
      "Epoch 238/500\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 0.3842 - accuracy: 0.8254 - val_loss: 0.6768 - val_accuracy: 0.7143\n",
      "Epoch 239/500\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 0.3710 - accuracy: 0.8182 - val_loss: 0.7097 - val_accuracy: 0.7333\n",
      "Epoch 240/500\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 0.3724 - accuracy: 0.8373 - val_loss: 0.6859 - val_accuracy: 0.7143\n",
      "Epoch 241/500\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 0.3699 - accuracy: 0.8397 - val_loss: 0.6888 - val_accuracy: 0.7143\n",
      "Epoch 242/500\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 0.3778 - accuracy: 0.8278 - val_loss: 0.6793 - val_accuracy: 0.7143\n",
      "Epoch 243/500\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 0.3651 - accuracy: 0.8517 - val_loss: 0.6745 - val_accuracy: 0.7143\n",
      "Epoch 244/500\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 0.3655 - accuracy: 0.8421 - val_loss: 0.6843 - val_accuracy: 0.7143\n",
      "Epoch 245/500\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 0.3661 - accuracy: 0.8349 - val_loss: 0.6839 - val_accuracy: 0.7238\n",
      "Epoch 246/500\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 0.3582 - accuracy: 0.8565 - val_loss: 0.6749 - val_accuracy: 0.6952\n",
      "Epoch 247/500\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 0.3704 - accuracy: 0.8421 - val_loss: 0.6820 - val_accuracy: 0.7048\n",
      "Epoch 248/500\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 0.3543 - accuracy: 0.8493 - val_loss: 0.6859 - val_accuracy: 0.7048\n",
      "Epoch 249/500\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 0.3652 - accuracy: 0.8445 - val_loss: 0.6871 - val_accuracy: 0.7048\n",
      "Epoch 250/500\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 0.3543 - accuracy: 0.8565 - val_loss: 0.6735 - val_accuracy: 0.6952\n",
      "Epoch 251/500\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 0.3543 - accuracy: 0.8541 - val_loss: 0.6887 - val_accuracy: 0.7143\n",
      "Epoch 252/500\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 0.3532 - accuracy: 0.8541 - val_loss: 0.6899 - val_accuracy: 0.7048\n",
      "Epoch 253/500\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 0.3680 - accuracy: 0.8493 - val_loss: 0.6986 - val_accuracy: 0.7238\n",
      "Epoch 254/500\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 0.3676 - accuracy: 0.8373 - val_loss: 0.6977 - val_accuracy: 0.7048\n",
      "WARNING:tensorflow:5 out of the last 9 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7ffc9c4d3040> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1/500\n",
      "14/14 [==============================] - 1s 33ms/step - loss: 0.6127 - accuracy: 0.6818 - val_loss: 0.6256 - val_accuracy: 0.7048\n",
      "Epoch 2/500\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 0.6118 - accuracy: 0.7057 - val_loss: 0.6029 - val_accuracy: 0.7048\n",
      "Epoch 3/500\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 0.5918 - accuracy: 0.7057 - val_loss: 0.6012 - val_accuracy: 0.7048\n",
      "Epoch 4/500\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 0.5932 - accuracy: 0.7057 - val_loss: 0.5986 - val_accuracy: 0.7048\n",
      "Epoch 5/500\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 0.5894 - accuracy: 0.7057 - val_loss: 0.5973 - val_accuracy: 0.7048\n",
      "Epoch 6/500\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 0.5919 - accuracy: 0.7057 - val_loss: 0.5978 - val_accuracy: 0.7048\n",
      "Epoch 7/500\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 0.5900 - accuracy: 0.7057 - val_loss: 0.6052 - val_accuracy: 0.6952\n",
      "Epoch 8/500\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 0.5967 - accuracy: 0.7057 - val_loss: 0.5983 - val_accuracy: 0.7048\n",
      "Epoch 9/500\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 0.5812 - accuracy: 0.7057 - val_loss: 0.5971 - val_accuracy: 0.7048\n",
      "Epoch 10/500\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 0.5865 - accuracy: 0.7057 - val_loss: 0.5980 - val_accuracy: 0.7048\n",
      "Epoch 11/500\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 0.5848 - accuracy: 0.7033 - val_loss: 0.6011 - val_accuracy: 0.6952\n",
      "Epoch 12/500\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 0.5883 - accuracy: 0.7033 - val_loss: 0.5964 - val_accuracy: 0.6952\n",
      "Epoch 13/500\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 0.5795 - accuracy: 0.7057 - val_loss: 0.5961 - val_accuracy: 0.7048\n",
      "Epoch 14/500\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 0.5817 - accuracy: 0.7057 - val_loss: 0.5962 - val_accuracy: 0.7048\n",
      "Epoch 15/500\n",
      "14/14 [==============================] - 0s 20ms/step - loss: 0.5793 - accuracy: 0.7057 - val_loss: 0.5935 - val_accuracy: 0.6952\n",
      "Epoch 16/500\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 0.5766 - accuracy: 0.7057 - val_loss: 0.5943 - val_accuracy: 0.6952\n",
      "Epoch 17/500\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 0.5768 - accuracy: 0.7033 - val_loss: 0.5953 - val_accuracy: 0.6952\n",
      "Epoch 18/500\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 0.5760 - accuracy: 0.7033 - val_loss: 0.5943 - val_accuracy: 0.6952\n",
      "Epoch 19/500\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 0.5742 - accuracy: 0.7057 - val_loss: 0.5931 - val_accuracy: 0.6952\n",
      "Epoch 20/500\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 0.5721 - accuracy: 0.7105 - val_loss: 0.5929 - val_accuracy: 0.6952\n",
      "Epoch 21/500\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 0.5729 - accuracy: 0.7105 - val_loss: 0.5927 - val_accuracy: 0.6952\n",
      "Epoch 22/500\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 0.5765 - accuracy: 0.7177 - val_loss: 0.5958 - val_accuracy: 0.6952\n",
      "Epoch 23/500\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 0.5721 - accuracy: 0.7129 - val_loss: 0.5937 - val_accuracy: 0.6952\n",
      "Epoch 24/500\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 0.5695 - accuracy: 0.7105 - val_loss: 0.5946 - val_accuracy: 0.6857\n",
      "Epoch 25/500\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 0.5695 - accuracy: 0.7105 - val_loss: 0.5940 - val_accuracy: 0.6857\n",
      "Epoch 26/500\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 0.5653 - accuracy: 0.7105 - val_loss: 0.5929 - val_accuracy: 0.6857\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/500\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 0.5670 - accuracy: 0.7105 - val_loss: 0.5930 - val_accuracy: 0.6857\n",
      "Epoch 28/500\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 0.5621 - accuracy: 0.7153 - val_loss: 0.5896 - val_accuracy: 0.6667\n",
      "Epoch 29/500\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 0.5645 - accuracy: 0.7129 - val_loss: 0.5935 - val_accuracy: 0.6857\n",
      "Epoch 30/500\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 0.5623 - accuracy: 0.7153 - val_loss: 0.5926 - val_accuracy: 0.6857\n",
      "Epoch 31/500\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 0.5618 - accuracy: 0.7177 - val_loss: 0.5932 - val_accuracy: 0.6762\n",
      "Epoch 32/500\n",
      "14/14 [==============================] - 0s 20ms/step - loss: 0.5599 - accuracy: 0.7225 - val_loss: 0.5923 - val_accuracy: 0.6571\n",
      "Epoch 33/500\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 0.5612 - accuracy: 0.7201 - val_loss: 0.5970 - val_accuracy: 0.6857\n",
      "Epoch 34/500\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 0.5566 - accuracy: 0.7249 - val_loss: 0.5942 - val_accuracy: 0.6667\n",
      "Epoch 35/500\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 0.5570 - accuracy: 0.7249 - val_loss: 0.5939 - val_accuracy: 0.6762\n",
      "Epoch 36/500\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 0.5732 - accuracy: 0.7129 - val_loss: 0.5930 - val_accuracy: 0.6762\n",
      "Epoch 37/500\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 0.5541 - accuracy: 0.7225 - val_loss: 0.5958 - val_accuracy: 0.6857\n",
      "Epoch 38/500\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 0.5592 - accuracy: 0.7225 - val_loss: 0.5992 - val_accuracy: 0.6857\n",
      "Epoch 39/500\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 0.5563 - accuracy: 0.7225 - val_loss: 0.6009 - val_accuracy: 0.6667\n",
      "Epoch 40/500\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 0.5549 - accuracy: 0.7249 - val_loss: 0.6016 - val_accuracy: 0.6667\n",
      "Epoch 41/500\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 0.5514 - accuracy: 0.7201 - val_loss: 0.5992 - val_accuracy: 0.6762\n",
      "Epoch 42/500\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 0.5479 - accuracy: 0.7297 - val_loss: 0.5970 - val_accuracy: 0.6857\n",
      "Epoch 43/500\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 0.5469 - accuracy: 0.7273 - val_loss: 0.5969 - val_accuracy: 0.6857\n",
      "Epoch 44/500\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 0.5453 - accuracy: 0.7368 - val_loss: 0.5949 - val_accuracy: 0.6762\n",
      "Epoch 45/500\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 0.5452 - accuracy: 0.7321 - val_loss: 0.5955 - val_accuracy: 0.6762\n",
      "Epoch 46/500\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 0.5454 - accuracy: 0.7225 - val_loss: 0.5985 - val_accuracy: 0.6857\n",
      "Epoch 47/500\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 0.5461 - accuracy: 0.7273 - val_loss: 0.5942 - val_accuracy: 0.6667\n",
      "Epoch 48/500\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 0.5442 - accuracy: 0.7368 - val_loss: 0.5962 - val_accuracy: 0.6762\n",
      "Epoch 49/500\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 0.5406 - accuracy: 0.7344 - val_loss: 0.5974 - val_accuracy: 0.6857\n",
      "Epoch 50/500\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 0.5384 - accuracy: 0.7321 - val_loss: 0.5932 - val_accuracy: 0.6667\n",
      "Epoch 51/500\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 0.5406 - accuracy: 0.7416 - val_loss: 0.5921 - val_accuracy: 0.6667\n",
      "Epoch 52/500\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 0.5404 - accuracy: 0.7416 - val_loss: 0.5915 - val_accuracy: 0.6667\n",
      "Epoch 53/500\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 0.5399 - accuracy: 0.7440 - val_loss: 0.5934 - val_accuracy: 0.6857\n",
      "Epoch 54/500\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.5411 - accuracy: 0.7464 - val_loss: 0.5906 - val_accuracy: 0.6667\n",
      "Epoch 55/500\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.5435 - accuracy: 0.7416 - val_loss: 0.5942 - val_accuracy: 0.6857\n",
      "Epoch 56/500\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 0.5334 - accuracy: 0.7368 - val_loss: 0.5933 - val_accuracy: 0.6762\n",
      "Epoch 57/500\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 0.5320 - accuracy: 0.7488 - val_loss: 0.5905 - val_accuracy: 0.6952\n",
      "Epoch 58/500\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 0.5375 - accuracy: 0.7512 - val_loss: 0.5873 - val_accuracy: 0.6667\n",
      "Epoch 59/500\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 0.5277 - accuracy: 0.7488 - val_loss: 0.5887 - val_accuracy: 0.6952\n",
      "Epoch 60/500\n",
      "14/14 [==============================] - 0s 20ms/step - loss: 0.5295 - accuracy: 0.7464 - val_loss: 0.5912 - val_accuracy: 0.6762\n",
      "Epoch 61/500\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 0.5270 - accuracy: 0.7488 - val_loss: 0.6015 - val_accuracy: 0.6952\n",
      "Epoch 62/500\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 0.5379 - accuracy: 0.7440 - val_loss: 0.6002 - val_accuracy: 0.6762\n",
      "Epoch 63/500\n",
      "14/14 [==============================] - 0s 20ms/step - loss: 0.5289 - accuracy: 0.7488 - val_loss: 0.5940 - val_accuracy: 0.6762\n",
      "Epoch 64/500\n",
      "14/14 [==============================] - 0s 19ms/step - loss: 0.5302 - accuracy: 0.7440 - val_loss: 0.5893 - val_accuracy: 0.6762\n",
      "Epoch 65/500\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 0.5253 - accuracy: 0.7679 - val_loss: 0.5869 - val_accuracy: 0.6762\n",
      "Epoch 66/500\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 0.5256 - accuracy: 0.7608 - val_loss: 0.5901 - val_accuracy: 0.6762\n",
      "Epoch 67/500\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 0.5296 - accuracy: 0.7536 - val_loss: 0.5914 - val_accuracy: 0.6762\n",
      "Epoch 68/500\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 0.5232 - accuracy: 0.7703 - val_loss: 0.5882 - val_accuracy: 0.6762\n",
      "Epoch 69/500\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.5221 - accuracy: 0.7632 - val_loss: 0.5853 - val_accuracy: 0.6762\n",
      "Epoch 70/500\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 0.5192 - accuracy: 0.7536 - val_loss: 0.5871 - val_accuracy: 0.6762\n",
      "Epoch 71/500\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.5182 - accuracy: 0.7512 - val_loss: 0.5872 - val_accuracy: 0.7048\n",
      "Epoch 72/500\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 0.5177 - accuracy: 0.7679 - val_loss: 0.5869 - val_accuracy: 0.6857\n",
      "Epoch 73/500\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 0.5185 - accuracy: 0.7464 - val_loss: 0.5886 - val_accuracy: 0.6762\n",
      "Epoch 74/500\n",
      "14/14 [==============================] - 0s 20ms/step - loss: 0.5304 - accuracy: 0.7560 - val_loss: 0.6024 - val_accuracy: 0.6952\n",
      "Epoch 75/500\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 0.5216 - accuracy: 0.7608 - val_loss: 0.5868 - val_accuracy: 0.6762\n",
      "Epoch 76/500\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 0.5165 - accuracy: 0.7560 - val_loss: 0.5884 - val_accuracy: 0.6667\n",
      "Epoch 77/500\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 0.5148 - accuracy: 0.7608 - val_loss: 0.5875 - val_accuracy: 0.6667\n",
      "Epoch 78/500\n",
      "14/14 [==============================] - 0s 20ms/step - loss: 0.5124 - accuracy: 0.7679 - val_loss: 0.5826 - val_accuracy: 0.7048\n",
      "Epoch 79/500\n",
      "14/14 [==============================] - 0s 19ms/step - loss: 0.5112 - accuracy: 0.7847 - val_loss: 0.5867 - val_accuracy: 0.6857\n",
      "Epoch 80/500\n",
      "14/14 [==============================] - 0s 19ms/step - loss: 0.5165 - accuracy: 0.7608 - val_loss: 0.5893 - val_accuracy: 0.6667\n",
      "Epoch 81/500\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 0.5147 - accuracy: 0.7632 - val_loss: 0.5827 - val_accuracy: 0.7048\n",
      "Epoch 82/500\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 0.5118 - accuracy: 0.7751 - val_loss: 0.5879 - val_accuracy: 0.6762\n",
      "Epoch 83/500\n",
      "14/14 [==============================] - 0s 19ms/step - loss: 0.5124 - accuracy: 0.7584 - val_loss: 0.5865 - val_accuracy: 0.6762\n",
      "Epoch 84/500\n",
      "14/14 [==============================] - 0s 19ms/step - loss: 0.5085 - accuracy: 0.7727 - val_loss: 0.5856 - val_accuracy: 0.6952\n",
      "Epoch 85/500\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 0.5074 - accuracy: 0.7775 - val_loss: 0.5874 - val_accuracy: 0.6667\n",
      "Epoch 86/500\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 0.5030 - accuracy: 0.7679 - val_loss: 0.5836 - val_accuracy: 0.6952\n",
      "Epoch 87/500\n",
      "14/14 [==============================] - 0s 20ms/step - loss: 0.5075 - accuracy: 0.7632 - val_loss: 0.5858 - val_accuracy: 0.7048\n",
      "Epoch 88/500\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 0.5061 - accuracy: 0.7656 - val_loss: 0.5917 - val_accuracy: 0.6857\n",
      "Epoch 89/500\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 0.5042 - accuracy: 0.7584 - val_loss: 0.5867 - val_accuracy: 0.6952\n",
      "Epoch 90/500\n",
      "14/14 [==============================] - 0s 20ms/step - loss: 0.5117 - accuracy: 0.7727 - val_loss: 0.5840 - val_accuracy: 0.6952\n",
      "Epoch 91/500\n",
      "14/14 [==============================] - 0s 20ms/step - loss: 0.5081 - accuracy: 0.7751 - val_loss: 0.5840 - val_accuracy: 0.6952\n",
      "Epoch 92/500\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 0.5063 - accuracy: 0.7679 - val_loss: 0.5859 - val_accuracy: 0.6952\n",
      "Epoch 93/500\n",
      "14/14 [==============================] - 0s 20ms/step - loss: 0.4955 - accuracy: 0.7799 - val_loss: 0.5825 - val_accuracy: 0.7048\n",
      "Epoch 94/500\n",
      "14/14 [==============================] - 0s 20ms/step - loss: 0.5036 - accuracy: 0.7823 - val_loss: 0.5817 - val_accuracy: 0.7143\n",
      "Epoch 95/500\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 0.4983 - accuracy: 0.7727 - val_loss: 0.5883 - val_accuracy: 0.6857\n",
      "Epoch 96/500\n",
      "14/14 [==============================] - 0s 20ms/step - loss: 0.5014 - accuracy: 0.7679 - val_loss: 0.5849 - val_accuracy: 0.6952\n",
      "Epoch 97/500\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 0.4930 - accuracy: 0.7799 - val_loss: 0.5836 - val_accuracy: 0.7048\n",
      "Epoch 98/500\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 0.4968 - accuracy: 0.7775 - val_loss: 0.5866 - val_accuracy: 0.6952\n",
      "Epoch 99/500\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 0.4938 - accuracy: 0.7727 - val_loss: 0.5852 - val_accuracy: 0.6952\n",
      "Epoch 100/500\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 0.4981 - accuracy: 0.7727 - val_loss: 0.5862 - val_accuracy: 0.6952\n",
      "Epoch 101/500\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 0.5046 - accuracy: 0.7656 - val_loss: 0.5835 - val_accuracy: 0.6857\n",
      "Epoch 102/500\n",
      "14/14 [==============================] - 0s 20ms/step - loss: 0.4945 - accuracy: 0.7775 - val_loss: 0.5844 - val_accuracy: 0.6952\n",
      "Epoch 103/500\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 0.4892 - accuracy: 0.7799 - val_loss: 0.5820 - val_accuracy: 0.6857\n",
      "Epoch 104/500\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 0.4823 - accuracy: 0.7967 - val_loss: 0.5838 - val_accuracy: 0.6952\n",
      "Epoch 105/500\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 0.4909 - accuracy: 0.7775 - val_loss: 0.5842 - val_accuracy: 0.6952\n",
      "Epoch 106/500\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 0.4822 - accuracy: 0.8014 - val_loss: 0.5829 - val_accuracy: 0.6857\n",
      "Epoch 107/500\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 0.4918 - accuracy: 0.7727 - val_loss: 0.5839 - val_accuracy: 0.6952\n",
      "Epoch 108/500\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 0.4843 - accuracy: 0.7799 - val_loss: 0.5883 - val_accuracy: 0.6952\n",
      "Epoch 109/500\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 0.4850 - accuracy: 0.7871 - val_loss: 0.5832 - val_accuracy: 0.7048\n",
      "Epoch 110/500\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 0.4872 - accuracy: 0.7943 - val_loss: 0.5793 - val_accuracy: 0.6952\n",
      "Epoch 111/500\n",
      "14/14 [==============================] - 0s 19ms/step - loss: 0.4830 - accuracy: 0.7895 - val_loss: 0.5767 - val_accuracy: 0.7048\n",
      "Epoch 112/500\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 0.4805 - accuracy: 0.8014 - val_loss: 0.5777 - val_accuracy: 0.6952\n",
      "Epoch 113/500\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.4770 - accuracy: 0.7847 - val_loss: 0.5795 - val_accuracy: 0.6857\n",
      "Epoch 114/500\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.4771 - accuracy: 0.7871 - val_loss: 0.5857 - val_accuracy: 0.6857\n",
      "Epoch 115/500\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 0.4774 - accuracy: 0.7895 - val_loss: 0.5844 - val_accuracy: 0.6952\n",
      "Epoch 116/500\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 0.4813 - accuracy: 0.7919 - val_loss: 0.5812 - val_accuracy: 0.6952\n",
      "Epoch 117/500\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 0.4764 - accuracy: 0.7727 - val_loss: 0.5828 - val_accuracy: 0.6952\n",
      "Epoch 118/500\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 0.4786 - accuracy: 0.7895 - val_loss: 0.5839 - val_accuracy: 0.6952\n",
      "Epoch 119/500\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 0.4861 - accuracy: 0.7775 - val_loss: 0.5811 - val_accuracy: 0.7048\n",
      "Epoch 120/500\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 0.4775 - accuracy: 0.7871 - val_loss: 0.5860 - val_accuracy: 0.6952\n",
      "Epoch 121/500\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 0.4785 - accuracy: 0.7895 - val_loss: 0.5861 - val_accuracy: 0.6952\n",
      "Epoch 122/500\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 0.4703 - accuracy: 0.7943 - val_loss: 0.5873 - val_accuracy: 0.7048\n",
      "Epoch 123/500\n",
      "14/14 [==============================] - 0s 18ms/step - loss: 0.4710 - accuracy: 0.7943 - val_loss: 0.5847 - val_accuracy: 0.6952\n",
      "Epoch 124/500\n",
      "14/14 [==============================] - 0s 20ms/step - loss: 0.4775 - accuracy: 0.7823 - val_loss: 0.5839 - val_accuracy: 0.6952\n",
      "Epoch 125/500\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 0.4679 - accuracy: 0.7847 - val_loss: 0.5855 - val_accuracy: 0.7048\n",
      "Epoch 126/500\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 0.4703 - accuracy: 0.7823 - val_loss: 0.5838 - val_accuracy: 0.6952\n",
      "Epoch 127/500\n",
      "14/14 [==============================] - 0s 19ms/step - loss: 0.4684 - accuracy: 0.8062 - val_loss: 0.5850 - val_accuracy: 0.6952\n",
      "Epoch 128/500\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 0.4653 - accuracy: 0.8062 - val_loss: 0.5836 - val_accuracy: 0.6952\n",
      "Epoch 129/500\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 0.4647 - accuracy: 0.8014 - val_loss: 0.5861 - val_accuracy: 0.6952\n",
      "Epoch 130/500\n",
      "14/14 [==============================] - 0s 19ms/step - loss: 0.4632 - accuracy: 0.8086 - val_loss: 0.5877 - val_accuracy: 0.6952\n",
      "Epoch 131/500\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 0.4634 - accuracy: 0.7990 - val_loss: 0.5861 - val_accuracy: 0.6952\n",
      "Epoch 132/500\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 0.4566 - accuracy: 0.8038 - val_loss: 0.5921 - val_accuracy: 0.6952\n",
      "Epoch 133/500\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 0.4594 - accuracy: 0.7919 - val_loss: 0.5918 - val_accuracy: 0.6952\n",
      "Epoch 134/500\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 0.4588 - accuracy: 0.7895 - val_loss: 0.5934 - val_accuracy: 0.6952\n",
      "Epoch 135/500\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 0.4569 - accuracy: 0.8038 - val_loss: 0.5940 - val_accuracy: 0.6952\n",
      "Epoch 136/500\n",
      "14/14 [==============================] - 0s 20ms/step - loss: 0.4540 - accuracy: 0.8110 - val_loss: 0.5941 - val_accuracy: 0.6952\n",
      "Epoch 137/500\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 0.4617 - accuracy: 0.8134 - val_loss: 0.5860 - val_accuracy: 0.6762\n",
      "Epoch 138/500\n",
      "14/14 [==============================] - 0s 19ms/step - loss: 0.4501 - accuracy: 0.7967 - val_loss: 0.5883 - val_accuracy: 0.7048\n",
      "Epoch 139/500\n",
      "14/14 [==============================] - 0s 19ms/step - loss: 0.4494 - accuracy: 0.8110 - val_loss: 0.5881 - val_accuracy: 0.6857\n",
      "Epoch 140/500\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 0.4475 - accuracy: 0.8206 - val_loss: 0.5864 - val_accuracy: 0.6857\n",
      "Epoch 141/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14/14 [==============================] - 0s 20ms/step - loss: 0.4478 - accuracy: 0.8110 - val_loss: 0.5855 - val_accuracy: 0.6857\n",
      "Epoch 142/500\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 0.4406 - accuracy: 0.8086 - val_loss: 0.5893 - val_accuracy: 0.7048\n",
      "Epoch 143/500\n",
      "14/14 [==============================] - 0s 20ms/step - loss: 0.4472 - accuracy: 0.8158 - val_loss: 0.5902 - val_accuracy: 0.6762\n",
      "Epoch 144/500\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 0.4438 - accuracy: 0.8062 - val_loss: 0.5903 - val_accuracy: 0.6857\n",
      "Epoch 145/500\n",
      "14/14 [==============================] - 0s 20ms/step - loss: 0.4426 - accuracy: 0.8158 - val_loss: 0.5936 - val_accuracy: 0.6952\n",
      "Epoch 146/500\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 0.4467 - accuracy: 0.8158 - val_loss: 0.5896 - val_accuracy: 0.6857\n",
      "Epoch 147/500\n",
      "14/14 [==============================] - 0s 20ms/step - loss: 0.4360 - accuracy: 0.8134 - val_loss: 0.5886 - val_accuracy: 0.6762\n",
      "Epoch 148/500\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 0.4329 - accuracy: 0.8206 - val_loss: 0.5888 - val_accuracy: 0.6762\n",
      "Epoch 149/500\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 0.4393 - accuracy: 0.8086 - val_loss: 0.5902 - val_accuracy: 0.6857\n",
      "Epoch 150/500\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 0.4389 - accuracy: 0.8182 - val_loss: 0.5893 - val_accuracy: 0.6857\n",
      "Epoch 151/500\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 0.4502 - accuracy: 0.8038 - val_loss: 0.5845 - val_accuracy: 0.6762\n",
      "Epoch 152/500\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 0.4361 - accuracy: 0.8182 - val_loss: 0.5883 - val_accuracy: 0.7238\n",
      "Epoch 153/500\n",
      "14/14 [==============================] - 0s 19ms/step - loss: 0.4370 - accuracy: 0.8134 - val_loss: 0.5954 - val_accuracy: 0.6381\n",
      "Epoch 154/500\n",
      "14/14 [==============================] - 0s 20ms/step - loss: 0.4474 - accuracy: 0.8230 - val_loss: 0.5928 - val_accuracy: 0.6952\n",
      "Epoch 155/500\n",
      "14/14 [==============================] - 0s 20ms/step - loss: 0.4391 - accuracy: 0.8038 - val_loss: 0.5909 - val_accuracy: 0.7048\n",
      "Epoch 156/500\n",
      "14/14 [==============================] - 0s 20ms/step - loss: 0.4307 - accuracy: 0.8134 - val_loss: 0.5899 - val_accuracy: 0.6857\n",
      "Epoch 157/500\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 0.4242 - accuracy: 0.8134 - val_loss: 0.5898 - val_accuracy: 0.6952\n",
      "Epoch 158/500\n",
      "14/14 [==============================] - 0s 20ms/step - loss: 0.4334 - accuracy: 0.8038 - val_loss: 0.5927 - val_accuracy: 0.6952\n",
      "Epoch 159/500\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 0.4303 - accuracy: 0.8182 - val_loss: 0.5879 - val_accuracy: 0.6952\n",
      "Epoch 160/500\n",
      "14/14 [==============================] - 0s 20ms/step - loss: 0.4314 - accuracy: 0.8134 - val_loss: 0.5866 - val_accuracy: 0.6952\n",
      "Epoch 161/500\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 0.4290 - accuracy: 0.8086 - val_loss: 0.5901 - val_accuracy: 0.6667\n",
      "Epoch 162/500\n",
      "14/14 [==============================] - 0s 19ms/step - loss: 0.4146 - accuracy: 0.8397 - val_loss: 0.5863 - val_accuracy: 0.6952\n",
      "Epoch 163/500\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 0.4199 - accuracy: 0.8158 - val_loss: 0.5966 - val_accuracy: 0.6857\n",
      "Epoch 164/500\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 0.4239 - accuracy: 0.8278 - val_loss: 0.5951 - val_accuracy: 0.6857\n",
      "Epoch 165/500\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 0.4231 - accuracy: 0.8230 - val_loss: 0.5919 - val_accuracy: 0.6857\n",
      "Epoch 166/500\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 0.4236 - accuracy: 0.8230 - val_loss: 0.5955 - val_accuracy: 0.6857\n",
      "Epoch 167/500\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 0.4171 - accuracy: 0.8230 - val_loss: 0.5902 - val_accuracy: 0.6952\n",
      "Epoch 168/500\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 0.4175 - accuracy: 0.8278 - val_loss: 0.5951 - val_accuracy: 0.6857\n",
      "Epoch 169/500\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 0.4222 - accuracy: 0.8158 - val_loss: 0.5939 - val_accuracy: 0.6762\n",
      "Epoch 170/500\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 0.4150 - accuracy: 0.8254 - val_loss: 0.5955 - val_accuracy: 0.6952\n",
      "Epoch 171/500\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 0.4164 - accuracy: 0.8254 - val_loss: 0.5959 - val_accuracy: 0.6952\n",
      "Epoch 172/500\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 0.4163 - accuracy: 0.8397 - val_loss: 0.5950 - val_accuracy: 0.6952\n",
      "Epoch 173/500\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 0.4055 - accuracy: 0.8278 - val_loss: 0.5909 - val_accuracy: 0.6952\n",
      "Epoch 174/500\n",
      "14/14 [==============================] - 0s 20ms/step - loss: 0.4094 - accuracy: 0.8349 - val_loss: 0.5935 - val_accuracy: 0.6952\n",
      "Epoch 175/500\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 0.4099 - accuracy: 0.8445 - val_loss: 0.5937 - val_accuracy: 0.6952\n",
      "Epoch 176/500\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 0.4174 - accuracy: 0.8301 - val_loss: 0.5977 - val_accuracy: 0.7048\n",
      "Epoch 177/500\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 0.4084 - accuracy: 0.8349 - val_loss: 0.6036 - val_accuracy: 0.6571\n",
      "Epoch 178/500\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 0.4217 - accuracy: 0.8254 - val_loss: 0.5984 - val_accuracy: 0.7238\n",
      "Epoch 179/500\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 0.4016 - accuracy: 0.8301 - val_loss: 0.5970 - val_accuracy: 0.6952\n",
      "Epoch 180/500\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 0.4079 - accuracy: 0.8254 - val_loss: 0.5957 - val_accuracy: 0.7048\n",
      "Epoch 181/500\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.4073 - accuracy: 0.8254 - val_loss: 0.5946 - val_accuracy: 0.6857\n",
      "Epoch 182/500\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 0.4062 - accuracy: 0.8445 - val_loss: 0.5974 - val_accuracy: 0.6857\n",
      "Epoch 183/500\n",
      "14/14 [==============================] - 0s 30ms/step - loss: 0.4070 - accuracy: 0.8445 - val_loss: 0.5934 - val_accuracy: 0.6952\n",
      "Epoch 184/500\n",
      "14/14 [==============================] - 0s 32ms/step - loss: 0.4063 - accuracy: 0.8230 - val_loss: 0.5967 - val_accuracy: 0.7238\n",
      "Epoch 185/500\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 0.4055 - accuracy: 0.8254 - val_loss: 0.5997 - val_accuracy: 0.6952\n",
      "Epoch 186/500\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 0.4051 - accuracy: 0.8373 - val_loss: 0.5997 - val_accuracy: 0.7143\n",
      "Epoch 187/500\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 0.3947 - accuracy: 0.8445 - val_loss: 0.5985 - val_accuracy: 0.6952\n",
      "Epoch 188/500\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 0.3914 - accuracy: 0.8541 - val_loss: 0.5998 - val_accuracy: 0.7143\n",
      "Epoch 189/500\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 0.3901 - accuracy: 0.8517 - val_loss: 0.5992 - val_accuracy: 0.7048\n",
      "Epoch 190/500\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.3915 - accuracy: 0.8445 - val_loss: 0.5961 - val_accuracy: 0.7143\n",
      "Epoch 191/500\n",
      "14/14 [==============================] - 0s 29ms/step - loss: 0.3972 - accuracy: 0.8349 - val_loss: 0.5970 - val_accuracy: 0.6952\n",
      "Epoch 192/500\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 0.4062 - accuracy: 0.8421 - val_loss: 0.5993 - val_accuracy: 0.7048\n",
      "Epoch 193/500\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 0.4029 - accuracy: 0.8301 - val_loss: 0.6049 - val_accuracy: 0.7143\n",
      "Epoch 194/500\n",
      "14/14 [==============================] - 0s 29ms/step - loss: 0.3879 - accuracy: 0.8397 - val_loss: 0.5949 - val_accuracy: 0.7048\n",
      "Epoch 195/500\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.3927 - accuracy: 0.8469 - val_loss: 0.5961 - val_accuracy: 0.7143\n",
      "Epoch 196/500\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 0.3923 - accuracy: 0.8517 - val_loss: 0.5999 - val_accuracy: 0.7048\n",
      "Epoch 197/500\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 0.3951 - accuracy: 0.8445 - val_loss: 0.6024 - val_accuracy: 0.7143\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 198/500\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 0.3865 - accuracy: 0.8397 - val_loss: 0.6020 - val_accuracy: 0.7143\n",
      "Epoch 199/500\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 0.3912 - accuracy: 0.8445 - val_loss: 0.6077 - val_accuracy: 0.7143\n",
      "Epoch 200/500\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 0.3913 - accuracy: 0.8349 - val_loss: 0.6033 - val_accuracy: 0.7143\n",
      "Epoch 201/500\n",
      "14/14 [==============================] - 0s 20ms/step - loss: 0.3867 - accuracy: 0.8469 - val_loss: 0.5997 - val_accuracy: 0.7143\n",
      "Epoch 202/500\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 0.3792 - accuracy: 0.8517 - val_loss: 0.6028 - val_accuracy: 0.7143\n",
      "Epoch 203/500\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 0.3814 - accuracy: 0.8397 - val_loss: 0.6076 - val_accuracy: 0.7048\n",
      "Epoch 204/500\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 0.3852 - accuracy: 0.8421 - val_loss: 0.6109 - val_accuracy: 0.7143\n",
      "Epoch 205/500\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.3886 - accuracy: 0.8493 - val_loss: 0.6091 - val_accuracy: 0.6571\n",
      "Epoch 206/500\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.3883 - accuracy: 0.8469 - val_loss: 0.6021 - val_accuracy: 0.7238\n",
      "Epoch 207/500\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 0.3819 - accuracy: 0.8397 - val_loss: 0.6057 - val_accuracy: 0.7048\n",
      "Epoch 208/500\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 0.3783 - accuracy: 0.8565 - val_loss: 0.6058 - val_accuracy: 0.7048\n",
      "Epoch 209/500\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 0.3822 - accuracy: 0.8469 - val_loss: 0.5979 - val_accuracy: 0.7048\n",
      "Epoch 210/500\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 0.3772 - accuracy: 0.8565 - val_loss: 0.6042 - val_accuracy: 0.7048\n",
      "Epoch 211/500\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 0.3815 - accuracy: 0.8541 - val_loss: 0.6083 - val_accuracy: 0.7048\n",
      "Epoch 212/500\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 0.3729 - accuracy: 0.8445 - val_loss: 0.6020 - val_accuracy: 0.7143\n",
      "Epoch 213/500\n",
      "14/14 [==============================] - 0s 31ms/step - loss: 0.3703 - accuracy: 0.8589 - val_loss: 0.6050 - val_accuracy: 0.7143\n",
      "Epoch 214/500\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 0.3604 - accuracy: 0.8660 - val_loss: 0.6120 - val_accuracy: 0.6762\n",
      "Epoch 215/500\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 0.3787 - accuracy: 0.8517 - val_loss: 0.6094 - val_accuracy: 0.6952\n",
      "Epoch 216/500\n",
      "14/14 [==============================] - 0s 30ms/step - loss: 0.3721 - accuracy: 0.8732 - val_loss: 0.6033 - val_accuracy: 0.7143\n",
      "Epoch 217/500\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 0.3683 - accuracy: 0.8517 - val_loss: 0.6111 - val_accuracy: 0.7048\n",
      "Epoch 218/500\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 0.3653 - accuracy: 0.8589 - val_loss: 0.6111 - val_accuracy: 0.6952\n",
      "Epoch 219/500\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 0.3674 - accuracy: 0.8565 - val_loss: 0.6088 - val_accuracy: 0.7143\n",
      "Epoch 220/500\n",
      "14/14 [==============================] - 0s 31ms/step - loss: 0.3719 - accuracy: 0.8541 - val_loss: 0.6081 - val_accuracy: 0.7143\n",
      "Epoch 221/500\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 0.3565 - accuracy: 0.8660 - val_loss: 0.6115 - val_accuracy: 0.6952\n",
      "Epoch 222/500\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 0.3637 - accuracy: 0.8684 - val_loss: 0.6140 - val_accuracy: 0.7143\n",
      "Epoch 223/500\n",
      "14/14 [==============================] - 0s 29ms/step - loss: 0.3590 - accuracy: 0.8684 - val_loss: 0.6186 - val_accuracy: 0.6952\n",
      "Epoch 224/500\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.3695 - accuracy: 0.8565 - val_loss: 0.6186 - val_accuracy: 0.6952\n",
      "Epoch 225/500\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 0.3608 - accuracy: 0.8469 - val_loss: 0.6140 - val_accuracy: 0.7048\n",
      "Epoch 226/500\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 0.3460 - accuracy: 0.8732 - val_loss: 0.6131 - val_accuracy: 0.7143\n",
      "Epoch 227/500\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 0.3630 - accuracy: 0.8541 - val_loss: 0.6186 - val_accuracy: 0.6952\n",
      "Epoch 228/500\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 0.3582 - accuracy: 0.8612 - val_loss: 0.6229 - val_accuracy: 0.6762\n",
      "Epoch 229/500\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 0.3601 - accuracy: 0.8660 - val_loss: 0.6152 - val_accuracy: 0.7048\n",
      "Epoch 230/500\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 0.3474 - accuracy: 0.8756 - val_loss: 0.6164 - val_accuracy: 0.7143\n",
      "Epoch 231/500\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 0.3479 - accuracy: 0.8708 - val_loss: 0.6199 - val_accuracy: 0.7143\n",
      "Epoch 232/500\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 0.3532 - accuracy: 0.8589 - val_loss: 0.6231 - val_accuracy: 0.6857\n",
      "Epoch 233/500\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 0.3638 - accuracy: 0.8565 - val_loss: 0.6164 - val_accuracy: 0.6952\n",
      "Epoch 234/500\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 0.3523 - accuracy: 0.8708 - val_loss: 0.6131 - val_accuracy: 0.7143\n",
      "Epoch 235/500\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 0.3442 - accuracy: 0.8612 - val_loss: 0.6149 - val_accuracy: 0.6952\n",
      "Epoch 236/500\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 0.3508 - accuracy: 0.8565 - val_loss: 0.6240 - val_accuracy: 0.6857\n",
      "Epoch 237/500\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 0.3489 - accuracy: 0.8732 - val_loss: 0.6250 - val_accuracy: 0.6762\n",
      "Epoch 238/500\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 0.3430 - accuracy: 0.8804 - val_loss: 0.6248 - val_accuracy: 0.6952\n",
      "Epoch 239/500\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 0.3453 - accuracy: 0.8708 - val_loss: 0.6296 - val_accuracy: 0.7048\n",
      "Epoch 240/500\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 0.3468 - accuracy: 0.8732 - val_loss: 0.6193 - val_accuracy: 0.6952\n",
      "Epoch 241/500\n",
      "14/14 [==============================] - 0s 19ms/step - loss: 0.3480 - accuracy: 0.8684 - val_loss: 0.6159 - val_accuracy: 0.6952\n",
      "Epoch 242/500\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 0.3444 - accuracy: 0.8636 - val_loss: 0.6149 - val_accuracy: 0.7048\n",
      "Epoch 243/500\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 0.3468 - accuracy: 0.8732 - val_loss: 0.6192 - val_accuracy: 0.7048\n",
      "Epoch 244/500\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.3371 - accuracy: 0.8756 - val_loss: 0.6185 - val_accuracy: 0.7048\n",
      "Epoch 245/500\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.3442 - accuracy: 0.8756 - val_loss: 0.6238 - val_accuracy: 0.6952\n",
      "Epoch 246/500\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 0.3385 - accuracy: 0.8684 - val_loss: 0.6243 - val_accuracy: 0.6952\n",
      "Epoch 247/500\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 0.3512 - accuracy: 0.8565 - val_loss: 0.6228 - val_accuracy: 0.6952\n",
      "Epoch 248/500\n",
      "14/14 [==============================] - 0s 20ms/step - loss: 0.3589 - accuracy: 0.8684 - val_loss: 0.6298 - val_accuracy: 0.6667\n",
      "Epoch 249/500\n",
      "14/14 [==============================] - 0s 20ms/step - loss: 0.3430 - accuracy: 0.8708 - val_loss: 0.6214 - val_accuracy: 0.7333\n",
      "Epoch 250/500\n",
      "14/14 [==============================] - 0s 20ms/step - loss: 0.3415 - accuracy: 0.8684 - val_loss: 0.6335 - val_accuracy: 0.6571\n",
      "Epoch 251/500\n",
      "14/14 [==============================] - 0s 18ms/step - loss: 0.3385 - accuracy: 0.8828 - val_loss: 0.6247 - val_accuracy: 0.7048\n",
      "Epoch 252/500\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 0.3251 - accuracy: 0.8876 - val_loss: 0.6178 - val_accuracy: 0.7238\n",
      "Epoch 253/500\n",
      "14/14 [==============================] - 0s 20ms/step - loss: 0.3340 - accuracy: 0.8756 - val_loss: 0.6260 - val_accuracy: 0.7048\n",
      "Epoch 254/500\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 0.3247 - accuracy: 0.8804 - val_loss: 0.6299 - val_accuracy: 0.6952\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 255/500\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 0.3236 - accuracy: 0.8828 - val_loss: 0.6255 - val_accuracy: 0.7143\n",
      "Epoch 256/500\n",
      "14/14 [==============================] - 0s 19ms/step - loss: 0.3247 - accuracy: 0.8852 - val_loss: 0.6170 - val_accuracy: 0.6857\n",
      "Epoch 257/500\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 0.3419 - accuracy: 0.8780 - val_loss: 0.6199 - val_accuracy: 0.6667\n",
      "Epoch 258/500\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 0.3315 - accuracy: 0.8780 - val_loss: 0.6222 - val_accuracy: 0.6857\n",
      "Epoch 259/500\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 0.3415 - accuracy: 0.8708 - val_loss: 0.6200 - val_accuracy: 0.6762\n",
      "Epoch 260/500\n",
      "14/14 [==============================] - 0s 20ms/step - loss: 0.3229 - accuracy: 0.8780 - val_loss: 0.6153 - val_accuracy: 0.7048\n",
      "Epoch 261/500\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 0.3185 - accuracy: 0.8852 - val_loss: 0.6176 - val_accuracy: 0.6857\n",
      "Epoch 262/500\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 0.3259 - accuracy: 0.8852 - val_loss: 0.6215 - val_accuracy: 0.6952\n",
      "Epoch 263/500\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 0.3313 - accuracy: 0.8541 - val_loss: 0.6343 - val_accuracy: 0.6857\n",
      "Epoch 264/500\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 0.3184 - accuracy: 0.8923 - val_loss: 0.6453 - val_accuracy: 0.6762\n",
      "Epoch 265/500\n",
      "14/14 [==============================] - 0s 20ms/step - loss: 0.3141 - accuracy: 0.8995 - val_loss: 0.6356 - val_accuracy: 0.6952\n",
      "Epoch 266/500\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 0.3176 - accuracy: 0.8828 - val_loss: 0.6364 - val_accuracy: 0.6857\n",
      "Epoch 267/500\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 0.3238 - accuracy: 0.8804 - val_loss: 0.6299 - val_accuracy: 0.7048\n",
      "Epoch 268/500\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 0.3126 - accuracy: 0.8780 - val_loss: 0.6272 - val_accuracy: 0.7143\n",
      "Epoch 269/500\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 0.3153 - accuracy: 0.8876 - val_loss: 0.6351 - val_accuracy: 0.7143\n",
      "Epoch 270/500\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 0.3215 - accuracy: 0.8708 - val_loss: 0.6490 - val_accuracy: 0.6571\n",
      "Epoch 271/500\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 0.3157 - accuracy: 0.8876 - val_loss: 0.6294 - val_accuracy: 0.7048\n",
      "Epoch 272/500\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.3122 - accuracy: 0.8900 - val_loss: 0.6213 - val_accuracy: 0.7048\n",
      "Epoch 273/500\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.3105 - accuracy: 0.8923 - val_loss: 0.6264 - val_accuracy: 0.6952\n",
      "Epoch 274/500\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 0.3172 - accuracy: 0.8852 - val_loss: 0.6341 - val_accuracy: 0.6857\n",
      "Epoch 275/500\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 0.3104 - accuracy: 0.8804 - val_loss: 0.6339 - val_accuracy: 0.7048\n",
      "Epoch 276/500\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 0.3031 - accuracy: 0.8971 - val_loss: 0.6336 - val_accuracy: 0.7048\n",
      "Epoch 277/500\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.3037 - accuracy: 0.8947 - val_loss: 0.6367 - val_accuracy: 0.6762\n",
      "Epoch 278/500\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.3155 - accuracy: 0.8780 - val_loss: 0.6340 - val_accuracy: 0.7048\n",
      "Epoch 279/500\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 0.3119 - accuracy: 0.8923 - val_loss: 0.6288 - val_accuracy: 0.7238\n",
      "Epoch 280/500\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 0.3126 - accuracy: 0.8732 - val_loss: 0.6294 - val_accuracy: 0.6952\n",
      "Epoch 281/500\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 0.3042 - accuracy: 0.8876 - val_loss: 0.6372 - val_accuracy: 0.6952\n",
      "Epoch 282/500\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 0.2956 - accuracy: 0.8947 - val_loss: 0.6380 - val_accuracy: 0.7143\n",
      "Epoch 283/500\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 0.2990 - accuracy: 0.8947 - val_loss: 0.6394 - val_accuracy: 0.7048\n",
      "Epoch 284/500\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 0.3024 - accuracy: 0.8947 - val_loss: 0.6411 - val_accuracy: 0.6857\n",
      "Epoch 285/500\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 0.3103 - accuracy: 0.8852 - val_loss: 0.6399 - val_accuracy: 0.6952\n",
      "Epoch 286/500\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 0.2886 - accuracy: 0.8900 - val_loss: 0.6243 - val_accuracy: 0.7048\n",
      "Epoch 287/500\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 0.2927 - accuracy: 0.8995 - val_loss: 0.6362 - val_accuracy: 0.7048\n",
      "Epoch 288/500\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 0.3059 - accuracy: 0.8708 - val_loss: 0.6494 - val_accuracy: 0.7143\n",
      "Epoch 289/500\n",
      "14/14 [==============================] - 0s 20ms/step - loss: 0.2873 - accuracy: 0.8900 - val_loss: 0.6490 - val_accuracy: 0.7048\n",
      "Epoch 290/500\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 0.3023 - accuracy: 0.8923 - val_loss: 0.6486 - val_accuracy: 0.7048\n",
      "Epoch 291/500\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 0.2985 - accuracy: 0.8876 - val_loss: 0.6432 - val_accuracy: 0.6857\n",
      "Epoch 292/500\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 0.2974 - accuracy: 0.8900 - val_loss: 0.6422 - val_accuracy: 0.6952\n",
      "Epoch 293/500\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 0.2872 - accuracy: 0.8947 - val_loss: 0.6384 - val_accuracy: 0.7048\n",
      "Epoch 294/500\n",
      "14/14 [==============================] - 0s 20ms/step - loss: 0.2943 - accuracy: 0.8923 - val_loss: 0.6470 - val_accuracy: 0.6952\n",
      "Epoch 295/500\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 0.2940 - accuracy: 0.9019 - val_loss: 0.6464 - val_accuracy: 0.7048\n",
      "Epoch 296/500\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 0.2932 - accuracy: 0.8923 - val_loss: 0.6490 - val_accuracy: 0.7143\n",
      "Epoch 297/500\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 0.2910 - accuracy: 0.8923 - val_loss: 0.6437 - val_accuracy: 0.7048\n",
      "Epoch 298/500\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 0.2860 - accuracy: 0.8971 - val_loss: 0.6578 - val_accuracy: 0.6762\n",
      "Epoch 299/500\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 0.2944 - accuracy: 0.8923 - val_loss: 0.6523 - val_accuracy: 0.7048\n",
      "Epoch 300/500\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 0.2820 - accuracy: 0.9067 - val_loss: 0.6541 - val_accuracy: 0.6857\n",
      "Epoch 301/500\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 0.2852 - accuracy: 0.8995 - val_loss: 0.6556 - val_accuracy: 0.6952\n",
      "Epoch 302/500\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 0.2796 - accuracy: 0.8947 - val_loss: 0.6528 - val_accuracy: 0.7048\n",
      "Epoch 303/500\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 0.2886 - accuracy: 0.8995 - val_loss: 0.6597 - val_accuracy: 0.6762\n",
      "Epoch 304/500\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 0.2828 - accuracy: 0.8900 - val_loss: 0.6594 - val_accuracy: 0.6952\n",
      "Epoch 305/500\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.2807 - accuracy: 0.9043 - val_loss: 0.6535 - val_accuracy: 0.6857\n",
      "Epoch 306/500\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 0.2840 - accuracy: 0.8923 - val_loss: 0.6555 - val_accuracy: 0.7238\n",
      "Epoch 307/500\n",
      "14/14 [==============================] - 0s 20ms/step - loss: 0.2827 - accuracy: 0.8900 - val_loss: 0.6640 - val_accuracy: 0.6762\n",
      "Epoch 308/500\n",
      "14/14 [==============================] - 0s 20ms/step - loss: 0.2779 - accuracy: 0.9043 - val_loss: 0.6618 - val_accuracy: 0.7143\n",
      "Epoch 309/500\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 0.2888 - accuracy: 0.8971 - val_loss: 0.6605 - val_accuracy: 0.6952\n",
      "Epoch 310/500\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 0.2806 - accuracy: 0.8876 - val_loss: 0.6626 - val_accuracy: 0.6857\n",
      "Epoch 311/500\n",
      "14/14 [==============================] - 0s 20ms/step - loss: 0.2804 - accuracy: 0.8947 - val_loss: 0.6602 - val_accuracy: 0.7048\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 312/500\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 0.2759 - accuracy: 0.8971 - val_loss: 0.6650 - val_accuracy: 0.6762\n",
      "Epoch 313/500\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.2635 - accuracy: 0.9091 - val_loss: 0.6696 - val_accuracy: 0.6857\n",
      "Epoch 314/500\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 0.2756 - accuracy: 0.9043 - val_loss: 0.6721 - val_accuracy: 0.6857\n",
      "Epoch 315/500\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 0.2613 - accuracy: 0.9115 - val_loss: 0.6671 - val_accuracy: 0.6952\n",
      "Epoch 316/500\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 0.2697 - accuracy: 0.9043 - val_loss: 0.6637 - val_accuracy: 0.6952\n",
      "Epoch 317/500\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 0.2700 - accuracy: 0.8995 - val_loss: 0.6624 - val_accuracy: 0.7048\n",
      "Epoch 318/500\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.2726 - accuracy: 0.9091 - val_loss: 0.6637 - val_accuracy: 0.6762\n",
      "Epoch 319/500\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.2630 - accuracy: 0.9115 - val_loss: 0.6615 - val_accuracy: 0.6857\n",
      "Epoch 320/500\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 0.2697 - accuracy: 0.9091 - val_loss: 0.6688 - val_accuracy: 0.6857\n",
      "Epoch 321/500\n",
      "14/14 [==============================] - 0s 20ms/step - loss: 0.2617 - accuracy: 0.9019 - val_loss: 0.6671 - val_accuracy: 0.7048\n",
      "Epoch 322/500\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 0.2672 - accuracy: 0.8971 - val_loss: 0.6666 - val_accuracy: 0.7143\n",
      "Epoch 323/500\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 0.2667 - accuracy: 0.8995 - val_loss: 0.6657 - val_accuracy: 0.7048\n",
      "Epoch 324/500\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 0.2689 - accuracy: 0.8900 - val_loss: 0.6663 - val_accuracy: 0.6952\n",
      "Epoch 325/500\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 0.2672 - accuracy: 0.8995 - val_loss: 0.6783 - val_accuracy: 0.6857\n",
      "Epoch 326/500\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 0.2598 - accuracy: 0.8995 - val_loss: 0.6722 - val_accuracy: 0.7048\n",
      "Epoch 327/500\n",
      "14/14 [==============================] - 0s 19ms/step - loss: 0.2621 - accuracy: 0.9139 - val_loss: 0.6778 - val_accuracy: 0.6857\n",
      "Epoch 328/500\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 0.2597 - accuracy: 0.9163 - val_loss: 0.6679 - val_accuracy: 0.7143\n",
      "Epoch 329/500\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 0.2652 - accuracy: 0.8876 - val_loss: 0.6722 - val_accuracy: 0.7333\n",
      "Epoch 330/500\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 0.2628 - accuracy: 0.9043 - val_loss: 0.6933 - val_accuracy: 0.6476\n",
      "Epoch 331/500\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 0.2676 - accuracy: 0.9115 - val_loss: 0.6802 - val_accuracy: 0.6952\n",
      "Epoch 332/500\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 0.2571 - accuracy: 0.9115 - val_loss: 0.6751 - val_accuracy: 0.7143\n",
      "Epoch 333/500\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 0.2600 - accuracy: 0.9139 - val_loss: 0.6725 - val_accuracy: 0.6952\n",
      "Epoch 334/500\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 0.2608 - accuracy: 0.9139 - val_loss: 0.6786 - val_accuracy: 0.6952\n",
      "Epoch 335/500\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 0.2519 - accuracy: 0.9139 - val_loss: 0.6929 - val_accuracy: 0.6667\n",
      "Epoch 336/500\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 0.2621 - accuracy: 0.9211 - val_loss: 0.6792 - val_accuracy: 0.7333\n",
      "Epoch 337/500\n",
      "14/14 [==============================] - 0s 31ms/step - loss: 0.2577 - accuracy: 0.9163 - val_loss: 0.6843 - val_accuracy: 0.7048\n",
      "Epoch 338/500\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 0.2445 - accuracy: 0.9115 - val_loss: 0.6768 - val_accuracy: 0.7143\n",
      "Epoch 339/500\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 0.2522 - accuracy: 0.9211 - val_loss: 0.6754 - val_accuracy: 0.6952\n",
      "Epoch 340/500\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 0.2455 - accuracy: 0.9043 - val_loss: 0.6897 - val_accuracy: 0.6476\n",
      "Epoch 341/500\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 0.2518 - accuracy: 0.9187 - val_loss: 0.6760 - val_accuracy: 0.7143\n",
      "Epoch 342/500\n",
      "14/14 [==============================] - 0s 20ms/step - loss: 0.2621 - accuracy: 0.9019 - val_loss: 0.6832 - val_accuracy: 0.7048\n",
      "Epoch 343/500\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.2483 - accuracy: 0.9163 - val_loss: 0.6994 - val_accuracy: 0.6571\n",
      "Epoch 344/500\n",
      "14/14 [==============================] - 0s 29ms/step - loss: 0.2479 - accuracy: 0.9091 - val_loss: 0.6847 - val_accuracy: 0.7143\n",
      "Epoch 345/500\n",
      "14/14 [==============================] - 0s 19ms/step - loss: 0.2519 - accuracy: 0.9139 - val_loss: 0.6853 - val_accuracy: 0.6762\n",
      "Epoch 346/500\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 0.2491 - accuracy: 0.9163 - val_loss: 0.6894 - val_accuracy: 0.7143\n",
      "Epoch 347/500\n",
      "14/14 [==============================] - 0s 32ms/step - loss: 0.2452 - accuracy: 0.9139 - val_loss: 0.6869 - val_accuracy: 0.6857\n",
      "Epoch 348/500\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.2437 - accuracy: 0.9067 - val_loss: 0.6851 - val_accuracy: 0.6952\n",
      "Epoch 349/500\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.2402 - accuracy: 0.9043 - val_loss: 0.6885 - val_accuracy: 0.7143\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7ffd1a7af820> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 1/500\n",
      "14/14 [==============================] - 1s 41ms/step - loss: 0.6199 - accuracy: 0.6890 - val_loss: 0.5922 - val_accuracy: 0.7048\n",
      "Epoch 2/500\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 0.5979 - accuracy: 0.7057 - val_loss: 0.5922 - val_accuracy: 0.7048\n",
      "Epoch 3/500\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 0.5998 - accuracy: 0.7057 - val_loss: 0.5883 - val_accuracy: 0.7048\n",
      "Epoch 4/500\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 0.5911 - accuracy: 0.7057 - val_loss: 0.5871 - val_accuracy: 0.7048\n",
      "Epoch 5/500\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 0.5883 - accuracy: 0.7057 - val_loss: 0.5864 - val_accuracy: 0.7048\n",
      "Epoch 6/500\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 0.5867 - accuracy: 0.7057 - val_loss: 0.5858 - val_accuracy: 0.7048\n",
      "Epoch 7/500\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 0.5867 - accuracy: 0.7057 - val_loss: 0.5872 - val_accuracy: 0.7048\n",
      "Epoch 8/500\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 0.5845 - accuracy: 0.7057 - val_loss: 0.5849 - val_accuracy: 0.7048\n",
      "Epoch 9/500\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 0.5804 - accuracy: 0.7057 - val_loss: 0.5852 - val_accuracy: 0.7048\n",
      "Epoch 10/500\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 0.5849 - accuracy: 0.7057 - val_loss: 0.5855 - val_accuracy: 0.7048\n",
      "Epoch 11/500\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 0.5783 - accuracy: 0.7057 - val_loss: 0.5876 - val_accuracy: 0.7048\n",
      "Epoch 12/500\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 0.5755 - accuracy: 0.7057 - val_loss: 0.5834 - val_accuracy: 0.7048\n",
      "Epoch 13/500\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 0.5730 - accuracy: 0.7057 - val_loss: 0.5832 - val_accuracy: 0.7048\n",
      "Epoch 14/500\n",
      "14/14 [==============================] - 0s 30ms/step - loss: 0.5717 - accuracy: 0.7057 - val_loss: 0.5838 - val_accuracy: 0.7048\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/500\n",
      "14/14 [==============================] - 0s 31ms/step - loss: 0.5738 - accuracy: 0.7057 - val_loss: 0.5848 - val_accuracy: 0.7048\n",
      "Epoch 16/500\n",
      "14/14 [==============================] - 0s 34ms/step - loss: 0.5664 - accuracy: 0.7057 - val_loss: 0.5823 - val_accuracy: 0.7048\n",
      "Epoch 17/500\n",
      "14/14 [==============================] - 0s 31ms/step - loss: 0.5658 - accuracy: 0.7057 - val_loss: 0.5800 - val_accuracy: 0.7048\n",
      "Epoch 18/500\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 0.5683 - accuracy: 0.7057 - val_loss: 0.5805 - val_accuracy: 0.7048\n",
      "Epoch 19/500\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 0.5635 - accuracy: 0.7081 - val_loss: 0.5831 - val_accuracy: 0.7048\n",
      "Epoch 20/500\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 0.5620 - accuracy: 0.7081 - val_loss: 0.5832 - val_accuracy: 0.7048\n",
      "Epoch 21/500\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 0.5611 - accuracy: 0.7057 - val_loss: 0.5819 - val_accuracy: 0.7048\n",
      "Epoch 22/500\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 0.5588 - accuracy: 0.7057 - val_loss: 0.5828 - val_accuracy: 0.7048\n",
      "Epoch 23/500\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 0.5588 - accuracy: 0.7057 - val_loss: 0.5836 - val_accuracy: 0.7048\n",
      "Epoch 24/500\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 0.5572 - accuracy: 0.7033 - val_loss: 0.5829 - val_accuracy: 0.7048\n",
      "Epoch 25/500\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 0.5541 - accuracy: 0.7105 - val_loss: 0.5850 - val_accuracy: 0.7048\n",
      "Epoch 26/500\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 0.5553 - accuracy: 0.7057 - val_loss: 0.5847 - val_accuracy: 0.7048\n",
      "Epoch 27/500\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.5545 - accuracy: 0.7105 - val_loss: 0.5869 - val_accuracy: 0.7048\n",
      "Epoch 28/500\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 0.5571 - accuracy: 0.7081 - val_loss: 0.5860 - val_accuracy: 0.7143\n",
      "Epoch 29/500\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 0.5506 - accuracy: 0.7129 - val_loss: 0.5885 - val_accuracy: 0.7143\n",
      "Epoch 30/500\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 0.5520 - accuracy: 0.7153 - val_loss: 0.5872 - val_accuracy: 0.7048\n",
      "Epoch 31/500\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 0.5550 - accuracy: 0.7225 - val_loss: 0.5884 - val_accuracy: 0.6952\n",
      "Epoch 32/500\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 0.5458 - accuracy: 0.7225 - val_loss: 0.5939 - val_accuracy: 0.7048\n",
      "Epoch 33/500\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 0.5471 - accuracy: 0.7177 - val_loss: 0.5873 - val_accuracy: 0.7048\n",
      "Epoch 34/500\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 0.5451 - accuracy: 0.7225 - val_loss: 0.5893 - val_accuracy: 0.7048\n",
      "Epoch 35/500\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 0.5445 - accuracy: 0.7177 - val_loss: 0.5903 - val_accuracy: 0.7048\n",
      "Epoch 36/500\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.5405 - accuracy: 0.7225 - val_loss: 0.5900 - val_accuracy: 0.7048\n",
      "Epoch 37/500\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 0.5474 - accuracy: 0.7249 - val_loss: 0.5907 - val_accuracy: 0.6952\n",
      "Epoch 38/500\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 0.5400 - accuracy: 0.7177 - val_loss: 0.5930 - val_accuracy: 0.7048\n",
      "Epoch 39/500\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 0.5428 - accuracy: 0.7201 - val_loss: 0.5931 - val_accuracy: 0.6952\n",
      "Epoch 40/500\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 0.5391 - accuracy: 0.7249 - val_loss: 0.5950 - val_accuracy: 0.7048\n",
      "Epoch 41/500\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 0.5369 - accuracy: 0.7153 - val_loss: 0.5968 - val_accuracy: 0.7048\n",
      "Epoch 42/500\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.5386 - accuracy: 0.7249 - val_loss: 0.5963 - val_accuracy: 0.6952\n",
      "Epoch 43/500\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 0.5365 - accuracy: 0.7297 - val_loss: 0.5963 - val_accuracy: 0.6952\n",
      "Epoch 44/500\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 0.5333 - accuracy: 0.7368 - val_loss: 0.5934 - val_accuracy: 0.6952\n",
      "Epoch 45/500\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 0.5326 - accuracy: 0.7225 - val_loss: 0.5954 - val_accuracy: 0.7048\n",
      "Epoch 46/500\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 0.5386 - accuracy: 0.7273 - val_loss: 0.5944 - val_accuracy: 0.6952\n",
      "Epoch 47/500\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 0.5327 - accuracy: 0.7273 - val_loss: 0.5957 - val_accuracy: 0.6952\n",
      "Epoch 48/500\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 0.5315 - accuracy: 0.7321 - val_loss: 0.5953 - val_accuracy: 0.6952\n",
      "Epoch 49/500\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 0.5332 - accuracy: 0.7249 - val_loss: 0.5948 - val_accuracy: 0.6952\n",
      "Epoch 50/500\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 0.5326 - accuracy: 0.7273 - val_loss: 0.5985 - val_accuracy: 0.6952\n",
      "Epoch 51/500\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 0.5280 - accuracy: 0.7225 - val_loss: 0.5976 - val_accuracy: 0.6952\n",
      "Epoch 52/500\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 0.5278 - accuracy: 0.7249 - val_loss: 0.6007 - val_accuracy: 0.6952\n",
      "Epoch 53/500\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 0.5277 - accuracy: 0.7297 - val_loss: 0.5993 - val_accuracy: 0.6857\n",
      "Epoch 54/500\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.5230 - accuracy: 0.7392 - val_loss: 0.6007 - val_accuracy: 0.6857\n",
      "Epoch 55/500\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 0.5262 - accuracy: 0.7297 - val_loss: 0.5986 - val_accuracy: 0.6762\n",
      "Epoch 56/500\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 0.5235 - accuracy: 0.7321 - val_loss: 0.6007 - val_accuracy: 0.6857\n",
      "Epoch 57/500\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 0.5232 - accuracy: 0.7297 - val_loss: 0.6011 - val_accuracy: 0.6857\n",
      "Epoch 58/500\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 0.5299 - accuracy: 0.7416 - val_loss: 0.5991 - val_accuracy: 0.6571\n",
      "Epoch 59/500\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 0.5186 - accuracy: 0.7297 - val_loss: 0.6081 - val_accuracy: 0.6952\n",
      "Epoch 60/500\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.5224 - accuracy: 0.7344 - val_loss: 0.6035 - val_accuracy: 0.6762\n",
      "Epoch 61/500\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 0.5115 - accuracy: 0.7368 - val_loss: 0.6027 - val_accuracy: 0.6762\n",
      "Epoch 62/500\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 0.5180 - accuracy: 0.7321 - val_loss: 0.6057 - val_accuracy: 0.6762\n",
      "Epoch 63/500\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 0.5153 - accuracy: 0.7368 - val_loss: 0.6042 - val_accuracy: 0.6762\n",
      "Epoch 64/500\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 0.5173 - accuracy: 0.7344 - val_loss: 0.6060 - val_accuracy: 0.6762\n",
      "Epoch 65/500\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 0.5127 - accuracy: 0.7416 - val_loss: 0.6045 - val_accuracy: 0.6667\n",
      "Epoch 66/500\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 0.5115 - accuracy: 0.7392 - val_loss: 0.6061 - val_accuracy: 0.6762\n",
      "Epoch 67/500\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 0.5127 - accuracy: 0.7464 - val_loss: 0.6045 - val_accuracy: 0.6476\n",
      "Epoch 68/500\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 0.5255 - accuracy: 0.7632 - val_loss: 0.6050 - val_accuracy: 0.6762\n",
      "Epoch 69/500\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 0.5103 - accuracy: 0.7392 - val_loss: 0.6135 - val_accuracy: 0.6857\n",
      "Epoch 70/500\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 0.5121 - accuracy: 0.7368 - val_loss: 0.6102 - val_accuracy: 0.6381\n",
      "Epoch 71/500\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 0.5150 - accuracy: 0.7440 - val_loss: 0.6118 - val_accuracy: 0.6762\n",
      "Epoch 72/500\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 0.5087 - accuracy: 0.7249 - val_loss: 0.6182 - val_accuracy: 0.6857\n",
      "Epoch 73/500\n",
      "14/14 [==============================] - 0s 31ms/step - loss: 0.5081 - accuracy: 0.7344 - val_loss: 0.6137 - val_accuracy: 0.6762\n",
      "Epoch 74/500\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 0.5062 - accuracy: 0.7656 - val_loss: 0.6112 - val_accuracy: 0.6762\n",
      "Epoch 75/500\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 0.5014 - accuracy: 0.7440 - val_loss: 0.6153 - val_accuracy: 0.6762\n",
      "Epoch 76/500\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 0.4998 - accuracy: 0.7392 - val_loss: 0.6154 - val_accuracy: 0.6762\n",
      "Epoch 77/500\n",
      "14/14 [==============================] - 0s 30ms/step - loss: 0.5034 - accuracy: 0.7632 - val_loss: 0.6129 - val_accuracy: 0.6667\n",
      "Epoch 78/500\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.5085 - accuracy: 0.7584 - val_loss: 0.6141 - val_accuracy: 0.6667\n",
      "Epoch 79/500\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 0.4997 - accuracy: 0.7584 - val_loss: 0.6248 - val_accuracy: 0.6857\n",
      "Epoch 80/500\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 0.5023 - accuracy: 0.7584 - val_loss: 0.6179 - val_accuracy: 0.6571\n",
      "Epoch 81/500\n",
      "14/14 [==============================] - 0s 20ms/step - loss: 0.5022 - accuracy: 0.7512 - val_loss: 0.6211 - val_accuracy: 0.6571\n",
      "Epoch 82/500\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 0.4929 - accuracy: 0.7584 - val_loss: 0.6217 - val_accuracy: 0.6381\n",
      "Epoch 83/500\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 0.4985 - accuracy: 0.7536 - val_loss: 0.6197 - val_accuracy: 0.6381\n",
      "Epoch 84/500\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 0.4879 - accuracy: 0.7656 - val_loss: 0.6225 - val_accuracy: 0.6476\n",
      "Epoch 85/500\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 0.4822 - accuracy: 0.7584 - val_loss: 0.6302 - val_accuracy: 0.6571\n",
      "Epoch 86/500\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 0.4904 - accuracy: 0.7560 - val_loss: 0.6262 - val_accuracy: 0.6571\n",
      "Epoch 87/500\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 0.4838 - accuracy: 0.7464 - val_loss: 0.6237 - val_accuracy: 0.6571\n",
      "Epoch 88/500\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 0.4859 - accuracy: 0.7608 - val_loss: 0.6237 - val_accuracy: 0.6476\n",
      "Epoch 89/500\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 0.4912 - accuracy: 0.7464 - val_loss: 0.6269 - val_accuracy: 0.6381\n",
      "Epoch 90/500\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 0.4797 - accuracy: 0.7775 - val_loss: 0.6324 - val_accuracy: 0.6381\n",
      "Epoch 91/500\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 0.4872 - accuracy: 0.7823 - val_loss: 0.6350 - val_accuracy: 0.6381\n",
      "Epoch 92/500\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.4802 - accuracy: 0.7751 - val_loss: 0.6375 - val_accuracy: 0.6381\n",
      "Epoch 93/500\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 0.4836 - accuracy: 0.7751 - val_loss: 0.6333 - val_accuracy: 0.6381\n",
      "Epoch 94/500\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 0.4868 - accuracy: 0.7560 - val_loss: 0.6256 - val_accuracy: 0.6476\n",
      "Epoch 95/500\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 0.4864 - accuracy: 0.7703 - val_loss: 0.6316 - val_accuracy: 0.6571\n",
      "Epoch 96/500\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.4844 - accuracy: 0.7608 - val_loss: 0.6260 - val_accuracy: 0.6476\n",
      "Epoch 97/500\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 0.4750 - accuracy: 0.7656 - val_loss: 0.6309 - val_accuracy: 0.6286\n",
      "Epoch 98/500\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 0.4797 - accuracy: 0.7727 - val_loss: 0.6359 - val_accuracy: 0.6476\n",
      "Epoch 99/500\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 0.4765 - accuracy: 0.7679 - val_loss: 0.6402 - val_accuracy: 0.6476\n",
      "Epoch 100/500\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 0.4790 - accuracy: 0.7727 - val_loss: 0.6377 - val_accuracy: 0.6476\n",
      "Epoch 101/500\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.4705 - accuracy: 0.7775 - val_loss: 0.6345 - val_accuracy: 0.6476\n",
      "Epoch 102/500\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 0.4610 - accuracy: 0.7799 - val_loss: 0.6370 - val_accuracy: 0.6476\n",
      "Epoch 103/500\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 0.4723 - accuracy: 0.7775 - val_loss: 0.6373 - val_accuracy: 0.6476\n",
      "Epoch 104/500\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 0.4734 - accuracy: 0.7584 - val_loss: 0.6394 - val_accuracy: 0.6571\n",
      "Epoch 105/500\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 0.4651 - accuracy: 0.7656 - val_loss: 0.6340 - val_accuracy: 0.6476\n",
      "Epoch 106/500\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.4687 - accuracy: 0.7727 - val_loss: 0.6352 - val_accuracy: 0.6476\n",
      "Epoch 107/500\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 0.4725 - accuracy: 0.7560 - val_loss: 0.6448 - val_accuracy: 0.6571\n",
      "Epoch 108/500\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 0.4620 - accuracy: 0.7871 - val_loss: 0.6318 - val_accuracy: 0.6286\n",
      "Epoch 109/500\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 0.4685 - accuracy: 0.7775 - val_loss: 0.6397 - val_accuracy: 0.6476\n",
      "Epoch 110/500\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 0.4741 - accuracy: 0.7584 - val_loss: 0.6473 - val_accuracy: 0.6476\n",
      "Epoch 111/500\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 0.4661 - accuracy: 0.7775 - val_loss: 0.6443 - val_accuracy: 0.6286\n",
      "Epoch 112/500\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 0.4592 - accuracy: 0.7823 - val_loss: 0.6463 - val_accuracy: 0.6286\n",
      "Epoch 113/500\n",
      "14/14 [==============================] - 0s 20ms/step - loss: 0.4712 - accuracy: 0.7823 - val_loss: 0.6542 - val_accuracy: 0.6286\n",
      "Epoch 114/500\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 0.4664 - accuracy: 0.7703 - val_loss: 0.6583 - val_accuracy: 0.6381\n",
      "Epoch 115/500\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 0.4595 - accuracy: 0.7679 - val_loss: 0.6501 - val_accuracy: 0.6095\n",
      "Epoch 116/500\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 0.4720 - accuracy: 0.7608 - val_loss: 0.6587 - val_accuracy: 0.6476\n",
      "Epoch 117/500\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 0.4586 - accuracy: 0.7895 - val_loss: 0.6518 - val_accuracy: 0.6000\n",
      "Epoch 118/500\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 0.4547 - accuracy: 0.7847 - val_loss: 0.6587 - val_accuracy: 0.6381\n",
      "Epoch 119/500\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 0.4517 - accuracy: 0.7799 - val_loss: 0.6542 - val_accuracy: 0.6381\n",
      "Epoch 120/500\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 0.4539 - accuracy: 0.7847 - val_loss: 0.6572 - val_accuracy: 0.6476\n",
      "Epoch 121/500\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 0.4556 - accuracy: 0.7871 - val_loss: 0.6548 - val_accuracy: 0.6190\n",
      "Epoch 122/500\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 0.4479 - accuracy: 0.7919 - val_loss: 0.6645 - val_accuracy: 0.6476\n",
      "Epoch 123/500\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 0.4437 - accuracy: 0.7943 - val_loss: 0.6607 - val_accuracy: 0.6095\n",
      "Epoch 124/500\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 0.4439 - accuracy: 0.7847 - val_loss: 0.6630 - val_accuracy: 0.6000\n",
      "Epoch 125/500\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.4577 - accuracy: 0.7703 - val_loss: 0.6690 - val_accuracy: 0.6476\n",
      "Epoch 126/500\n",
      "14/14 [==============================] - 0s 30ms/step - loss: 0.4485 - accuracy: 0.7919 - val_loss: 0.6609 - val_accuracy: 0.6000\n",
      "Epoch 127/500\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.4410 - accuracy: 0.8014 - val_loss: 0.6759 - val_accuracy: 0.6476\n",
      "Epoch 128/500\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.4481 - accuracy: 0.7799 - val_loss: 0.6661 - val_accuracy: 0.6000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 1/500\n",
      "14/14 [==============================] - 1s 47ms/step - loss: 0.6305 - accuracy: 0.7041 - val_loss: 0.6073 - val_accuracy: 0.7115\n",
      "Epoch 2/500\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.6053 - accuracy: 0.7041 - val_loss: 0.5936 - val_accuracy: 0.7115\n",
      "Epoch 3/500\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 0.6005 - accuracy: 0.7041 - val_loss: 0.5908 - val_accuracy: 0.7115\n",
      "Epoch 4/500\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 0.5867 - accuracy: 0.7041 - val_loss: 0.5894 - val_accuracy: 0.7115\n",
      "Epoch 5/500\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 0.5892 - accuracy: 0.7041 - val_loss: 0.5882 - val_accuracy: 0.7115\n",
      "Epoch 6/500\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 0.5845 - accuracy: 0.7041 - val_loss: 0.5891 - val_accuracy: 0.7115\n",
      "Epoch 7/500\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 0.5861 - accuracy: 0.7041 - val_loss: 0.5883 - val_accuracy: 0.7115\n",
      "Epoch 8/500\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 0.5866 - accuracy: 0.7041 - val_loss: 0.5870 - val_accuracy: 0.7115\n",
      "Epoch 9/500\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.5810 - accuracy: 0.7041 - val_loss: 0.5863 - val_accuracy: 0.7115\n",
      "Epoch 10/500\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 0.5759 - accuracy: 0.7041 - val_loss: 0.5850 - val_accuracy: 0.7115\n",
      "Epoch 11/500\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 0.5786 - accuracy: 0.7041 - val_loss: 0.5861 - val_accuracy: 0.7115\n",
      "Epoch 12/500\n",
      "14/14 [==============================] - 0s 35ms/step - loss: 0.5792 - accuracy: 0.7041 - val_loss: 0.5847 - val_accuracy: 0.7115\n",
      "Epoch 13/500\n",
      "14/14 [==============================] - 1s 42ms/step - loss: 0.5784 - accuracy: 0.7041 - val_loss: 0.5833 - val_accuracy: 0.7115\n",
      "Epoch 14/500\n",
      "14/14 [==============================] - 1s 38ms/step - loss: 0.5690 - accuracy: 0.7088 - val_loss: 0.5841 - val_accuracy: 0.7115\n",
      "Epoch 15/500\n",
      "14/14 [==============================] - 0s 32ms/step - loss: 0.5726 - accuracy: 0.7064 - val_loss: 0.5810 - val_accuracy: 0.7212\n",
      "Epoch 16/500\n",
      "14/14 [==============================] - 0s 29ms/step - loss: 0.5663 - accuracy: 0.7112 - val_loss: 0.5817 - val_accuracy: 0.7212\n",
      "Epoch 17/500\n",
      "14/14 [==============================] - 0s 30ms/step - loss: 0.5657 - accuracy: 0.7112 - val_loss: 0.5795 - val_accuracy: 0.7212\n",
      "Epoch 18/500\n",
      "14/14 [==============================] - 0s 35ms/step - loss: 0.5633 - accuracy: 0.7064 - val_loss: 0.5785 - val_accuracy: 0.7212\n",
      "Epoch 19/500\n",
      "14/14 [==============================] - 0s 33ms/step - loss: 0.5643 - accuracy: 0.7160 - val_loss: 0.5777 - val_accuracy: 0.7212\n",
      "Epoch 20/500\n",
      "14/14 [==============================] - 1s 40ms/step - loss: 0.5637 - accuracy: 0.7136 - val_loss: 0.5755 - val_accuracy: 0.7212\n",
      "Epoch 21/500\n",
      "14/14 [==============================] - 0s 33ms/step - loss: 0.5583 - accuracy: 0.7112 - val_loss: 0.5752 - val_accuracy: 0.7212\n",
      "Epoch 22/500\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 0.5589 - accuracy: 0.7136 - val_loss: 0.5755 - val_accuracy: 0.7212\n",
      "Epoch 23/500\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 0.5648 - accuracy: 0.7279 - val_loss: 0.5831 - val_accuracy: 0.7019\n",
      "Epoch 24/500\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 0.5556 - accuracy: 0.7208 - val_loss: 0.5752 - val_accuracy: 0.7212\n",
      "Epoch 25/500\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.5526 - accuracy: 0.7208 - val_loss: 0.5775 - val_accuracy: 0.7115\n",
      "Epoch 26/500\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 0.5570 - accuracy: 0.7255 - val_loss: 0.5737 - val_accuracy: 0.7212\n",
      "Epoch 27/500\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 0.5499 - accuracy: 0.7184 - val_loss: 0.5708 - val_accuracy: 0.7212\n",
      "Epoch 28/500\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 0.5520 - accuracy: 0.7279 - val_loss: 0.5717 - val_accuracy: 0.7115\n",
      "Epoch 29/500\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 0.5474 - accuracy: 0.7232 - val_loss: 0.5712 - val_accuracy: 0.7115\n",
      "Epoch 30/500\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 0.5517 - accuracy: 0.7327 - val_loss: 0.5741 - val_accuracy: 0.7115\n",
      "Epoch 31/500\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 0.5502 - accuracy: 0.7136 - val_loss: 0.5719 - val_accuracy: 0.7212\n",
      "Epoch 32/500\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 0.5477 - accuracy: 0.7375 - val_loss: 0.5770 - val_accuracy: 0.7212\n",
      "Epoch 33/500\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 0.5472 - accuracy: 0.7351 - val_loss: 0.5723 - val_accuracy: 0.7019\n",
      "Epoch 34/500\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 0.5469 - accuracy: 0.7279 - val_loss: 0.5702 - val_accuracy: 0.7115\n",
      "Epoch 35/500\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 0.5399 - accuracy: 0.7232 - val_loss: 0.5718 - val_accuracy: 0.7115\n",
      "Epoch 36/500\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 0.5425 - accuracy: 0.7255 - val_loss: 0.5715 - val_accuracy: 0.7019\n",
      "Epoch 37/500\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.5418 - accuracy: 0.7327 - val_loss: 0.5710 - val_accuracy: 0.7019\n",
      "Epoch 38/500\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 0.5368 - accuracy: 0.7303 - val_loss: 0.5706 - val_accuracy: 0.7019\n",
      "Epoch 39/500\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 0.5341 - accuracy: 0.7279 - val_loss: 0.5709 - val_accuracy: 0.7019\n",
      "Epoch 40/500\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 0.5389 - accuracy: 0.7279 - val_loss: 0.5718 - val_accuracy: 0.7019\n",
      "Epoch 41/500\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 0.5325 - accuracy: 0.7303 - val_loss: 0.5696 - val_accuracy: 0.7019\n",
      "Epoch 42/500\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 0.5336 - accuracy: 0.7399 - val_loss: 0.5698 - val_accuracy: 0.7019\n",
      "Epoch 43/500\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 0.5359 - accuracy: 0.7279 - val_loss: 0.5695 - val_accuracy: 0.7019\n",
      "Epoch 44/500\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 0.5328 - accuracy: 0.7518 - val_loss: 0.5675 - val_accuracy: 0.7212\n",
      "Epoch 45/500\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 0.5300 - accuracy: 0.7422 - val_loss: 0.5677 - val_accuracy: 0.7115\n",
      "Epoch 46/500\n",
      "14/14 [==============================] - 0s 20ms/step - loss: 0.5262 - accuracy: 0.7446 - val_loss: 0.5686 - val_accuracy: 0.7212\n",
      "Epoch 47/500\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 0.5297 - accuracy: 0.7613 - val_loss: 0.5687 - val_accuracy: 0.7115\n",
      "Epoch 48/500\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 0.5242 - accuracy: 0.7399 - val_loss: 0.5671 - val_accuracy: 0.7019\n",
      "Epoch 49/500\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 0.5226 - accuracy: 0.7470 - val_loss: 0.5654 - val_accuracy: 0.7212\n",
      "Epoch 50/500\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 0.5156 - accuracy: 0.7589 - val_loss: 0.5661 - val_accuracy: 0.6923\n",
      "Epoch 51/500\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 0.5264 - accuracy: 0.7446 - val_loss: 0.5652 - val_accuracy: 0.7212\n",
      "Epoch 52/500\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 0.5180 - accuracy: 0.7637 - val_loss: 0.5627 - val_accuracy: 0.7115\n",
      "Epoch 53/500\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 0.5245 - accuracy: 0.7518 - val_loss: 0.5654 - val_accuracy: 0.6923\n",
      "Epoch 54/500\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 0.5234 - accuracy: 0.7542 - val_loss: 0.5679 - val_accuracy: 0.7212\n",
      "Epoch 55/500\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 0.5157 - accuracy: 0.7733 - val_loss: 0.5657 - val_accuracy: 0.7019\n",
      "Epoch 56/500\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 0.5143 - accuracy: 0.7613 - val_loss: 0.5665 - val_accuracy: 0.7115\n",
      "Epoch 57/500\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 0.5185 - accuracy: 0.7589 - val_loss: 0.5657 - val_accuracy: 0.7115\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 58/500\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 0.5109 - accuracy: 0.7637 - val_loss: 0.5643 - val_accuracy: 0.6827\n",
      "Epoch 59/500\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 0.5096 - accuracy: 0.7685 - val_loss: 0.5624 - val_accuracy: 0.6923\n",
      "Epoch 60/500\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 0.5131 - accuracy: 0.7661 - val_loss: 0.5638 - val_accuracy: 0.7115\n",
      "Epoch 61/500\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 0.5094 - accuracy: 0.7828 - val_loss: 0.5658 - val_accuracy: 0.6731\n",
      "Epoch 62/500\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 0.5099 - accuracy: 0.7709 - val_loss: 0.5668 - val_accuracy: 0.7019\n",
      "Epoch 63/500\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 0.5119 - accuracy: 0.7566 - val_loss: 0.5647 - val_accuracy: 0.7115\n",
      "Epoch 64/500\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 0.5032 - accuracy: 0.7733 - val_loss: 0.5659 - val_accuracy: 0.6923\n",
      "Epoch 65/500\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 0.5063 - accuracy: 0.7780 - val_loss: 0.5642 - val_accuracy: 0.6635\n",
      "Epoch 66/500\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 0.5062 - accuracy: 0.7757 - val_loss: 0.5664 - val_accuracy: 0.6923\n",
      "Epoch 67/500\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 0.5038 - accuracy: 0.7637 - val_loss: 0.5669 - val_accuracy: 0.6923\n",
      "Epoch 68/500\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 0.5007 - accuracy: 0.7876 - val_loss: 0.5659 - val_accuracy: 0.6923\n",
      "Epoch 69/500\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 0.5055 - accuracy: 0.7780 - val_loss: 0.5695 - val_accuracy: 0.6923\n",
      "Epoch 70/500\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 0.5016 - accuracy: 0.7804 - val_loss: 0.5672 - val_accuracy: 0.6923\n",
      "Epoch 71/500\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 0.5010 - accuracy: 0.7494 - val_loss: 0.5665 - val_accuracy: 0.6923\n",
      "Epoch 72/500\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 0.4978 - accuracy: 0.7828 - val_loss: 0.5677 - val_accuracy: 0.6923\n",
      "Epoch 73/500\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 0.4943 - accuracy: 0.7780 - val_loss: 0.5668 - val_accuracy: 0.6923\n",
      "Epoch 74/500\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 0.4973 - accuracy: 0.7589 - val_loss: 0.5676 - val_accuracy: 0.6923\n",
      "Epoch 75/500\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 0.4919 - accuracy: 0.7661 - val_loss: 0.5690 - val_accuracy: 0.6731\n",
      "Epoch 76/500\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 0.4890 - accuracy: 0.7924 - val_loss: 0.5668 - val_accuracy: 0.6731\n",
      "Epoch 77/500\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 0.4942 - accuracy: 0.7661 - val_loss: 0.5685 - val_accuracy: 0.6827\n",
      "Epoch 78/500\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 0.4875 - accuracy: 0.7757 - val_loss: 0.5673 - val_accuracy: 0.6635\n",
      "Epoch 79/500\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 0.4905 - accuracy: 0.8019 - val_loss: 0.5666 - val_accuracy: 0.6635\n",
      "Epoch 80/500\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.4913 - accuracy: 0.7733 - val_loss: 0.5698 - val_accuracy: 0.6827\n",
      "Epoch 81/500\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 0.4948 - accuracy: 0.7733 - val_loss: 0.5654 - val_accuracy: 0.6635\n",
      "Epoch 82/500\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 0.4920 - accuracy: 0.7947 - val_loss: 0.5661 - val_accuracy: 0.6731\n",
      "Epoch 83/500\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 0.4908 - accuracy: 0.7757 - val_loss: 0.5684 - val_accuracy: 0.6635\n",
      "Epoch 84/500\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 0.4859 - accuracy: 0.7852 - val_loss: 0.5684 - val_accuracy: 0.6538\n",
      "Epoch 85/500\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 0.4887 - accuracy: 0.7852 - val_loss: 0.5670 - val_accuracy: 0.6635\n",
      "Epoch 86/500\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.4870 - accuracy: 0.7828 - val_loss: 0.5685 - val_accuracy: 0.6731\n",
      "Epoch 87/500\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 0.4792 - accuracy: 0.7828 - val_loss: 0.5660 - val_accuracy: 0.6731\n",
      "Epoch 88/500\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 0.4840 - accuracy: 0.7804 - val_loss: 0.5664 - val_accuracy: 0.6635\n",
      "Epoch 89/500\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 0.4839 - accuracy: 0.7876 - val_loss: 0.5673 - val_accuracy: 0.6635\n",
      "Epoch 90/500\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 0.4786 - accuracy: 0.8019 - val_loss: 0.5674 - val_accuracy: 0.6635\n",
      "Epoch 91/500\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 0.4756 - accuracy: 0.7876 - val_loss: 0.5666 - val_accuracy: 0.6731\n",
      "Epoch 92/500\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 0.4788 - accuracy: 0.7828 - val_loss: 0.5667 - val_accuracy: 0.6827\n",
      "Epoch 93/500\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 0.4710 - accuracy: 0.8091 - val_loss: 0.5666 - val_accuracy: 0.6827\n",
      "Epoch 94/500\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 0.4701 - accuracy: 0.7900 - val_loss: 0.5707 - val_accuracy: 0.6731\n",
      "Epoch 95/500\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 0.4784 - accuracy: 0.8043 - val_loss: 0.5712 - val_accuracy: 0.6538\n",
      "Epoch 96/500\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 0.4651 - accuracy: 0.7924 - val_loss: 0.5746 - val_accuracy: 0.6635\n",
      "Epoch 97/500\n",
      "14/14 [==============================] - 0s 20ms/step - loss: 0.4579 - accuracy: 0.8138 - val_loss: 0.5745 - val_accuracy: 0.6731\n",
      "Epoch 98/500\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 0.4609 - accuracy: 0.7876 - val_loss: 0.5751 - val_accuracy: 0.6538\n",
      "Epoch 99/500\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 0.4648 - accuracy: 0.8019 - val_loss: 0.5844 - val_accuracy: 0.6731\n",
      "Epoch 100/500\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 0.4690 - accuracy: 0.7924 - val_loss: 0.5820 - val_accuracy: 0.6635\n",
      "Epoch 101/500\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 0.4608 - accuracy: 0.8162 - val_loss: 0.5825 - val_accuracy: 0.6635\n",
      "Epoch 102/500\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 0.4579 - accuracy: 0.7971 - val_loss: 0.5821 - val_accuracy: 0.6827\n",
      "Epoch 103/500\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 0.4572 - accuracy: 0.8019 - val_loss: 0.5824 - val_accuracy: 0.6635\n",
      "Epoch 104/500\n",
      "14/14 [==============================] - 0s 20ms/step - loss: 0.4622 - accuracy: 0.8067 - val_loss: 0.5784 - val_accuracy: 0.6731\n",
      "Epoch 105/500\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 0.4652 - accuracy: 0.8115 - val_loss: 0.5774 - val_accuracy: 0.6635\n",
      "Epoch 106/500\n",
      "14/14 [==============================] - 0s 20ms/step - loss: 0.4699 - accuracy: 0.7947 - val_loss: 0.5758 - val_accuracy: 0.6538\n",
      "Epoch 107/500\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 0.4534 - accuracy: 0.8138 - val_loss: 0.5736 - val_accuracy: 0.6635\n",
      "Epoch 108/500\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 0.4627 - accuracy: 0.8091 - val_loss: 0.5714 - val_accuracy: 0.6538\n",
      "Epoch 109/500\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 0.4572 - accuracy: 0.7995 - val_loss: 0.5746 - val_accuracy: 0.6442\n",
      "Epoch 110/500\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 0.4479 - accuracy: 0.8186 - val_loss: 0.5749 - val_accuracy: 0.6538\n",
      "Epoch 111/500\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 0.4645 - accuracy: 0.8043 - val_loss: 0.5741 - val_accuracy: 0.6635\n",
      "Epoch 112/500\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 0.4586 - accuracy: 0.8186 - val_loss: 0.5777 - val_accuracy: 0.6731\n",
      "Epoch 113/500\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 0.4525 - accuracy: 0.8234 - val_loss: 0.5774 - val_accuracy: 0.6635\n",
      "Epoch 114/500\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 0.4506 - accuracy: 0.8138 - val_loss: 0.5800 - val_accuracy: 0.6538\n",
      "Epoch 115/500\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 0.4555 - accuracy: 0.8138 - val_loss: 0.5797 - val_accuracy: 0.6635\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "Epoch 1/500\n",
      "14/14 [==============================] - 1s 29ms/step - loss: 0.6152 - accuracy: 0.7041 - val_loss: 0.5953 - val_accuracy: 0.7019\n",
      "Epoch 2/500\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 0.6036 - accuracy: 0.7064 - val_loss: 0.5904 - val_accuracy: 0.7019\n",
      "Epoch 3/500\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 0.5998 - accuracy: 0.7064 - val_loss: 0.5897 - val_accuracy: 0.7019\n",
      "Epoch 4/500\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 0.5901 - accuracy: 0.7064 - val_loss: 0.5883 - val_accuracy: 0.7019\n",
      "Epoch 5/500\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 0.5891 - accuracy: 0.7064 - val_loss: 0.5879 - val_accuracy: 0.7019\n",
      "Epoch 6/500\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 0.5885 - accuracy: 0.7064 - val_loss: 0.5865 - val_accuracy: 0.7019\n",
      "Epoch 7/500\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 0.5903 - accuracy: 0.7064 - val_loss: 0.5871 - val_accuracy: 0.7019\n",
      "Epoch 8/500\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 0.5841 - accuracy: 0.7064 - val_loss: 0.5867 - val_accuracy: 0.7019\n",
      "Epoch 9/500\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 0.5839 - accuracy: 0.7064 - val_loss: 0.5859 - val_accuracy: 0.7019\n",
      "Epoch 10/500\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 0.5834 - accuracy: 0.7064 - val_loss: 0.5851 - val_accuracy: 0.7019\n",
      "Epoch 11/500\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 0.5866 - accuracy: 0.7064 - val_loss: 0.5906 - val_accuracy: 0.7019\n",
      "Epoch 12/500\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 0.5859 - accuracy: 0.7064 - val_loss: 0.5866 - val_accuracy: 0.7019\n",
      "Epoch 13/500\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 0.5799 - accuracy: 0.7064 - val_loss: 0.5856 - val_accuracy: 0.7019\n",
      "Epoch 14/500\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 0.5807 - accuracy: 0.7064 - val_loss: 0.5899 - val_accuracy: 0.7019\n",
      "Epoch 15/500\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 0.5786 - accuracy: 0.7064 - val_loss: 0.5845 - val_accuracy: 0.7019\n",
      "Epoch 16/500\n",
      "14/14 [==============================] - 0s 20ms/step - loss: 0.5746 - accuracy: 0.7088 - val_loss: 0.5851 - val_accuracy: 0.7019\n",
      "Epoch 17/500\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 0.5732 - accuracy: 0.7064 - val_loss: 0.5838 - val_accuracy: 0.7019\n",
      "Epoch 18/500\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.5699 - accuracy: 0.7088 - val_loss: 0.5839 - val_accuracy: 0.7019\n",
      "Epoch 19/500\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.5713 - accuracy: 0.7088 - val_loss: 0.5839 - val_accuracy: 0.7019\n",
      "Epoch 20/500\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.5665 - accuracy: 0.7088 - val_loss: 0.5840 - val_accuracy: 0.7019\n",
      "Epoch 21/500\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 0.5675 - accuracy: 0.7088 - val_loss: 0.5863 - val_accuracy: 0.7019\n",
      "Epoch 22/500\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.5626 - accuracy: 0.7088 - val_loss: 0.5845 - val_accuracy: 0.7019\n",
      "Epoch 23/500\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 0.5655 - accuracy: 0.7112 - val_loss: 0.5853 - val_accuracy: 0.7019\n",
      "Epoch 24/500\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 0.5618 - accuracy: 0.7112 - val_loss: 0.5840 - val_accuracy: 0.7019\n",
      "Epoch 25/500\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 0.5616 - accuracy: 0.7112 - val_loss: 0.5848 - val_accuracy: 0.7019\n",
      "Epoch 26/500\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 0.5614 - accuracy: 0.7112 - val_loss: 0.5853 - val_accuracy: 0.7019\n",
      "Epoch 27/500\n",
      "14/14 [==============================] - 0s 30ms/step - loss: 0.5558 - accuracy: 0.7112 - val_loss: 0.5858 - val_accuracy: 0.7019\n",
      "Epoch 28/500\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 0.5619 - accuracy: 0.7088 - val_loss: 0.5863 - val_accuracy: 0.7019\n",
      "Epoch 29/500\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 0.5607 - accuracy: 0.7160 - val_loss: 0.5876 - val_accuracy: 0.7019\n",
      "Epoch 30/500\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.5523 - accuracy: 0.7112 - val_loss: 0.5879 - val_accuracy: 0.7019\n",
      "Epoch 31/500\n",
      "14/14 [==============================] - 0s 20ms/step - loss: 0.5614 - accuracy: 0.7112 - val_loss: 0.5867 - val_accuracy: 0.7019\n",
      "Epoch 32/500\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 0.5715 - accuracy: 0.7184 - val_loss: 0.5933 - val_accuracy: 0.6923\n",
      "Epoch 33/500\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 0.5536 - accuracy: 0.7136 - val_loss: 0.5872 - val_accuracy: 0.7019\n",
      "Epoch 34/500\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 0.5577 - accuracy: 0.7112 - val_loss: 0.5890 - val_accuracy: 0.7019\n",
      "Epoch 35/500\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 0.5491 - accuracy: 0.7112 - val_loss: 0.5866 - val_accuracy: 0.7019\n",
      "Epoch 36/500\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 0.5516 - accuracy: 0.7112 - val_loss: 0.5870 - val_accuracy: 0.7019\n",
      "Epoch 37/500\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 0.5504 - accuracy: 0.7136 - val_loss: 0.5881 - val_accuracy: 0.7019\n",
      "Epoch 38/500\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 0.5521 - accuracy: 0.7112 - val_loss: 0.5886 - val_accuracy: 0.7019\n",
      "Epoch 39/500\n",
      "14/14 [==============================] - 0s 30ms/step - loss: 0.5494 - accuracy: 0.7136 - val_loss: 0.5879 - val_accuracy: 0.7019\n",
      "Epoch 40/500\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.5508 - accuracy: 0.7136 - val_loss: 0.5889 - val_accuracy: 0.7019\n",
      "Epoch 41/500\n",
      "14/14 [==============================] - 0s 33ms/step - loss: 0.5435 - accuracy: 0.7184 - val_loss: 0.5886 - val_accuracy: 0.6923\n",
      "Epoch 42/500\n",
      "14/14 [==============================] - 1s 41ms/step - loss: 0.5449 - accuracy: 0.7184 - val_loss: 0.5886 - val_accuracy: 0.6923\n",
      "Epoch 43/500\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.5415 - accuracy: 0.7160 - val_loss: 0.5901 - val_accuracy: 0.6923\n",
      "Epoch 44/500\n",
      "14/14 [==============================] - 0s 29ms/step - loss: 0.5477 - accuracy: 0.7160 - val_loss: 0.5908 - val_accuracy: 0.7019\n",
      "Epoch 45/500\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 0.5387 - accuracy: 0.7208 - val_loss: 0.5899 - val_accuracy: 0.6923\n",
      "Epoch 46/500\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 0.5380 - accuracy: 0.7327 - val_loss: 0.5909 - val_accuracy: 0.6923\n",
      "Epoch 47/500\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 0.5415 - accuracy: 0.7327 - val_loss: 0.5915 - val_accuracy: 0.6923\n",
      "Epoch 48/500\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 0.5376 - accuracy: 0.7232 - val_loss: 0.5911 - val_accuracy: 0.6923\n",
      "Epoch 49/500\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 0.5343 - accuracy: 0.7303 - val_loss: 0.5915 - val_accuracy: 0.6923\n",
      "Epoch 50/500\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.5344 - accuracy: 0.7303 - val_loss: 0.5921 - val_accuracy: 0.6923\n",
      "Epoch 51/500\n",
      "14/14 [==============================] - 0s 20ms/step - loss: 0.5323 - accuracy: 0.7208 - val_loss: 0.5936 - val_accuracy: 0.6923\n",
      "Epoch 52/500\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 0.5386 - accuracy: 0.7184 - val_loss: 0.5936 - val_accuracy: 0.7019\n",
      "Epoch 53/500\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 0.5314 - accuracy: 0.7184 - val_loss: 0.5939 - val_accuracy: 0.6923\n",
      "Epoch 54/500\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 0.5347 - accuracy: 0.7255 - val_loss: 0.5924 - val_accuracy: 0.6923\n",
      "Epoch 55/500\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 0.5266 - accuracy: 0.7422 - val_loss: 0.5920 - val_accuracy: 0.6827\n",
      "Epoch 56/500\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 0.5290 - accuracy: 0.7303 - val_loss: 0.5930 - val_accuracy: 0.6923\n",
      "Epoch 57/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14/14 [==============================] - 0s 23ms/step - loss: 0.5276 - accuracy: 0.7279 - val_loss: 0.5930 - val_accuracy: 0.6923\n",
      "Epoch 58/500\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 0.5228 - accuracy: 0.7279 - val_loss: 0.5912 - val_accuracy: 0.6731\n",
      "Epoch 59/500\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 0.5289 - accuracy: 0.7351 - val_loss: 0.5932 - val_accuracy: 0.7019\n",
      "Epoch 60/500\n",
      "14/14 [==============================] - 0s 30ms/step - loss: 0.5258 - accuracy: 0.7399 - val_loss: 0.5934 - val_accuracy: 0.7019\n",
      "Epoch 61/500\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 0.5236 - accuracy: 0.7422 - val_loss: 0.5941 - val_accuracy: 0.7019\n",
      "Epoch 62/500\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 0.5213 - accuracy: 0.7351 - val_loss: 0.5950 - val_accuracy: 0.7019\n",
      "Epoch 63/500\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.5254 - accuracy: 0.7327 - val_loss: 0.5941 - val_accuracy: 0.7019\n",
      "Epoch 64/500\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 0.5222 - accuracy: 0.7470 - val_loss: 0.5948 - val_accuracy: 0.6923\n",
      "Epoch 65/500\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 0.5260 - accuracy: 0.7351 - val_loss: 0.5957 - val_accuracy: 0.7115\n",
      "Epoch 66/500\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 0.5260 - accuracy: 0.7566 - val_loss: 0.5961 - val_accuracy: 0.6827\n",
      "Epoch 67/500\n",
      "14/14 [==============================] - 0s 20ms/step - loss: 0.5185 - accuracy: 0.7518 - val_loss: 0.5976 - val_accuracy: 0.7019\n",
      "Epoch 68/500\n",
      "14/14 [==============================] - 0s 20ms/step - loss: 0.5137 - accuracy: 0.7542 - val_loss: 0.5970 - val_accuracy: 0.6923\n",
      "Epoch 69/500\n",
      "14/14 [==============================] - 0s 35ms/step - loss: 0.5217 - accuracy: 0.7470 - val_loss: 0.5985 - val_accuracy: 0.6923\n",
      "Epoch 70/500\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 0.5174 - accuracy: 0.7446 - val_loss: 0.5979 - val_accuracy: 0.6827\n",
      "Epoch 71/500\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 0.5173 - accuracy: 0.7518 - val_loss: 0.5975 - val_accuracy: 0.6827\n",
      "Epoch 72/500\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 0.5152 - accuracy: 0.7566 - val_loss: 0.5988 - val_accuracy: 0.6923\n",
      "Epoch 73/500\n",
      "14/14 [==============================] - 0s 32ms/step - loss: 0.5163 - accuracy: 0.7399 - val_loss: 0.5999 - val_accuracy: 0.6923\n",
      "Epoch 74/500\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 0.5137 - accuracy: 0.7709 - val_loss: 0.5991 - val_accuracy: 0.6827\n",
      "Epoch 75/500\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 0.5109 - accuracy: 0.7542 - val_loss: 0.6026 - val_accuracy: 0.7019\n",
      "Epoch 76/500\n",
      "14/14 [==============================] - 0s 33ms/step - loss: 0.5061 - accuracy: 0.7518 - val_loss: 0.6006 - val_accuracy: 0.6731\n",
      "Epoch 77/500\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 0.5092 - accuracy: 0.7542 - val_loss: 0.6031 - val_accuracy: 0.7019\n",
      "Epoch 78/500\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 0.5076 - accuracy: 0.7709 - val_loss: 0.6005 - val_accuracy: 0.6827\n",
      "Epoch 79/500\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 0.5062 - accuracy: 0.7661 - val_loss: 0.6006 - val_accuracy: 0.6827\n",
      "Epoch 80/500\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 0.5075 - accuracy: 0.7613 - val_loss: 0.6005 - val_accuracy: 0.6731\n",
      "Epoch 81/500\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 0.5087 - accuracy: 0.7637 - val_loss: 0.6010 - val_accuracy: 0.6731\n",
      "Epoch 82/500\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 0.5081 - accuracy: 0.7709 - val_loss: 0.5995 - val_accuracy: 0.6635\n",
      "Epoch 83/500\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 0.5066 - accuracy: 0.7542 - val_loss: 0.5990 - val_accuracy: 0.7019\n",
      "Epoch 84/500\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 0.5020 - accuracy: 0.7494 - val_loss: 0.5984 - val_accuracy: 0.6635\n",
      "Epoch 85/500\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 0.5019 - accuracy: 0.7780 - val_loss: 0.6007 - val_accuracy: 0.6731\n",
      "Epoch 86/500\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 0.4989 - accuracy: 0.7661 - val_loss: 0.6003 - val_accuracy: 0.6923\n",
      "Epoch 87/500\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 0.5059 - accuracy: 0.7613 - val_loss: 0.6000 - val_accuracy: 0.6827\n",
      "Epoch 88/500\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 0.5017 - accuracy: 0.7351 - val_loss: 0.6015 - val_accuracy: 0.7019\n",
      "Epoch 89/500\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 0.5067 - accuracy: 0.7804 - val_loss: 0.6029 - val_accuracy: 0.6827\n",
      "Epoch 90/500\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 0.4958 - accuracy: 0.7685 - val_loss: 0.6019 - val_accuracy: 0.7019\n",
      "Epoch 91/500\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 0.5012 - accuracy: 0.7685 - val_loss: 0.5990 - val_accuracy: 0.6731\n",
      "Epoch 92/500\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 0.4985 - accuracy: 0.7780 - val_loss: 0.6016 - val_accuracy: 0.6731\n",
      "Epoch 93/500\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 0.4919 - accuracy: 0.7566 - val_loss: 0.6014 - val_accuracy: 0.6635\n",
      "Epoch 94/500\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 0.4893 - accuracy: 0.7709 - val_loss: 0.6007 - val_accuracy: 0.6635\n",
      "Epoch 95/500\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 0.4995 - accuracy: 0.7733 - val_loss: 0.6024 - val_accuracy: 0.6827\n",
      "Epoch 96/500\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 0.4958 - accuracy: 0.7589 - val_loss: 0.6014 - val_accuracy: 0.6635\n",
      "Epoch 97/500\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 0.4942 - accuracy: 0.7661 - val_loss: 0.6028 - val_accuracy: 0.6538\n",
      "Epoch 98/500\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 0.4873 - accuracy: 0.7876 - val_loss: 0.6044 - val_accuracy: 0.6635\n",
      "Epoch 99/500\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 0.5034 - accuracy: 0.7733 - val_loss: 0.6060 - val_accuracy: 0.6635\n",
      "Epoch 100/500\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 0.4930 - accuracy: 0.7613 - val_loss: 0.6098 - val_accuracy: 0.7019\n",
      "Epoch 101/500\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 0.4914 - accuracy: 0.7566 - val_loss: 0.6008 - val_accuracy: 0.6635\n",
      "Epoch 102/500\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 0.4888 - accuracy: 0.7900 - val_loss: 0.6002 - val_accuracy: 0.6538\n",
      "Epoch 103/500\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 0.4788 - accuracy: 0.7780 - val_loss: 0.5982 - val_accuracy: 0.6635\n",
      "Epoch 104/500\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 0.4887 - accuracy: 0.7709 - val_loss: 0.6037 - val_accuracy: 0.6538\n",
      "Epoch 105/500\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 0.4859 - accuracy: 0.7947 - val_loss: 0.6052 - val_accuracy: 0.6731\n",
      "Epoch 106/500\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 0.4824 - accuracy: 0.7852 - val_loss: 0.6041 - val_accuracy: 0.6442\n",
      "Epoch 107/500\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 0.4729 - accuracy: 0.7971 - val_loss: 0.6039 - val_accuracy: 0.6442\n",
      "Epoch 108/500\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.4834 - accuracy: 0.7828 - val_loss: 0.6035 - val_accuracy: 0.6635\n",
      "Epoch 109/500\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.4772 - accuracy: 0.7852 - val_loss: 0.6060 - val_accuracy: 0.6731\n",
      "Epoch 110/500\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.4888 - accuracy: 0.7709 - val_loss: 0.6058 - val_accuracy: 0.6731\n",
      "Epoch 111/500\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 0.4739 - accuracy: 0.7995 - val_loss: 0.6034 - val_accuracy: 0.6442\n",
      "Epoch 112/500\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.4749 - accuracy: 0.8019 - val_loss: 0.6065 - val_accuracy: 0.6731\n",
      "Epoch 113/500\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.4804 - accuracy: 0.7947 - val_loss: 0.6052 - val_accuracy: 0.6731\n",
      "Epoch 114/500\n",
      "14/14 [==============================] - 0s 31ms/step - loss: 0.4829 - accuracy: 0.7947 - val_loss: 0.6018 - val_accuracy: 0.6538\n",
      "Epoch 115/500\n",
      "14/14 [==============================] - 0s 29ms/step - loss: 0.4696 - accuracy: 0.7900 - val_loss: 0.6042 - val_accuracy: 0.6635\n",
      "Epoch 116/500\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.4751 - accuracy: 0.7852 - val_loss: 0.6042 - val_accuracy: 0.6538\n",
      "Epoch 117/500\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 0.4708 - accuracy: 0.7971 - val_loss: 0.6050 - val_accuracy: 0.6442\n",
      "Epoch 118/500\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 0.4749 - accuracy: 0.7971 - val_loss: 0.6030 - val_accuracy: 0.6442\n",
      "Epoch 119/500\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.4677 - accuracy: 0.7947 - val_loss: 0.6030 - val_accuracy: 0.6635\n",
      "Epoch 120/500\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 0.4742 - accuracy: 0.7995 - val_loss: 0.6030 - val_accuracy: 0.6635\n",
      "Epoch 121/500\n",
      "14/14 [==============================] - 0s 30ms/step - loss: 0.4659 - accuracy: 0.7947 - val_loss: 0.6032 - val_accuracy: 0.6442\n",
      "Epoch 122/500\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.4677 - accuracy: 0.7995 - val_loss: 0.6026 - val_accuracy: 0.6635\n",
      "Epoch 123/500\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 0.4655 - accuracy: 0.8043 - val_loss: 0.6025 - val_accuracy: 0.6635\n",
      "Epoch 124/500\n",
      "14/14 [==============================] - 0s 31ms/step - loss: 0.4646 - accuracy: 0.7876 - val_loss: 0.6049 - val_accuracy: 0.6731\n",
      "Epoch 125/500\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 0.4691 - accuracy: 0.7924 - val_loss: 0.6031 - val_accuracy: 0.6635\n",
      "Epoch 126/500\n",
      "14/14 [==============================] - 0s 19ms/step - loss: 0.4608 - accuracy: 0.7947 - val_loss: 0.6048 - val_accuracy: 0.6538\n",
      "Epoch 127/500\n",
      "14/14 [==============================] - 0s 20ms/step - loss: 0.4607 - accuracy: 0.8091 - val_loss: 0.6053 - val_accuracy: 0.6538\n",
      "Epoch 128/500\n",
      "14/14 [==============================] - 0s 20ms/step - loss: 0.4655 - accuracy: 0.7924 - val_loss: 0.6069 - val_accuracy: 0.6538\n",
      "Epoch 129/500\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 0.4560 - accuracy: 0.8115 - val_loss: 0.6044 - val_accuracy: 0.6538\n",
      "Epoch 130/500\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 0.4623 - accuracy: 0.7971 - val_loss: 0.6025 - val_accuracy: 0.6538\n",
      "Epoch 131/500\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 0.4669 - accuracy: 0.7995 - val_loss: 0.6005 - val_accuracy: 0.6635\n",
      "Epoch 132/500\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 0.4644 - accuracy: 0.7900 - val_loss: 0.5963 - val_accuracy: 0.6538\n",
      "Epoch 133/500\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 0.4607 - accuracy: 0.8067 - val_loss: 0.5988 - val_accuracy: 0.6538\n",
      "Epoch 134/500\n",
      "14/14 [==============================] - 0s 18ms/step - loss: 0.4627 - accuracy: 0.7924 - val_loss: 0.5997 - val_accuracy: 0.6635\n",
      "Epoch 135/500\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 0.4521 - accuracy: 0.8067 - val_loss: 0.6040 - val_accuracy: 0.6635\n",
      "Epoch 136/500\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 0.4633 - accuracy: 0.7947 - val_loss: 0.6015 - val_accuracy: 0.6635\n",
      "Epoch 137/500\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 0.4482 - accuracy: 0.8043 - val_loss: 0.6014 - val_accuracy: 0.6731\n",
      "Epoch 138/500\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 0.4514 - accuracy: 0.8043 - val_loss: 0.6038 - val_accuracy: 0.6635\n",
      "Epoch 139/500\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 0.4526 - accuracy: 0.8162 - val_loss: 0.6013 - val_accuracy: 0.6731\n",
      "Epoch 140/500\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 0.4478 - accuracy: 0.8043 - val_loss: 0.6033 - val_accuracy: 0.6731\n",
      "Epoch 141/500\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 0.4502 - accuracy: 0.8067 - val_loss: 0.6053 - val_accuracy: 0.6635\n",
      "Epoch 142/500\n",
      "14/14 [==============================] - 0s 30ms/step - loss: 0.4400 - accuracy: 0.8162 - val_loss: 0.6053 - val_accuracy: 0.6635\n",
      "Epoch 143/500\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 0.4477 - accuracy: 0.8067 - val_loss: 0.6058 - val_accuracy: 0.6731\n",
      "Epoch 144/500\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 0.4420 - accuracy: 0.8067 - val_loss: 0.6111 - val_accuracy: 0.6827\n",
      "Epoch 145/500\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 0.4494 - accuracy: 0.8234 - val_loss: 0.6042 - val_accuracy: 0.6731\n",
      "Epoch 146/500\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 0.4425 - accuracy: 0.8115 - val_loss: 0.6057 - val_accuracy: 0.6731\n",
      "Epoch 147/500\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 0.4414 - accuracy: 0.8091 - val_loss: 0.6045 - val_accuracy: 0.6731\n",
      "Epoch 148/500\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.4351 - accuracy: 0.8091 - val_loss: 0.6055 - val_accuracy: 0.6731\n",
      "Epoch 149/500\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 0.4333 - accuracy: 0.8258 - val_loss: 0.6063 - val_accuracy: 0.6827\n",
      "Epoch 150/500\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.4410 - accuracy: 0.8115 - val_loss: 0.6052 - val_accuracy: 0.6635\n",
      "Epoch 151/500\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 0.4286 - accuracy: 0.8258 - val_loss: 0.6062 - val_accuracy: 0.6635\n",
      "Epoch 152/500\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 0.4359 - accuracy: 0.8234 - val_loss: 0.6093 - val_accuracy: 0.6827\n",
      "Epoch 153/500\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 0.4301 - accuracy: 0.8186 - val_loss: 0.6071 - val_accuracy: 0.6731\n",
      "Epoch 154/500\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 0.4323 - accuracy: 0.8210 - val_loss: 0.6090 - val_accuracy: 0.6538\n",
      "Epoch 155/500\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.4298 - accuracy: 0.8115 - val_loss: 0.6073 - val_accuracy: 0.6827\n",
      "Epoch 156/500\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 0.4236 - accuracy: 0.8282 - val_loss: 0.6096 - val_accuracy: 0.6538\n",
      "Epoch 157/500\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 0.4321 - accuracy: 0.8162 - val_loss: 0.6069 - val_accuracy: 0.6731\n",
      "Epoch 158/500\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 0.4267 - accuracy: 0.8067 - val_loss: 0.6064 - val_accuracy: 0.6635\n",
      "Epoch 159/500\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.4291 - accuracy: 0.8258 - val_loss: 0.6066 - val_accuracy: 0.6827\n",
      "Epoch 160/500\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.4295 - accuracy: 0.8210 - val_loss: 0.6090 - val_accuracy: 0.6635\n",
      "Epoch 161/500\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.4228 - accuracy: 0.8329 - val_loss: 0.6054 - val_accuracy: 0.6827\n",
      "Epoch 162/500\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 0.4332 - accuracy: 0.8210 - val_loss: 0.6076 - val_accuracy: 0.6827\n",
      "Epoch 163/500\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.4360 - accuracy: 0.8138 - val_loss: 0.6060 - val_accuracy: 0.6635\n",
      "Epoch 164/500\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.4331 - accuracy: 0.8138 - val_loss: 0.6059 - val_accuracy: 0.7019\n",
      "Epoch 165/500\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.4314 - accuracy: 0.8258 - val_loss: 0.6096 - val_accuracy: 0.6731\n",
      "2/2 [==============================] - 0s 3ms/step\n"
     ]
    }
   ],
   "source": [
    "# Variable for keeping count of split we are executing\n",
    "j = 0\n",
    "list_kfold = list(kfold.split(X_trainval, y_trainval))\n",
    "data_kfold = pd.DataFrame()\n",
    "for i in range(5):\n",
    "    train_idx, val_idx = list_kfold[i]\n",
    "    X_train_kfold = X_trainval[train_idx]\n",
    "    y_train_kfold = y_trainval[train_idx]\n",
    "    y_train_kfold_complement = 1 - y_train_kfold \n",
    "    #Compute its complement and try to join it with 'y_train_kfold' to make it 2-D.\n",
    "    y_train_kfold = np.stack((y_train_kfold, y_train_kfold_complement), axis=1)\n",
    "    X_val_kfold = X_trainval[val_idx]\n",
    "    y_val_kfold = y_trainval[val_idx]\n",
    "    y_val_kfold_complement = 1 - y_val_kfold\n",
    "    y_val_kfold = np.stack((y_val_kfold, y_val_kfold_complement), axis=1)\n",
    "    j += 1\n",
    "    model_kfold = get_model()\n",
    "    model_kfold.fit(X_train_kfold, y_train_kfold,\n",
    "                   validation_data = (X_val_kfold, y_val_kfold),\n",
    "                   epochs = 500, verbose = True,\n",
    "                   callbacks = [early_stopping_monitor])\n",
    "    pred = model_kfold.predict(X_test)\n",
    "    predicted_class_indices=np.argmax(pred, axis = 1)\n",
    "    data_kfold[j] = predicted_class_indices\n",
    "#    gc.collect()    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d672c05",
   "metadata": {},
   "source": [
    "So in the above code, we are just repeating the CNN model fitting and predicting on test set 5 times and storing the output in data_kfold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "6d363ba1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    1  2  3  4  5\n",
       "0   1  1  1  1  1\n",
       "1   1  1  1  1  1\n",
       "2   1  0  1  1  1\n",
       "3   0  0  1  1  1\n",
       "4   0  0  1  1  0\n",
       "5   0  0  1  1  0\n",
       "6   1  1  1  1  1\n",
       "7   1  1  1  1  1\n",
       "8   1  1  1  1  1\n",
       "9   1  1  1  1  1\n",
       "10  1  1  1  1  1\n",
       "11  1  1  1  1  1\n",
       "12  1  1  1  1  1\n",
       "13  1  1  1  1  1\n",
       "14  1  1  1  1  1\n",
       "15  1  1  1  1  1\n",
       "16  1  1  1  1  1\n",
       "17  1  1  1  1  1\n",
       "18  1  1  1  1  1\n",
       "19  0  1  1  1  1\n",
       "20  1  1  1  1  1\n",
       "21  1  1  1  1  1\n",
       "22  1  1  1  1  1\n",
       "23  0  0  1  1  1\n",
       "24  1  1  1  1  1\n",
       "25  0  0  1  1  1\n",
       "26  1  1  1  1  1\n",
       "27  1  1  1  1  1\n",
       "28  1  1  1  1  1\n",
       "29  1  0  1  1  0\n",
       "30  0  0  1  0  0\n",
       "31  1  1  1  1  1\n",
       "32  0  0  1  1  0\n",
       "33  1  1  1  1  1\n",
       "34  1  1  1  1  1\n",
       "35  1  1  1  1  1\n",
       "36  1  1  1  1  1\n",
       "37  1  1  1  1  1\n",
       "38  1  1  1  1  1\n",
       "39  1  1  1  1  1\n",
       "40  1  1  1  1  1\n",
       "41  1  1  1  1  1\n",
       "42  0  0  1  1  0\n",
       "43  0  0  1  1  0\n",
       "44  1  1  1  1  1\n",
       "45  1  1  1  1  1\n",
       "46  1  1  1  1  1\n",
       "47  1  1  1  1  1\n",
       "48  1  1  1  1  1\n",
       "49  1  1  1  1  1\n",
       "50  1  1  1  1  1\n",
       "51  1  1  1  1  1"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_kfold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "6cf5a228",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1, 1, 0])"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_kfold_arr = np.array(data_kfold) #Convert 'data_kfold' from a dataframe to an array.\n",
    "data_kfold_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "a82073f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(data_kfold_arr[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "2cd7f58a",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_kfold = np.array(range(52))\n",
    "for i in range(52):\n",
    "    sum_kfold = sum(data_kfold_arr[i])\n",
    "    if sum_kfold < 3:\n",
    "        predict_kfold[i] = 0\n",
    "    else:\n",
    "        predict_kfold[i] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "62b6c0f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_kfold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "26aa44fd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1],\n",
       "       [0, 1],\n",
       "       [0, 1],\n",
       "       [0, 1],\n",
       "       [1, 0],\n",
       "       [1, 0],\n",
       "       [0, 1],\n",
       "       [0, 1],\n",
       "       [0, 1],\n",
       "       [0, 1],\n",
       "       [0, 1],\n",
       "       [0, 1],\n",
       "       [0, 1],\n",
       "       [0, 1],\n",
       "       [0, 1],\n",
       "       [0, 1],\n",
       "       [0, 1],\n",
       "       [0, 1],\n",
       "       [0, 1],\n",
       "       [0, 1],\n",
       "       [0, 1],\n",
       "       [0, 1],\n",
       "       [0, 1],\n",
       "       [0, 1],\n",
       "       [0, 1],\n",
       "       [0, 1],\n",
       "       [0, 1],\n",
       "       [0, 1],\n",
       "       [0, 1],\n",
       "       [0, 1],\n",
       "       [1, 0],\n",
       "       [0, 1],\n",
       "       [1, 0],\n",
       "       [0, 1],\n",
       "       [0, 1],\n",
       "       [0, 1],\n",
       "       [0, 1],\n",
       "       [0, 1],\n",
       "       [0, 1],\n",
       "       [0, 1],\n",
       "       [0, 1],\n",
       "       [0, 1],\n",
       "       [1, 0],\n",
       "       [1, 0],\n",
       "       [0, 1],\n",
       "       [0, 1],\n",
       "       [0, 1],\n",
       "       [0, 1],\n",
       "       [0, 1],\n",
       "       [0, 1],\n",
       "       [0, 1],\n",
       "       [0, 1]])"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_kfold_complement = 1 - predict_kfold\n",
    "predict_kfold_2D = np.stack((predict_kfold_complement, predict_kfold), axis=1)\n",
    "predict_kfold_2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "e3832c42",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Progressor</th>\n",
       "      <th>Non-Progressor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>222</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>250</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>251</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>252</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>258</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>259</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>260</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>261</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>263</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>264</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>273</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>274</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>277</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>278</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>288</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>312</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>313</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>320</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>321</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>334</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>335</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>339</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>340</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>370</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>437</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>478</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>479</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>482</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>489</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>501</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>519</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>532</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>533</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>544</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>545</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Progressor  Non-Progressor\n",
       "26            0               1\n",
       "27            0               1\n",
       "51            1               0\n",
       "52            0               1\n",
       "62            1               0\n",
       "63            0               1\n",
       "118           0               1\n",
       "143           1               0\n",
       "144           0               1\n",
       "145           0               1\n",
       "146           0               1\n",
       "196           0               1\n",
       "199           0               1\n",
       "200           0               1\n",
       "201           0               1\n",
       "221           0               1\n",
       "222           0               1\n",
       "249           0               1\n",
       "250           0               1\n",
       "251           0               1\n",
       "252           0               1\n",
       "258           0               1\n",
       "259           0               1\n",
       "260           0               1\n",
       "261           0               1\n",
       "263           1               0\n",
       "264           0               1\n",
       "273           0               1\n",
       "274           0               1\n",
       "277           1               0\n",
       "278           0               1\n",
       "288           1               0\n",
       "312           0               1\n",
       "313           1               0\n",
       "320           0               1\n",
       "321           0               1\n",
       "334           1               0\n",
       "335           1               0\n",
       "339           0               1\n",
       "340           0               1\n",
       "370           0               1\n",
       "437           0               1\n",
       "478           0               1\n",
       "479           1               0\n",
       "482           1               0\n",
       "489           0               1\n",
       "501           0               1\n",
       "519           1               0\n",
       "532           1               0\n",
       "533           1               0\n",
       "544           0               1\n",
       "545           0               1"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "4a7753c8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Progressor</th>\n",
       "      <th>Non-Progressor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Progressor  Non-Progressor\n",
       "0            0               1\n",
       "1            0               1\n",
       "2            0               1\n",
       "3            0               1\n",
       "4            1               0\n",
       "5            1               0\n",
       "6            0               1\n",
       "7            0               1\n",
       "8            0               1\n",
       "9            0               1\n",
       "10           0               1\n",
       "11           0               1\n",
       "12           0               1\n",
       "13           0               1\n",
       "14           0               1\n",
       "15           0               1\n",
       "16           0               1\n",
       "17           0               1\n",
       "18           0               1\n",
       "19           0               1\n",
       "20           0               1\n",
       "21           0               1\n",
       "22           0               1\n",
       "23           0               1\n",
       "24           0               1\n",
       "25           0               1\n",
       "26           0               1\n",
       "27           0               1\n",
       "28           0               1\n",
       "29           0               1\n",
       "30           1               0\n",
       "31           0               1\n",
       "32           1               0\n",
       "33           0               1\n",
       "34           0               1\n",
       "35           0               1\n",
       "36           0               1\n",
       "37           0               1\n",
       "38           0               1\n",
       "39           0               1\n",
       "40           0               1\n",
       "41           0               1\n",
       "42           1               0\n",
       "43           1               0\n",
       "44           0               1\n",
       "45           0               1\n",
       "46           0               1\n",
       "47           0               1\n",
       "48           0               1\n",
       "49           0               1\n",
       "50           0               1\n",
       "51           0               1"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_kfold_2D = pd.DataFrame(predict_kfold_2D, columns=('Progressor', 'Non-Progressor'))\n",
    "predict_kfold_2D"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7393993",
   "metadata": {},
   "source": [
    "Evaluation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "52d6cb6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "roc auc score:  0.518796992481203\n",
      "average precision score:  0.5083192932163183\n"
     ]
    }
   ],
   "source": [
    "roc_value = roc_auc_score(y_test, predict_kfold_2D)\n",
    "ap_score = average_precision_score(y_test, predict_kfold_2D)\n",
    "print('roc auc score: ', roc_value)\n",
    "print('average precision score: ', ap_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0504cac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "51955dc5",
   "metadata": {},
   "source": [
    "## Random Search for Model Tuning\n",
    "\n",
    "@Yanlong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c5304d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# not run\n",
    "# adding second convolutional layer \n",
    "    keras.layers.Conv1D(\n",
    "        #adding filter \n",
    "        filters=hp.Int('conv_2_filter', min_value=16, max_value=64, step=16),\n",
    "        #adding filter size or kernel size\n",
    "        kernel_size=hp.Choice('conv_2_kernel', values = [3,5]),\n",
    "        #activation function\n",
    "        activation='relu'\n",
    "    ),\n",
    "    keras.layers.MaxPooling1D(\n",
    "        pool_size= 2\n",
    "    ),\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b0d58c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(hp):\n",
    "    # create model object\n",
    "    model = keras.Sequential([\n",
    "    #adding first convolutional layer    \n",
    "    keras.layers.Conv1D(\n",
    "        #adding filter \n",
    "        filters=hp.Int('conv_1_filter', min_value=16, max_value=128, step=16),\n",
    "        # adding filter size or kernel size\n",
    "        kernel_size=hp.Choice('conv_1_kernel', values = [3,5]),\n",
    "        #activation function\n",
    "        activation='relu',\n",
    "        input_shape=(768,1)\n",
    "    ),\n",
    "    keras.layers.MaxPooling1D(\n",
    "        pool_size= hp.Choice('pool_size',values = [2,3,5])\n",
    "    ),\n",
    "    keras.layers.Dropout(0.5),\n",
    "    # adding flatten layer    \n",
    "    keras.layers.Flatten(),\n",
    "    # adding dense layer    \n",
    "    keras.layers.Dense(\n",
    "        units=hp.Int('dense_1_units', min_value=16, max_value=128, step=16),\n",
    "        activation='relu'\n",
    "    ),\n",
    "    # output layer    \n",
    "    keras.layers.Dense(2, activation='softmax')\n",
    "    ])\n",
    "    #compilation of model\n",
    "    model.compile(optimizer=keras.optimizers.Adam(hp.Choice('learning_rate', values=[1e-3, 1e-4])),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "22958712",
   "metadata": {},
   "outputs": [],
   "source": [
    "es = EarlyStopping(\n",
    "    monitor='val_accuracy',\n",
    "    min_delta=0,\n",
    "    patience=100,\n",
    "    verbose=0,\n",
    "    mode='auto',\n",
    "    baseline=None,\n",
    "    restore_best_weights=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "00151207",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 100 Complete [00h 00m 20s]\n",
      "val_accuracy: 0.7272727489471436\n",
      "\n",
      "Best val_accuracy So Far: 0.7818182110786438\n",
      "Total elapsed time: 00h 19m 37s\n",
      "INFO:tensorflow:Oracle triggered exit\n"
     ]
    }
   ],
   "source": [
    "#importing random search\n",
    "#from keras_tuner import RandomSearch\n",
    "import keras_tuner as kt\n",
    "from tensorflow import keras\n",
    "#creating randomsearch object\n",
    "tuner = kt.RandomSearch(build_model,\n",
    "                    objective='val_accuracy',\n",
    "                    max_trials = 100)\n",
    "# search best parameter\n",
    "tuner.search(X_train,y_train,epochs=200,\n",
    "             validation_data=(X_val,y_val),\n",
    "            callbacks=[es]\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "28f9055c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d (Conv1D)              (None, 766, 48)           192       \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (None, 383, 48)           0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 383, 48)           0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 18384)             0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 64)                1176640   \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2)                 130       \n",
      "=================================================================\n",
      "Total params: 1,176,962\n",
      "Trainable params: 1,176,962\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model=tuner.get_best_models(num_models=1)[0]\n",
    "#summary of best model\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "840d3923",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "roc auc score:  0.36466165413533835\n",
      "average precision score:  0.4604905526212037\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-29 12:46:37.010054: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    }
   ],
   "source": [
    "pred = model.predict(X_test)\n",
    "roc_value = roc_auc_score(y_test, pred)\n",
    "ap_score = average_precision_score(y_test, pred)\n",
    "print('roc auc score: ', roc_value)\n",
    "print('average precision score: ', ap_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "7a604c94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 22ms/step - loss: 1.1378 - accuracy: 0.6346\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.6423 - accuracy: 0.7818\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-29 12:46:39.893166: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    }
   ],
   "source": [
    "mbest_eval_test = model.evaluate(X_test, y_test)\n",
    "mbest_eval_val = model.evaluate(X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "df620fad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "roc auc score:  0.36466165413533835\n",
      "average precision score:  0.4604905526212037\n"
     ]
    }
   ],
   "source": [
    "pred = model.predict(X_test)\n",
    "roc_value = roc_auc_score(y_test, pred)\n",
    "ap_score = average_precision_score(y_test, pred)\n",
    "print('roc auc score: ', roc_value)\n",
    "print('average precision score: ', ap_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82a7d75f",
   "metadata": {},
   "source": [
    "Save the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "acfb7f7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'CNN_GRI_roc_value=0.36466165413533835_ap_score=0.4604905526212037.h5'"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename = 'CNN_GRI_roc_value=' + str(roc_value) + '_ap_score=' + str(ap_score) + '.h5'\n",
    "filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f2c2ab0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model/assets\n"
     ]
    }
   ],
   "source": [
    "model.save(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "7b376f20",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "y_c = (y_pred > 0.5).astype(\"int32\")\n",
    "y_test_np = y_test.to_numpy()\n",
    "y_test_np = y_test_np.astype('int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "e4e2b39a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 8ms/step - loss: 1.1378 - accuracy: 0.6346\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "cf_matrix = confusion_matrix(y_test_np.argmax(axis=1), y_c.argmax(axis=1)) \n",
    "\n",
    "ax = sns.heatmap(cf_matrix, annot=True, cmap='Blues')\n",
    "\n",
    "ax.set_title('Seaborn Confusion Matrix with labels\\n\\n');\n",
    "ax.set_xlabel('\\nPredicted Values')\n",
    "ax.set_ylabel('Actual Values ');\n",
    "\n",
    "## Ticket labels - List must be in alphabetical order\n",
    "ax.xaxis.set_ticklabels(['Progressor','Non-Progressor'])\n",
    "ax.yaxis.set_ticklabels(['Progressor','Non-Progressor'])\n",
    "\n",
    "## Display the visualization of the Confusion Matrix.\n",
    "plt.savefig('GRI_test.png')\n",
    "m1_eval_test = model.evaluate(X_test, y_test)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "067eeac5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 8ms/step - loss: 0.6423 - accuracy: 0.7818\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "y_pred = model.predict(X_val)\n",
    "y_c = (y_pred > 0.5).astype(\"int32\")\n",
    "y_val_np = y_val.to_numpy()\n",
    "y_val_np = y_val_np.astype('int32')\n",
    "\n",
    "cf_matrix = confusion_matrix(y_val_np.argmax(axis=1), y_c.argmax(axis=1)) \n",
    "\n",
    "ax = sns.heatmap(cf_matrix, annot=True, cmap='Blues')\n",
    "\n",
    "ax.set_title('Seaborn Confusion Matrix with labels\\n\\n');\n",
    "ax.set_xlabel('\\nPredicted Values')\n",
    "ax.set_ylabel('Actual Values ');\n",
    "\n",
    "## Ticket labels - List must be in alphabetical order\n",
    "ax.xaxis.set_ticklabels(['Progressor','Non-Progressor'])\n",
    "ax.yaxis.set_ticklabels(['Progressor','Non-Progressor'])\n",
    "\n",
    "## Display the visualization of the Confusion Matrix.\n",
    "plt.savefig('GRI_val.png')\n",
    "m1_eval_test = model.evaluate(X_val, y_val)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b65d44e3",
   "metadata": {},
   "source": [
    "## Grid Search for Model Tuning\n",
    "\n",
    "@Sherry, @Kara"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "4ed547e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "28778474",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1322677742053997\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'C': 10,\n",
       " 'break_ties': False,\n",
       " 'cache_size': 200,\n",
       " 'class_weight': None,\n",
       " 'coef0': 0.0,\n",
       " 'decision_function_shape': 'ovr',\n",
       " 'degree': 3,\n",
       " 'gamma': 'scale',\n",
       " 'kernel': 'rbf',\n",
       " 'max_iter': -1,\n",
       " 'probability': False,\n",
       " 'random_state': None,\n",
       " 'shrinking': True,\n",
       " 'tol': 0.001,\n",
       " 'verbose': False}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Take a look at current model hyper parameters\n",
    "print(svc_m1._gamma) # 1 / (n_features * X.var())\n",
    "svc_m1.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "60d521d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 20, 'gamma': 0.01, 'kernel': 'rbf'}\n"
     ]
    }
   ],
   "source": [
    "svc_param_grid = {'C': [1, 4, 6, 8, 10, 20], 'kernel': ['linear', 'rbf', 'poly'], \n",
    "                 'gamma': ['scale', 'auto', 0.01, 0.1, 0.25]}\n",
    "# gamma from scale is 0.5852071349905691\n",
    "# gamma from auto = 1/n = 0.001\n",
    "svm_mod_search_all = svm.SVC() \n",
    "svc_grid = GridSearchCV(svm_mod_search_all, svc_param_grid)\n",
    "svc_grid.fit(X_train, y_train_1d)\n",
    "print(svc_grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "6cbcc4db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=20, gamma=0.01)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svc_optimized = svm.SVC(kernel='rbf', C=20, gamma = 0.01)\n",
    "svc_optimized.fit(X_train, y_train_1d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "36dfed5e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7068965517241379\n",
      "True Positives: 0.0\n",
      "True Negatives: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/feiz/miniforge3/envs/tensorflow/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[41,  0],\n",
       "       [17,  0]])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svc_optimized_pred = svc_optimized.predict(X_test)\n",
    "\n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_test_1d, svc_optimized_pred))\n",
    "print(\"True Positives:\",metrics.precision_score(y_test_1d, svc_optimized_pred))\n",
    "print(\"True Negatives:\",metrics.recall_score(y_test_1d, svc_optimized_pred))\n",
    "metrics.confusion_matrix(y_test_1d, svc_optimized_pred)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33560cfb",
   "metadata": {},
   "source": [
    "The grid search optimal parameters may not give you the best performance on the test data, needs more investigation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77d044bb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
