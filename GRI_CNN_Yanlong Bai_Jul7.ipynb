{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "84f29bf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  0\n",
      "2.9.1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4ed51dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd   ## data analysis and manipulation\n",
    "import numpy as np    ## numerial computing\n",
    "import seaborn as sns ##  data visualization library based on matplotlib\n",
    "import tensorflow.keras as keras ## main deep learning API\n",
    "\n",
    "## additional functions\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, accuracy_score\n",
    "from sklearn.metrics import average_precision_score\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c4125419",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Conv1D\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import MaxPooling1D\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from sklearn.utils import class_weight\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "622f788c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>EID</th>\n",
       "      <th>PID</th>\n",
       "      <th>DOB</th>\n",
       "      <th>Eye</th>\n",
       "      <th>ImageID</th>\n",
       "      <th>Scan.Type</th>\n",
       "      <th>Diameter..mm.</th>\n",
       "      <th>Diameter....</th>\n",
       "      <th>Fixed.in.mm</th>\n",
       "      <th>ExamDate</th>\n",
       "      <th>...</th>\n",
       "      <th>VF_OCT_BASELINE_DIFF</th>\n",
       "      <th>VF_OCT_FINAL_DIFF</th>\n",
       "      <th>MD_BASELINE</th>\n",
       "      <th>MD_FINAL</th>\n",
       "      <th>VFI_BASELINE</th>\n",
       "      <th>VFI_FINAL</th>\n",
       "      <th>Y_GRI</th>\n",
       "      <th>Y_MD</th>\n",
       "      <th>Y_VFI</th>\n",
       "      <th>Y_combined</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10/24/1949</td>\n",
       "      <td>LE</td>\n",
       "      <td>282596.0</td>\n",
       "      <td>OCT Circle Scan</td>\n",
       "      <td>3.7</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0</td>\n",
       "      <td>5/11/2017</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.561944</td>\n",
       "      <td>-2.15</td>\n",
       "      <td>-3.26</td>\n",
       "      <td>98</td>\n",
       "      <td>96</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>10/24/1949</td>\n",
       "      <td>RE</td>\n",
       "      <td>282593.0</td>\n",
       "      <td>OCT Circle Scan</td>\n",
       "      <td>3.7</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0</td>\n",
       "      <td>5/11/2017</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.561944</td>\n",
       "      <td>-7.73</td>\n",
       "      <td>-11.45</td>\n",
       "      <td>82</td>\n",
       "      <td>73</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>8/7/1939</td>\n",
       "      <td>LE</td>\n",
       "      <td>239514.0</td>\n",
       "      <td>OCT Circle Scan</td>\n",
       "      <td>3.4</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0</td>\n",
       "      <td>8/26/2014</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.151951</td>\n",
       "      <td>-1.28</td>\n",
       "      <td>-1.13</td>\n",
       "      <td>98</td>\n",
       "      <td>97</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>8/7/1939</td>\n",
       "      <td>RE</td>\n",
       "      <td>239512.0</td>\n",
       "      <td>OCT Circle Scan</td>\n",
       "      <td>3.4</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0</td>\n",
       "      <td>8/26/2014</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.151951</td>\n",
       "      <td>-0.72</td>\n",
       "      <td>0.60</td>\n",
       "      <td>98</td>\n",
       "      <td>99</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>5/20/1943</td>\n",
       "      <td>LE</td>\n",
       "      <td>238460.0</td>\n",
       "      <td>OCT Circle Scan</td>\n",
       "      <td>3.5</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0</td>\n",
       "      <td>7/9/2014</td>\n",
       "      <td>...</td>\n",
       "      <td>0.024641</td>\n",
       "      <td>6.266940</td>\n",
       "      <td>-1.69</td>\n",
       "      <td>-0.51</td>\n",
       "      <td>99</td>\n",
       "      <td>99</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>579</th>\n",
       "      <td>580</td>\n",
       "      <td>329</td>\n",
       "      <td>3/22/1952</td>\n",
       "      <td>RE</td>\n",
       "      <td>837.0</td>\n",
       "      <td>OCT Circle Scan</td>\n",
       "      <td>3.7</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0</td>\n",
       "      <td>5/5/2011</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.601643</td>\n",
       "      <td>0.53</td>\n",
       "      <td>-2.51</td>\n",
       "      <td>98</td>\n",
       "      <td>93</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>580</th>\n",
       "      <td>581</td>\n",
       "      <td>330</td>\n",
       "      <td>5/15/1945</td>\n",
       "      <td>LE</td>\n",
       "      <td>243095.0</td>\n",
       "      <td>OCT Circle Scan</td>\n",
       "      <td>3.5</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0</td>\n",
       "      <td>12/17/2014</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.941136</td>\n",
       "      <td>-8.97</td>\n",
       "      <td>-14.71</td>\n",
       "      <td>78</td>\n",
       "      <td>56</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>581</th>\n",
       "      <td>582</td>\n",
       "      <td>330</td>\n",
       "      <td>5/15/1945</td>\n",
       "      <td>RE</td>\n",
       "      <td>243093.0</td>\n",
       "      <td>OCT Circle Scan</td>\n",
       "      <td>3.7</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0</td>\n",
       "      <td>12/17/2014</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.941136</td>\n",
       "      <td>-11.39</td>\n",
       "      <td>-11.37</td>\n",
       "      <td>70</td>\n",
       "      <td>67</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>582</th>\n",
       "      <td>583</td>\n",
       "      <td>331</td>\n",
       "      <td>5/31/1939</td>\n",
       "      <td>LE</td>\n",
       "      <td>109347.0</td>\n",
       "      <td>OCT Circle Scan</td>\n",
       "      <td>3.5</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0</td>\n",
       "      <td>8/13/2013</td>\n",
       "      <td>...</td>\n",
       "      <td>0.172485</td>\n",
       "      <td>6.193018</td>\n",
       "      <td>-3.48</td>\n",
       "      <td>-19.28</td>\n",
       "      <td>97</td>\n",
       "      <td>51</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>583</th>\n",
       "      <td>584</td>\n",
       "      <td>331</td>\n",
       "      <td>5/31/1939</td>\n",
       "      <td>RE</td>\n",
       "      <td>109343.2</td>\n",
       "      <td>OCT Circle Scan</td>\n",
       "      <td>3.5</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0</td>\n",
       "      <td>8/13/2013</td>\n",
       "      <td>...</td>\n",
       "      <td>0.172485</td>\n",
       "      <td>6.193018</td>\n",
       "      <td>-3.34</td>\n",
       "      <td>-16.15</td>\n",
       "      <td>93</td>\n",
       "      <td>47</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>584 rows × 815 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     EID  PID         DOB Eye   ImageID        Scan.Type  Diameter..mm.  \\\n",
       "0      1    1  10/24/1949  LE  282596.0  OCT Circle Scan            3.7   \n",
       "1      2    1  10/24/1949  RE  282593.0  OCT Circle Scan            3.7   \n",
       "2      3    2    8/7/1939  LE  239514.0  OCT Circle Scan            3.4   \n",
       "3      4    2    8/7/1939  RE  239512.0  OCT Circle Scan            3.4   \n",
       "4      5    3   5/20/1943  LE  238460.0  OCT Circle Scan            3.5   \n",
       "..   ...  ...         ...  ..       ...              ...            ...   \n",
       "579  580  329   3/22/1952  RE     837.0  OCT Circle Scan            3.7   \n",
       "580  581  330   5/15/1945  LE  243095.0  OCT Circle Scan            3.5   \n",
       "581  582  330   5/15/1945  RE  243093.0  OCT Circle Scan            3.7   \n",
       "582  583  331   5/31/1939  LE  109347.0  OCT Circle Scan            3.5   \n",
       "583  584  331   5/31/1939  RE  109343.2  OCT Circle Scan            3.5   \n",
       "\n",
       "     Diameter....  Fixed.in.mm    ExamDate  ... VF_OCT_BASELINE_DIFF  \\\n",
       "0            12.0            0   5/11/2017  ...             0.000000   \n",
       "1            12.0            0   5/11/2017  ...             0.000000   \n",
       "2            12.0            0   8/26/2014  ...             0.000000   \n",
       "3            12.0            0   8/26/2014  ...             0.000000   \n",
       "4            12.0            0    7/9/2014  ...             0.024641   \n",
       "..            ...          ...         ...  ...                  ...   \n",
       "579          12.0            0    5/5/2011  ...             0.000000   \n",
       "580          12.0            0  12/17/2014  ...             0.000000   \n",
       "581          12.0            0  12/17/2014  ...             0.000000   \n",
       "582          12.0            0   8/13/2013  ...             0.172485   \n",
       "583          12.0            0   8/13/2013  ...             0.172485   \n",
       "\n",
       "    VF_OCT_FINAL_DIFF  MD_BASELINE  MD_FINAL  VFI_BASELINE  VFI_FINAL  Y_GRI  \\\n",
       "0            3.561944        -2.15     -3.26            98         96      0   \n",
       "1            3.561944        -7.73    -11.45            82         73      1   \n",
       "2            6.151951        -1.28     -1.13            98         97      0   \n",
       "3            6.151951        -0.72      0.60            98         99      0   \n",
       "4            6.266940        -1.69     -0.51            99         99      0   \n",
       "..                ...          ...       ...           ...        ...    ...   \n",
       "579          9.601643         0.53     -2.51            98         93      1   \n",
       "580          5.941136        -8.97    -14.71            78         56      1   \n",
       "581          5.941136       -11.39    -11.37            70         67      1   \n",
       "582          6.193018        -3.48    -19.28            97         51      1   \n",
       "583          6.193018        -3.34    -16.15            93         47      1   \n",
       "\n",
       "     Y_MD  Y_VFI  Y_combined  \n",
       "0       0      0           0  \n",
       "1       0      0           1  \n",
       "2       0      0           0  \n",
       "3       0      0           0  \n",
       "4       0      0           0  \n",
       "..    ...    ...         ...  \n",
       "579     0      0           1  \n",
       "580     1      1           1  \n",
       "581     0      0           1  \n",
       "582     1      1           1  \n",
       "583     1      1           1  \n",
       "\n",
       "[584 rows x 815 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load the data\n",
    "df = pd.read_csv(\"/Users/a123456/Desktop/Fei's Project/Data/OCT_BASELINE_GRI__VF_6-3_FP-15_NO_PHI_CombinedProgression.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e47f1c67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(580, 815)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#filter only circle scan data\n",
    "circle_scan = (df['Scan.Type'] == 'OCT Circle Scan')\n",
    "df = df[circle_scan]\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "967ebc42",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw = df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9321d55a",
   "metadata": {},
   "source": [
    "We will use 'Y_combined' instead of 'Y_GRI' as our dependent variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b00675e",
   "metadata": {},
   "source": [
    "**Idea 1: Using 'RNFLT 1 to 768' as the predictors, 'Y_combined' as the dependent variable, no resampling, CNN method  \n",
    "Idea 2: Using 'RNFLT 1 to 768' as the predictors, 'Y_combined' as the dependent variable, with resampling, CNN method  \n",
    "Idea 3: Using 'RNFLT 1 to 768' as the predictors, 'Y_combined' as the dependent variable, no resampling, k-fold validation method  \n",
    "Idea 4: Using 'RNFLT 1 to 768' as the predictors, 'Y_combined' as the dependent variable, with resampling, k-fold validation method  \n",
    "Idea 5: Using all numerical values as the predictors, 'Y_combined' as the dependent variable, no resampling, CNN method  \n",
    "Idea 6: Using all numerical values as the predictors, 'Y_combined' as the dependent variable, with resampling, CNN method  \n",
    "Idea 7: Using all numerical values as the predictors, 'Y_combined' as the dependent variable, no resampling, k-fold method  \n",
    "Idea 8: Using all numerical values as the predictors, 'Y_combined' as the dependent variable, with resampling, k-fold method**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0db65ab7",
   "metadata": {},
   "source": [
    "### Idea 1: Using 'RNFLT 1 to 768' as the predictors, 'Y_combined' as the dependent variable, no resampling, CNN method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cc7a65ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PID</th>\n",
       "      <th>RNFLT.1</th>\n",
       "      <th>RNFLT.2</th>\n",
       "      <th>RNFLT.3</th>\n",
       "      <th>RNFLT.4</th>\n",
       "      <th>RNFLT.5</th>\n",
       "      <th>RNFLT.6</th>\n",
       "      <th>RNFLT.7</th>\n",
       "      <th>RNFLT.8</th>\n",
       "      <th>RNFLT.9</th>\n",
       "      <th>...</th>\n",
       "      <th>RNFLT.761</th>\n",
       "      <th>RNFLT.762</th>\n",
       "      <th>RNFLT.763</th>\n",
       "      <th>RNFLT.764</th>\n",
       "      <th>RNFLT.765</th>\n",
       "      <th>RNFLT.766</th>\n",
       "      <th>RNFLT.767</th>\n",
       "      <th>RNFLT.768</th>\n",
       "      <th>GRI</th>\n",
       "      <th>Y_combined</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>47.0</td>\n",
       "      <td>47.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>...</td>\n",
       "      <td>48.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>47.0</td>\n",
       "      <td>-3.688171</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>70.0</td>\n",
       "      <td>71.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>...</td>\n",
       "      <td>60.0</td>\n",
       "      <td>61.0</td>\n",
       "      <td>62.0</td>\n",
       "      <td>63.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>69.0</td>\n",
       "      <td>-6.827438</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>44.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>47.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>...</td>\n",
       "      <td>45.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>0.329429</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>44.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>47.0</td>\n",
       "      <td>47.0</td>\n",
       "      <td>...</td>\n",
       "      <td>43.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>0.581343</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>37.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>...</td>\n",
       "      <td>35.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>579</th>\n",
       "      <td>329</td>\n",
       "      <td>100.0</td>\n",
       "      <td>103.0</td>\n",
       "      <td>106.0</td>\n",
       "      <td>108.0</td>\n",
       "      <td>111.0</td>\n",
       "      <td>112.0</td>\n",
       "      <td>113.0</td>\n",
       "      <td>113.0</td>\n",
       "      <td>113.0</td>\n",
       "      <td>...</td>\n",
       "      <td>83.0</td>\n",
       "      <td>84.0</td>\n",
       "      <td>86.0</td>\n",
       "      <td>87.0</td>\n",
       "      <td>89.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>94.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>-11.691467</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>580</th>\n",
       "      <td>330</td>\n",
       "      <td>52.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>59.0</td>\n",
       "      <td>...</td>\n",
       "      <td>47.0</td>\n",
       "      <td>47.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>-19.908699</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>581</th>\n",
       "      <td>330</td>\n",
       "      <td>62.0</td>\n",
       "      <td>63.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>...</td>\n",
       "      <td>55.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>59.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>61.0</td>\n",
       "      <td>-10.130481</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>582</th>\n",
       "      <td>331</td>\n",
       "      <td>47.0</td>\n",
       "      <td>47.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>...</td>\n",
       "      <td>47.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>-24.731627</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>583</th>\n",
       "      <td>331</td>\n",
       "      <td>31.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>...</td>\n",
       "      <td>31.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>-18.674765</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>580 rows × 771 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     PID  RNFLT.1  RNFLT.2  RNFLT.3  RNFLT.4  RNFLT.5  RNFLT.6  RNFLT.7  \\\n",
       "0      1     47.0     47.0     46.0     46.0     45.0     45.0     45.0   \n",
       "1      1     70.0     71.0     72.0     72.0     73.0     73.0     73.0   \n",
       "2      2     44.0     45.0     45.0     45.0     46.0     47.0     48.0   \n",
       "3      2     44.0     44.0     44.0     45.0     45.0     46.0     46.0   \n",
       "4      3     37.0     38.0     39.0     40.0     41.0     42.0     43.0   \n",
       "..   ...      ...      ...      ...      ...      ...      ...      ...   \n",
       "579  329    100.0    103.0    106.0    108.0    111.0    112.0    113.0   \n",
       "580  330     52.0     52.0     53.0     54.0     55.0     56.0     57.0   \n",
       "581  330     62.0     63.0     64.0     65.0     66.0     67.0     68.0   \n",
       "582  331     47.0     47.0     48.0     48.0     49.0     49.0     50.0   \n",
       "583  331     31.0     31.0     32.0     33.0     33.0     34.0     35.0   \n",
       "\n",
       "     RNFLT.8  RNFLT.9  ...  RNFLT.761  RNFLT.762  RNFLT.763  RNFLT.764  \\\n",
       "0       45.0     45.0  ...       48.0       48.0       48.0       48.0   \n",
       "1       73.0     74.0  ...       60.0       61.0       62.0       63.0   \n",
       "2       50.0     51.0  ...       45.0       45.0       45.0       45.0   \n",
       "3       47.0     47.0  ...       43.0       43.0       43.0       43.0   \n",
       "4       44.0     46.0  ...       35.0       35.0       35.0       35.0   \n",
       "..       ...      ...  ...        ...        ...        ...        ...   \n",
       "579    113.0    113.0  ...       83.0       84.0       86.0       87.0   \n",
       "580     58.0     59.0  ...       47.0       47.0       48.0       48.0   \n",
       "581     68.0     68.0  ...       55.0       56.0       57.0       58.0   \n",
       "582     50.0     50.0  ...       47.0       46.0       46.0       45.0   \n",
       "583     36.0     37.0  ...       31.0       31.0       30.0       30.0   \n",
       "\n",
       "     RNFLT.765  RNFLT.766  RNFLT.767  RNFLT.768        GRI  Y_combined  \n",
       "0         48.0       48.0       48.0       47.0  -3.688171           0  \n",
       "1         65.0       66.0       67.0       69.0  -6.827438           1  \n",
       "2         45.0       45.0       45.0       44.0   0.329429           0  \n",
       "3         43.0       43.0       43.0       43.0   0.581343           0  \n",
       "4         35.0       35.0       36.0       36.0   0.000000           0  \n",
       "..         ...        ...        ...        ...        ...         ...  \n",
       "579       89.0       92.0       94.0       97.0 -11.691467           1  \n",
       "580       49.0       49.0       50.0       51.0 -19.908699           1  \n",
       "581       58.0       59.0       60.0       61.0 -10.130481           1  \n",
       "582       45.0       46.0       46.0       46.0 -24.731627           1  \n",
       "583       30.0       30.0       30.0       30.0 -18.674765           1  \n",
       "\n",
       "[580 rows x 771 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df_raw.iloc[:, np.r_[1, 28:797, 814]]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "81ed7178",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(575, 771)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#drop missing values\n",
    "df = df.dropna()\n",
    "df.isnull().values.sum()\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0d2b431c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      0\n",
       "1      1\n",
       "2      0\n",
       "3      0\n",
       "4      0\n",
       "      ..\n",
       "579    1\n",
       "580    1\n",
       "581    1\n",
       "582    1\n",
       "583    1\n",
       "Name: Y_combined, Length: 575, dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = df.iloc[:, 770]\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3fcb2547",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/applied-systems-biology/Dynamic_SPHARM/blob/master/SPHARM/classes/stratified_group_shuffle_split.py\n",
    "\n",
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "\n",
    "from sklearn.utils.validation import check_array\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "\n",
    "class GroupShuffleSplitStratified(StratifiedShuffleSplit):\n",
    "\n",
    "    def __init__(self, n_splits=5, test_size=2, train_size=None, random_state=None):\n",
    "\n",
    "        super(GroupShuffleSplitStratified, self).__init__(\n",
    "            n_splits=n_splits,\n",
    "            test_size=test_size,\n",
    "            train_size=train_size,\n",
    "            random_state=random_state)\n",
    "\n",
    "    def _iter_indices(self, X, y, groups):\n",
    "        if groups is None:\n",
    "            raise ValueError(\"The 'groups' parameter should not be None.\")\n",
    "        groups = check_array(groups, ensure_2d=False, dtype=None)\n",
    "        groups_unique, group_indices = np.unique(groups, return_inverse=True)\n",
    "        classes = []\n",
    "        for gr in groups_unique:\n",
    "            classes.append(y[np.where(groups==gr)[0][0]])\n",
    "\n",
    "        for group_train, group_test in super(\n",
    "                GroupShuffleSplitStratified, self)._iter_indices(X=groups_unique, y=classes):\n",
    "            # these are the indices of classes in the partition\n",
    "            # invert them into data indices\n",
    "\n",
    "            train = np.flatnonzero(np.in1d(group_indices, group_train))\n",
    "            test = np.flatnonzero(np.in1d(group_indices, group_test))\n",
    "\n",
    "            yield train, test\n",
    "\n",
    "    def split(self, X, y=None, groups=None):\n",
    "        return super(GroupShuffleSplitStratified, self).split(X, y, groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "73e85eb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(516, 771)\n",
      "(59, 771)\n"
     ]
    }
   ],
   "source": [
    "train_i,test_i = next(GroupShuffleSplitStratified(n_splits=2, test_size=0.1,\n",
    "                                        random_state=8).split(df,y, groups=df['PID']))\n",
    "TrainVal = df.iloc[train_i]\n",
    "TestSet = df.iloc[test_i]\n",
    "print(TrainVal.shape)\n",
    "print(TestSet.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ec4baf7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(463, 771)\n",
      "(53, 771)\n"
     ]
    }
   ],
   "source": [
    "train_id,val_id = next(GroupShuffleSplitStratified(n_splits=2, test_size=0.1,\n",
    "                                        random_state=8).split(TrainVal,y.iloc[train_i], groups=TrainVal['PID']))\n",
    "TrainSet = TrainVal.iloc[train_id]\n",
    "ValSet = TrainVal.iloc[val_id]\n",
    "print(TrainSet.shape)\n",
    "print(ValSet.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "716f7efd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "(59, 768)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RNFLT.1</th>\n",
       "      <th>RNFLT.2</th>\n",
       "      <th>RNFLT.3</th>\n",
       "      <th>RNFLT.4</th>\n",
       "      <th>RNFLT.5</th>\n",
       "      <th>RNFLT.6</th>\n",
       "      <th>RNFLT.7</th>\n",
       "      <th>RNFLT.8</th>\n",
       "      <th>RNFLT.9</th>\n",
       "      <th>RNFLT.10</th>\n",
       "      <th>...</th>\n",
       "      <th>RNFLT.759</th>\n",
       "      <th>RNFLT.760</th>\n",
       "      <th>RNFLT.761</th>\n",
       "      <th>RNFLT.762</th>\n",
       "      <th>RNFLT.763</th>\n",
       "      <th>RNFLT.764</th>\n",
       "      <th>RNFLT.765</th>\n",
       "      <th>RNFLT.766</th>\n",
       "      <th>RNFLT.767</th>\n",
       "      <th>RNFLT.768</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>60.0</td>\n",
       "      <td>61.0</td>\n",
       "      <td>61.0</td>\n",
       "      <td>62.0</td>\n",
       "      <td>62.0</td>\n",
       "      <td>63.0</td>\n",
       "      <td>63.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>...</td>\n",
       "      <td>53.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>59.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>100.0</td>\n",
       "      <td>104.0</td>\n",
       "      <td>106.0</td>\n",
       "      <td>106.0</td>\n",
       "      <td>104.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>84.0</td>\n",
       "      <td>...</td>\n",
       "      <td>61.0</td>\n",
       "      <td>63.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>79.0</td>\n",
       "      <td>84.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>95.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>35.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>...</td>\n",
       "      <td>30.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>34.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>73.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>76.0</td>\n",
       "      <td>77.0</td>\n",
       "      <td>77.0</td>\n",
       "      <td>...</td>\n",
       "      <td>72.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>73.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>85.0</td>\n",
       "      <td>86.0</td>\n",
       "      <td>87.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>89.0</td>\n",
       "      <td>91.0</td>\n",
       "      <td>93.0</td>\n",
       "      <td>94.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>...</td>\n",
       "      <td>74.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>76.0</td>\n",
       "      <td>77.0</td>\n",
       "      <td>78.0</td>\n",
       "      <td>79.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>81.0</td>\n",
       "      <td>82.0</td>\n",
       "      <td>83.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 768 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    RNFLT.1  RNFLT.2  RNFLT.3  RNFLT.4  RNFLT.5  RNFLT.6  RNFLT.7  RNFLT.8  \\\n",
       "28     60.0     61.0     61.0     62.0     62.0     63.0     63.0     64.0   \n",
       "29    100.0    104.0    106.0    106.0    104.0    101.0     97.0     92.0   \n",
       "71     35.0     36.0     37.0     37.0     38.0     39.0     40.0     40.0   \n",
       "72     73.0     73.0     73.0     74.0     74.0     75.0     75.0     76.0   \n",
       "73     85.0     86.0     87.0     88.0     89.0     91.0     93.0     94.0   \n",
       "\n",
       "    RNFLT.9  RNFLT.10  ...  RNFLT.759  RNFLT.760  RNFLT.761  RNFLT.762  \\\n",
       "28     64.0      65.0  ...       53.0       54.0       54.0       55.0   \n",
       "29     88.0      84.0  ...       61.0       63.0       64.0       67.0   \n",
       "71     41.0      42.0  ...       30.0       30.0       30.0       31.0   \n",
       "72     77.0      77.0  ...       72.0       72.0       72.0       72.0   \n",
       "73     96.0      98.0  ...       74.0       75.0       76.0       77.0   \n",
       "\n",
       "    RNFLT.763  RNFLT.764  RNFLT.765  RNFLT.766  RNFLT.767  RNFLT.768  \n",
       "28       55.0       56.0       57.0       58.0       58.0       59.0  \n",
       "29       70.0       74.0       79.0       84.0       90.0       95.0  \n",
       "71       31.0       32.0       32.0       33.0       34.0       34.0  \n",
       "72       72.0       72.0       73.0       73.0       73.0       73.0  \n",
       "73       78.0       79.0       80.0       81.0       82.0       83.0  \n",
       "\n",
       "[5 rows x 768 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = df.iloc[test_i, 1:769]\n",
    "print(x.isnull().values.sum())\n",
    "print(x.shape)\n",
    "x.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ad799228",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(59, 768, 1)\n"
     ]
    }
   ],
   "source": [
    "x = np.asarray(x)\n",
    "scaled_x = x/381\n",
    "scaled_x = scaled_x.reshape(scaled_x.shape[0],scaled_x.shape[1],1)\n",
    "X_test = scaled_x\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d829c767",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(463, 768, 1)\n"
     ]
    }
   ],
   "source": [
    "x = TrainVal.iloc[train_id, 1:769]\n",
    "x = np.asarray(x)\n",
    "scaled_x = x/381\n",
    "scaled_x = scaled_x.reshape(scaled_x.shape[0],scaled_x.shape[1],1)\n",
    "X_train = scaled_x\n",
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7e04cdf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(53, 768, 1)\n"
     ]
    }
   ],
   "source": [
    "x = TrainVal.iloc[val_id, 1:769]\n",
    "x = np.asarray(x)\n",
    "scaled_x = x/381\n",
    "scaled_x = scaled_x.reshape(scaled_x.shape[0],scaled_x.shape[1],1)\n",
    "X_val = scaled_x\n",
    "print(X_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07e6472d",
   "metadata": {},
   "source": [
    "Reshape the X matrix ONLY FOR CNN models, do not need to reshape for RF or SVM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c831389d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0  1\n",
      "1  0    394\n",
      "0  1    181\n",
      "dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>579</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>580</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>581</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>582</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>583</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>575 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     0  1\n",
       "0    1  0\n",
       "1    0  1\n",
       "2    1  0\n",
       "3    1  0\n",
       "4    1  0\n",
       "..  .. ..\n",
       "579  0  1\n",
       "580  0  1\n",
       "581  0  1\n",
       "582  0  1\n",
       "583  0  1\n",
       "\n",
       "[575 rows x 2 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#one-hot-encoding our label\n",
    "y = pd.get_dummies(y)\n",
    "print(y.value_counts())\n",
    "y #The second column is 'progressor', The first column is 'non-progressor'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3a5a4f0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Non-Progressor</th>\n",
       "      <th>Progressor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>579</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>580</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>581</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>582</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>583</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>575 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Non-Progressor  Progressor\n",
       "0                 1           0\n",
       "1                 0           1\n",
       "2                 1           0\n",
       "3                 1           0\n",
       "4                 1           0\n",
       "..              ...         ...\n",
       "579               0           1\n",
       "580               0           1\n",
       "581               0           1\n",
       "582               0           1\n",
       "583               0           1\n",
       "\n",
       "[575 rows x 2 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = y.rename(columns={0: \"Non-Progressor\", 1: \"Progressor\"})\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "db21658e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Non-Progressor</th>\n",
       "      <th>Progressor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>210</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>211</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>225</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>244</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>254</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>255</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>256</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>257</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>258</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>259</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>265</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>266</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>267</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>268</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>281</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>282</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>320</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>324</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>325</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>328</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>329</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>330</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>331</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>346</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>347</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>384</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>451</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>452</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>453</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>458</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>459</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>465</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>466</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>467</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>468</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>494</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>501</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>520</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>521</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>570</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Non-Progressor  Progressor\n",
       "28                1           0\n",
       "29                1           0\n",
       "71                0           1\n",
       "72                0           1\n",
       "73                0           1\n",
       "96                0           1\n",
       "97                1           0\n",
       "122               1           0\n",
       "123               0           1\n",
       "152               1           0\n",
       "153               1           0\n",
       "154               1           0\n",
       "155               0           1\n",
       "203               1           0\n",
       "204               1           0\n",
       "209               1           0\n",
       "210               1           0\n",
       "211               1           0\n",
       "212               0           1\n",
       "224               0           1\n",
       "225               0           1\n",
       "244               0           1\n",
       "254               0           1\n",
       "255               1           0\n",
       "256               1           0\n",
       "257               0           1\n",
       "258               1           0\n",
       "259               1           0\n",
       "265               1           0\n",
       "266               0           1\n",
       "267               1           0\n",
       "268               1           0\n",
       "281               1           0\n",
       "282               1           0\n",
       "320               1           0\n",
       "324               0           1\n",
       "325               1           0\n",
       "328               1           0\n",
       "329               1           0\n",
       "330               1           0\n",
       "331               1           0\n",
       "346               1           0\n",
       "347               1           0\n",
       "384               1           0\n",
       "451               1           0\n",
       "452               0           1\n",
       "453               0           1\n",
       "458               0           1\n",
       "459               1           0\n",
       "465               0           1\n",
       "466               0           1\n",
       "467               0           1\n",
       "468               1           0\n",
       "494               1           0\n",
       "495               1           0\n",
       "501               1           0\n",
       "520               1           0\n",
       "521               0           1\n",
       "570               1           0"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test = y.iloc[test_i]\n",
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7df6d036",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Non-Progressor</th>\n",
       "      <th>Progressor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>579</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>580</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>581</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>582</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>583</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>463 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Non-Progressor  Progressor\n",
       "0                 1           0\n",
       "1                 0           1\n",
       "2                 1           0\n",
       "3                 1           0\n",
       "4                 1           0\n",
       "..              ...         ...\n",
       "579               0           1\n",
       "580               0           1\n",
       "581               0           1\n",
       "582               0           1\n",
       "583               0           1\n",
       "\n",
       "[463 rows x 2 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train = y.iloc[train_i].iloc[train_id]\n",
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4f81db41",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Non-Progressor</th>\n",
       "      <th>Progressor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>222</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>228</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>229</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>230</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>231</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>283</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>285</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>286</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>292</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>293</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>294</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>305</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>306</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>334</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>335</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>364</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>365</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>370</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>371</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>398</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>438</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>439</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>447</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>448</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>506</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>507</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>542</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>543</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>552</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>553</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>566</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>567</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Non-Progressor  Progressor\n",
       "12                0           1\n",
       "32                1           0\n",
       "33                1           0\n",
       "40                0           1\n",
       "49                0           1\n",
       "50                1           0\n",
       "102               0           1\n",
       "126               1           0\n",
       "127               1           0\n",
       "128               0           1\n",
       "133               0           1\n",
       "134               0           1\n",
       "160               1           0\n",
       "161               1           0\n",
       "162               1           0\n",
       "163               1           0\n",
       "221               1           0\n",
       "222               1           0\n",
       "228               1           0\n",
       "229               1           0\n",
       "230               1           0\n",
       "231               0           1\n",
       "238               0           1\n",
       "239               1           0\n",
       "283               1           0\n",
       "284               0           1\n",
       "285               1           0\n",
       "286               1           0\n",
       "291               1           0\n",
       "292               1           0\n",
       "293               1           0\n",
       "294               0           1\n",
       "305               1           0\n",
       "306               1           0\n",
       "334               0           1\n",
       "335               0           1\n",
       "364               1           0\n",
       "365               1           0\n",
       "370               1           0\n",
       "371               1           0\n",
       "398               1           0\n",
       "438               1           0\n",
       "439               1           0\n",
       "447               0           1\n",
       "448               0           1\n",
       "506               1           0\n",
       "507               0           1\n",
       "542               0           1\n",
       "543               1           0\n",
       "552               1           0\n",
       "553               1           0\n",
       "566               1           0\n",
       "567               0           1"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_val = y.iloc[train_i].iloc[val_id]\n",
    "y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d8f01ab3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(463, 768, 1)\n",
      "(59, 768, 1)\n",
      "(53, 768, 1)\n",
      "Non-Progressor  Progressor\n",
      "1               0             321\n",
      "0               1             142\n",
      "dtype: int64 \n",
      "\n",
      "Non-Progressor  Progressor\n",
      "1               0             35\n",
      "0               1             18\n",
      "dtype: int64 \n",
      "\n",
      "Non-Progressor  Progressor\n",
      "1               0             38\n",
      "0               1             21\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(X_val.shape)\n",
    "print(y_train.value_counts(), '\\n')\n",
    "print(y_val.value_counts(), '\\n')\n",
    "print(y_test.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "b8863196",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv1d_7 (Conv1D)           (None, 766, 64)           256       \n",
      "                                                                 \n",
      " max_pooling1d_7 (MaxPooling  (None, 255, 64)          0         \n",
      " 1D)                                                             \n",
      "                                                                 \n",
      " dropout_7 (Dropout)         (None, 255, 64)           0         \n",
      "                                                                 \n",
      " flatten_7 (Flatten)         (None, 16320)             0         \n",
      "                                                                 \n",
      " dense_21 (Dense)            (None, 64)                1044544   \n",
      "                                                                 \n",
      " dense_22 (Dense)            (None, 16)                1040      \n",
      "                                                                 \n",
      " dense_23 (Dense)            (None, 2)                 34        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,045,874\n",
      "Trainable params: 1,045,874\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#create model1\n",
    "model_1 = Sequential()\n",
    "\n",
    "#add layers\n",
    "model_1.add(Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(768,1)))\n",
    "model_1.add(MaxPooling1D(pool_size=3))\n",
    "# model_1.add(Conv1D(filters=32, kernel_size=3, activation='relu'))\n",
    "model_1.add(Dropout(0.5))\n",
    "# model_1.add(MaxPooling1D(pool_size=2))\n",
    "model_1.add(Flatten())\n",
    "model_1.add(Dense(64, activation='relu'))\n",
    "model_1.add(Dense(16, activation='relu'))\n",
    "model_1.add(Dense(2, activation='softmax'))\n",
    "model_1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "42ab00f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "early_stopping_monitor = EarlyStopping(\n",
    "    monitor='val_accuracy',\n",
    "    min_delta=0,\n",
    "    patience=100,\n",
    "    verbose=0,\n",
    "    mode='auto',\n",
    "    baseline=None,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "opt1 = keras.optimizers.Adam(learning_rate = 1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "c024fecc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "15/15 [==============================] - 1s 37ms/step - loss: 0.6434 - accuracy: 0.6782 - val_loss: 0.6278 - val_accuracy: 0.6604\n",
      "Epoch 2/500\n",
      "15/15 [==============================] - 0s 27ms/step - loss: 0.6111 - accuracy: 0.6933 - val_loss: 0.6216 - val_accuracy: 0.6604\n",
      "Epoch 3/500\n",
      "15/15 [==============================] - 0s 27ms/step - loss: 0.6093 - accuracy: 0.6933 - val_loss: 0.6212 - val_accuracy: 0.6604\n",
      "Epoch 4/500\n",
      "15/15 [==============================] - 0s 24ms/step - loss: 0.6067 - accuracy: 0.6933 - val_loss: 0.6178 - val_accuracy: 0.6604\n",
      "Epoch 5/500\n",
      "15/15 [==============================] - 0s 24ms/step - loss: 0.6027 - accuracy: 0.6933 - val_loss: 0.6165 - val_accuracy: 0.6604\n",
      "Epoch 6/500\n",
      "15/15 [==============================] - 0s 24ms/step - loss: 0.6011 - accuracy: 0.6933 - val_loss: 0.6163 - val_accuracy: 0.6604\n",
      "Epoch 7/500\n",
      "15/15 [==============================] - 0s 28ms/step - loss: 0.5987 - accuracy: 0.6933 - val_loss: 0.6121 - val_accuracy: 0.6604\n",
      "Epoch 8/500\n",
      "15/15 [==============================] - 0s 27ms/step - loss: 0.5960 - accuracy: 0.6933 - val_loss: 0.6149 - val_accuracy: 0.6604\n",
      "Epoch 9/500\n",
      "15/15 [==============================] - 0s 25ms/step - loss: 0.6006 - accuracy: 0.6933 - val_loss: 0.6100 - val_accuracy: 0.6604\n",
      "Epoch 10/500\n",
      "15/15 [==============================] - 0s 26ms/step - loss: 0.5948 - accuracy: 0.6933 - val_loss: 0.6116 - val_accuracy: 0.6604\n",
      "Epoch 11/500\n",
      "15/15 [==============================] - 0s 24ms/step - loss: 0.5943 - accuracy: 0.6933 - val_loss: 0.6056 - val_accuracy: 0.6604\n",
      "Epoch 12/500\n",
      "15/15 [==============================] - 0s 24ms/step - loss: 0.5913 - accuracy: 0.6933 - val_loss: 0.6053 - val_accuracy: 0.6604\n",
      "Epoch 13/500\n",
      "15/15 [==============================] - 0s 22ms/step - loss: 0.5916 - accuracy: 0.6933 - val_loss: 0.6044 - val_accuracy: 0.6604\n",
      "Epoch 14/500\n",
      "15/15 [==============================] - 0s 26ms/step - loss: 0.5918 - accuracy: 0.6933 - val_loss: 0.6038 - val_accuracy: 0.6604\n",
      "Epoch 15/500\n",
      "15/15 [==============================] - 0s 26ms/step - loss: 0.5899 - accuracy: 0.6933 - val_loss: 0.6039 - val_accuracy: 0.6604\n",
      "Epoch 16/500\n",
      "15/15 [==============================] - 0s 27ms/step - loss: 0.5855 - accuracy: 0.6933 - val_loss: 0.6031 - val_accuracy: 0.6604\n",
      "Epoch 17/500\n",
      "15/15 [==============================] - 0s 25ms/step - loss: 0.5855 - accuracy: 0.6933 - val_loss: 0.6019 - val_accuracy: 0.6604\n",
      "Epoch 18/500\n",
      "15/15 [==============================] - 0s 24ms/step - loss: 0.5862 - accuracy: 0.6933 - val_loss: 0.6016 - val_accuracy: 0.6604\n",
      "Epoch 19/500\n",
      "15/15 [==============================] - 0s 27ms/step - loss: 0.5833 - accuracy: 0.6955 - val_loss: 0.5956 - val_accuracy: 0.6604\n",
      "Epoch 20/500\n",
      "15/15 [==============================] - 0s 25ms/step - loss: 0.5771 - accuracy: 0.6976 - val_loss: 0.5942 - val_accuracy: 0.6604\n",
      "Epoch 21/500\n",
      "15/15 [==============================] - 0s 24ms/step - loss: 0.5815 - accuracy: 0.6955 - val_loss: 0.6011 - val_accuracy: 0.6604\n",
      "Epoch 22/500\n",
      "15/15 [==============================] - 0s 24ms/step - loss: 0.5771 - accuracy: 0.6955 - val_loss: 0.5939 - val_accuracy: 0.6604\n",
      "Epoch 23/500\n",
      "15/15 [==============================] - 0s 28ms/step - loss: 0.5751 - accuracy: 0.6976 - val_loss: 0.5931 - val_accuracy: 0.6604\n",
      "Epoch 24/500\n",
      "15/15 [==============================] - 0s 24ms/step - loss: 0.5719 - accuracy: 0.7041 - val_loss: 0.5942 - val_accuracy: 0.6604\n",
      "Epoch 25/500\n",
      "15/15 [==============================] - 0s 26ms/step - loss: 0.5743 - accuracy: 0.6976 - val_loss: 0.5924 - val_accuracy: 0.6604\n",
      "Epoch 26/500\n",
      "15/15 [==============================] - 0s 25ms/step - loss: 0.5794 - accuracy: 0.6998 - val_loss: 0.5920 - val_accuracy: 0.6604\n",
      "Epoch 27/500\n",
      "15/15 [==============================] - 0s 25ms/step - loss: 0.5791 - accuracy: 0.6998 - val_loss: 0.5949 - val_accuracy: 0.6604\n",
      "Epoch 28/500\n",
      "15/15 [==============================] - 0s 27ms/step - loss: 0.5702 - accuracy: 0.7063 - val_loss: 0.5903 - val_accuracy: 0.6604\n",
      "Epoch 29/500\n",
      "15/15 [==============================] - 0s 26ms/step - loss: 0.5714 - accuracy: 0.7041 - val_loss: 0.5953 - val_accuracy: 0.6604\n",
      "Epoch 30/500\n",
      "15/15 [==============================] - 0s 24ms/step - loss: 0.5667 - accuracy: 0.7106 - val_loss: 0.5912 - val_accuracy: 0.6604\n",
      "Epoch 31/500\n",
      "15/15 [==============================] - 0s 26ms/step - loss: 0.5672 - accuracy: 0.7149 - val_loss: 0.5926 - val_accuracy: 0.6604\n",
      "Epoch 32/500\n",
      "15/15 [==============================] - 0s 24ms/step - loss: 0.5635 - accuracy: 0.6976 - val_loss: 0.5951 - val_accuracy: 0.6604\n",
      "Epoch 33/500\n",
      "15/15 [==============================] - 0s 26ms/step - loss: 0.5581 - accuracy: 0.6998 - val_loss: 0.5905 - val_accuracy: 0.6604\n",
      "Epoch 34/500\n",
      "15/15 [==============================] - 0s 26ms/step - loss: 0.5603 - accuracy: 0.7084 - val_loss: 0.5912 - val_accuracy: 0.6604\n",
      "Epoch 35/500\n",
      "15/15 [==============================] - 0s 28ms/step - loss: 0.5589 - accuracy: 0.7019 - val_loss: 0.5951 - val_accuracy: 0.6604\n",
      "Epoch 36/500\n",
      "15/15 [==============================] - 0s 23ms/step - loss: 0.5560 - accuracy: 0.7127 - val_loss: 0.5905 - val_accuracy: 0.6604\n",
      "Epoch 37/500\n",
      "15/15 [==============================] - 0s 25ms/step - loss: 0.5558 - accuracy: 0.7041 - val_loss: 0.5934 - val_accuracy: 0.6604\n",
      "Epoch 38/500\n",
      "15/15 [==============================] - 0s 30ms/step - loss: 0.5553 - accuracy: 0.7300 - val_loss: 0.5928 - val_accuracy: 0.6604\n",
      "Epoch 39/500\n",
      "15/15 [==============================] - 0s 27ms/step - loss: 0.5528 - accuracy: 0.7106 - val_loss: 0.5922 - val_accuracy: 0.6604\n",
      "Epoch 40/500\n",
      "15/15 [==============================] - 0s 26ms/step - loss: 0.5579 - accuracy: 0.7192 - val_loss: 0.5947 - val_accuracy: 0.6604\n",
      "Epoch 41/500\n",
      "15/15 [==============================] - 0s 25ms/step - loss: 0.5462 - accuracy: 0.7149 - val_loss: 0.5938 - val_accuracy: 0.6604\n",
      "Epoch 42/500\n",
      "15/15 [==============================] - 0s 26ms/step - loss: 0.5487 - accuracy: 0.7214 - val_loss: 0.5973 - val_accuracy: 0.6604\n",
      "Epoch 43/500\n",
      "15/15 [==============================] - 0s 23ms/step - loss: 0.5451 - accuracy: 0.7365 - val_loss: 0.5935 - val_accuracy: 0.6415\n",
      "Epoch 44/500\n",
      "15/15 [==============================] - 0s 26ms/step - loss: 0.5380 - accuracy: 0.7495 - val_loss: 0.5936 - val_accuracy: 0.6415\n",
      "Epoch 45/500\n",
      "15/15 [==============================] - 0s 26ms/step - loss: 0.5432 - accuracy: 0.7322 - val_loss: 0.5945 - val_accuracy: 0.6415\n",
      "Epoch 46/500\n",
      "15/15 [==============================] - 0s 26ms/step - loss: 0.5413 - accuracy: 0.7343 - val_loss: 0.5948 - val_accuracy: 0.6415\n",
      "Epoch 47/500\n",
      "15/15 [==============================] - 0s 25ms/step - loss: 0.5426 - accuracy: 0.7581 - val_loss: 0.5970 - val_accuracy: 0.6415\n",
      "Epoch 48/500\n",
      "15/15 [==============================] - 0s 26ms/step - loss: 0.5404 - accuracy: 0.7214 - val_loss: 0.5963 - val_accuracy: 0.6604\n",
      "Epoch 49/500\n",
      "15/15 [==============================] - 0s 23ms/step - loss: 0.5332 - accuracy: 0.7559 - val_loss: 0.5952 - val_accuracy: 0.6415\n",
      "Epoch 50/500\n",
      "15/15 [==============================] - 0s 24ms/step - loss: 0.5267 - accuracy: 0.7430 - val_loss: 0.5952 - val_accuracy: 0.6415\n",
      "Epoch 51/500\n",
      "15/15 [==============================] - 0s 22ms/step - loss: 0.5311 - accuracy: 0.7538 - val_loss: 0.5988 - val_accuracy: 0.6415\n",
      "Epoch 52/500\n",
      "15/15 [==============================] - 0s 25ms/step - loss: 0.5276 - accuracy: 0.7451 - val_loss: 0.5960 - val_accuracy: 0.6415\n",
      "Epoch 53/500\n",
      "15/15 [==============================] - 0s 22ms/step - loss: 0.5214 - accuracy: 0.7667 - val_loss: 0.5974 - val_accuracy: 0.6415\n",
      "Epoch 54/500\n",
      "15/15 [==============================] - 0s 25ms/step - loss: 0.5230 - accuracy: 0.7365 - val_loss: 0.5997 - val_accuracy: 0.6415\n",
      "Epoch 55/500\n",
      "15/15 [==============================] - 0s 22ms/step - loss: 0.5262 - accuracy: 0.7732 - val_loss: 0.6084 - val_accuracy: 0.6415\n",
      "Epoch 56/500\n",
      "15/15 [==============================] - 0s 23ms/step - loss: 0.5199 - accuracy: 0.7365 - val_loss: 0.6015 - val_accuracy: 0.6415\n",
      "Epoch 57/500\n",
      "15/15 [==============================] - 0s 24ms/step - loss: 0.5166 - accuracy: 0.7559 - val_loss: 0.6021 - val_accuracy: 0.6415\n",
      "Epoch 58/500\n",
      "15/15 [==============================] - 0s 22ms/step - loss: 0.5087 - accuracy: 0.7559 - val_loss: 0.6035 - val_accuracy: 0.6415\n",
      "Epoch 59/500\n",
      "15/15 [==============================] - 0s 22ms/step - loss: 0.5119 - accuracy: 0.7732 - val_loss: 0.6007 - val_accuracy: 0.6415\n",
      "Epoch 60/500\n",
      "15/15 [==============================] - 0s 22ms/step - loss: 0.5111 - accuracy: 0.7538 - val_loss: 0.6021 - val_accuracy: 0.6415\n",
      "Epoch 61/500\n",
      "15/15 [==============================] - 0s 25ms/step - loss: 0.5101 - accuracy: 0.7754 - val_loss: 0.6066 - val_accuracy: 0.6415\n",
      "Epoch 62/500\n",
      "15/15 [==============================] - 0s 24ms/step - loss: 0.5098 - accuracy: 0.7732 - val_loss: 0.6014 - val_accuracy: 0.6415\n",
      "Epoch 63/500\n",
      "15/15 [==============================] - 0s 24ms/step - loss: 0.5091 - accuracy: 0.7754 - val_loss: 0.6074 - val_accuracy: 0.6415\n",
      "Epoch 64/500\n",
      "15/15 [==============================] - 0s 22ms/step - loss: 0.5018 - accuracy: 0.7819 - val_loss: 0.6077 - val_accuracy: 0.6415\n",
      "Epoch 65/500\n",
      "15/15 [==============================] - 0s 22ms/step - loss: 0.5023 - accuracy: 0.7819 - val_loss: 0.6099 - val_accuracy: 0.6415\n",
      "Epoch 66/500\n",
      "15/15 [==============================] - 0s 23ms/step - loss: 0.4994 - accuracy: 0.7754 - val_loss: 0.6092 - val_accuracy: 0.6226\n",
      "Epoch 67/500\n",
      "15/15 [==============================] - 0s 22ms/step - loss: 0.4987 - accuracy: 0.7797 - val_loss: 0.6105 - val_accuracy: 0.6415\n",
      "Epoch 68/500\n",
      "15/15 [==============================] - 0s 22ms/step - loss: 0.4957 - accuracy: 0.7689 - val_loss: 0.6119 - val_accuracy: 0.6226\n",
      "Epoch 69/500\n",
      "15/15 [==============================] - 0s 23ms/step - loss: 0.4930 - accuracy: 0.7840 - val_loss: 0.6129 - val_accuracy: 0.6226\n",
      "Epoch 70/500\n",
      "15/15 [==============================] - 0s 22ms/step - loss: 0.4823 - accuracy: 0.7862 - val_loss: 0.6117 - val_accuracy: 0.6415\n",
      "Epoch 71/500\n",
      "15/15 [==============================] - 0s 24ms/step - loss: 0.4921 - accuracy: 0.7883 - val_loss: 0.6195 - val_accuracy: 0.6415\n",
      "Epoch 72/500\n",
      "15/15 [==============================] - 0s 23ms/step - loss: 0.4881 - accuracy: 0.7689 - val_loss: 0.6200 - val_accuracy: 0.6226\n",
      "Epoch 73/500\n",
      "15/15 [==============================] - 0s 24ms/step - loss: 0.4826 - accuracy: 0.7927 - val_loss: 0.6177 - val_accuracy: 0.6415\n",
      "Epoch 74/500\n",
      "15/15 [==============================] - 0s 23ms/step - loss: 0.4768 - accuracy: 0.7927 - val_loss: 0.6189 - val_accuracy: 0.6415\n",
      "Epoch 75/500\n",
      "15/15 [==============================] - 0s 22ms/step - loss: 0.4804 - accuracy: 0.8035 - val_loss: 0.6207 - val_accuracy: 0.6415\n",
      "Epoch 76/500\n",
      "15/15 [==============================] - 0s 23ms/step - loss: 0.4754 - accuracy: 0.7948 - val_loss: 0.6214 - val_accuracy: 0.6415\n",
      "Epoch 77/500\n",
      "15/15 [==============================] - 0s 22ms/step - loss: 0.4808 - accuracy: 0.7927 - val_loss: 0.6256 - val_accuracy: 0.6226\n",
      "Epoch 78/500\n",
      "15/15 [==============================] - 0s 22ms/step - loss: 0.4821 - accuracy: 0.7797 - val_loss: 0.6240 - val_accuracy: 0.6226\n",
      "Epoch 79/500\n",
      "15/15 [==============================] - 0s 23ms/step - loss: 0.4760 - accuracy: 0.7905 - val_loss: 0.6237 - val_accuracy: 0.6415\n",
      "Epoch 80/500\n",
      "15/15 [==============================] - 0s 22ms/step - loss: 0.4784 - accuracy: 0.7991 - val_loss: 0.6254 - val_accuracy: 0.6226\n",
      "Epoch 81/500\n",
      "15/15 [==============================] - 0s 24ms/step - loss: 0.4705 - accuracy: 0.7991 - val_loss: 0.6249 - val_accuracy: 0.6415\n",
      "Epoch 82/500\n",
      "15/15 [==============================] - 0s 21ms/step - loss: 0.4652 - accuracy: 0.7883 - val_loss: 0.6268 - val_accuracy: 0.6226\n",
      "Epoch 83/500\n",
      "15/15 [==============================] - 0s 25ms/step - loss: 0.4604 - accuracy: 0.8099 - val_loss: 0.6261 - val_accuracy: 0.6604\n",
      "Epoch 84/500\n",
      "15/15 [==============================] - 0s 22ms/step - loss: 0.4643 - accuracy: 0.7991 - val_loss: 0.6293 - val_accuracy: 0.6792\n",
      "Epoch 85/500\n",
      "15/15 [==============================] - 0s 25ms/step - loss: 0.4534 - accuracy: 0.8035 - val_loss: 0.6276 - val_accuracy: 0.6604\n",
      "Epoch 86/500\n",
      "15/15 [==============================] - 0s 24ms/step - loss: 0.4601 - accuracy: 0.8143 - val_loss: 0.6317 - val_accuracy: 0.6604\n",
      "Epoch 87/500\n",
      "15/15 [==============================] - 0s 26ms/step - loss: 0.4493 - accuracy: 0.8013 - val_loss: 0.6316 - val_accuracy: 0.6792\n",
      "Epoch 88/500\n",
      "15/15 [==============================] - 0s 21ms/step - loss: 0.4570 - accuracy: 0.8035 - val_loss: 0.6403 - val_accuracy: 0.6415\n",
      "Epoch 89/500\n",
      "15/15 [==============================] - 0s 22ms/step - loss: 0.4590 - accuracy: 0.7840 - val_loss: 0.6402 - val_accuracy: 0.5660\n",
      "Epoch 90/500\n",
      "15/15 [==============================] - 0s 25ms/step - loss: 0.4548 - accuracy: 0.7970 - val_loss: 0.6430 - val_accuracy: 0.6226\n",
      "Epoch 91/500\n",
      "15/15 [==============================] - 0s 24ms/step - loss: 0.4516 - accuracy: 0.8121 - val_loss: 0.6355 - val_accuracy: 0.6604\n",
      "Epoch 92/500\n",
      "15/15 [==============================] - 0s 26ms/step - loss: 0.4394 - accuracy: 0.8229 - val_loss: 0.6386 - val_accuracy: 0.6604\n",
      "Epoch 93/500\n",
      "15/15 [==============================] - 0s 23ms/step - loss: 0.4459 - accuracy: 0.8035 - val_loss: 0.6376 - val_accuracy: 0.6415\n",
      "Epoch 94/500\n",
      "15/15 [==============================] - 0s 22ms/step - loss: 0.4491 - accuracy: 0.8035 - val_loss: 0.6429 - val_accuracy: 0.6792\n",
      "Epoch 95/500\n",
      "15/15 [==============================] - 0s 21ms/step - loss: 0.4429 - accuracy: 0.8121 - val_loss: 0.6385 - val_accuracy: 0.6792\n",
      "Epoch 96/500\n",
      "15/15 [==============================] - 0s 22ms/step - loss: 0.4415 - accuracy: 0.8056 - val_loss: 0.6484 - val_accuracy: 0.6415\n",
      "Epoch 97/500\n",
      "15/15 [==============================] - 0s 23ms/step - loss: 0.4383 - accuracy: 0.8207 - val_loss: 0.6435 - val_accuracy: 0.6604\n",
      "Epoch 98/500\n",
      "15/15 [==============================] - 0s 22ms/step - loss: 0.4333 - accuracy: 0.8207 - val_loss: 0.6470 - val_accuracy: 0.6604\n",
      "Epoch 99/500\n",
      "15/15 [==============================] - 0s 23ms/step - loss: 0.4397 - accuracy: 0.8143 - val_loss: 0.6444 - val_accuracy: 0.6792\n",
      "Epoch 100/500\n",
      "15/15 [==============================] - 0s 22ms/step - loss: 0.4392 - accuracy: 0.8186 - val_loss: 0.6507 - val_accuracy: 0.6604\n",
      "Epoch 101/500\n",
      "15/15 [==============================] - 0s 22ms/step - loss: 0.4291 - accuracy: 0.8337 - val_loss: 0.6453 - val_accuracy: 0.6226\n",
      "Epoch 102/500\n",
      "15/15 [==============================] - 0s 26ms/step - loss: 0.4373 - accuracy: 0.8272 - val_loss: 0.6532 - val_accuracy: 0.6226\n",
      "Epoch 103/500\n",
      "15/15 [==============================] - 0s 22ms/step - loss: 0.4308 - accuracy: 0.8251 - val_loss: 0.6465 - val_accuracy: 0.6604\n",
      "Epoch 104/500\n",
      "15/15 [==============================] - 0s 23ms/step - loss: 0.4349 - accuracy: 0.8251 - val_loss: 0.6574 - val_accuracy: 0.6604\n",
      "Epoch 105/500\n",
      "15/15 [==============================] - 0s 23ms/step - loss: 0.4147 - accuracy: 0.8251 - val_loss: 0.6527 - val_accuracy: 0.6415\n",
      "Epoch 106/500\n",
      "15/15 [==============================] - 0s 23ms/step - loss: 0.4265 - accuracy: 0.8229 - val_loss: 0.6544 - val_accuracy: 0.6415\n",
      "Epoch 107/500\n",
      "15/15 [==============================] - 0s 22ms/step - loss: 0.4163 - accuracy: 0.8380 - val_loss: 0.6573 - val_accuracy: 0.6226\n",
      "Epoch 108/500\n",
      "15/15 [==============================] - 0s 24ms/step - loss: 0.4155 - accuracy: 0.8359 - val_loss: 0.6511 - val_accuracy: 0.6226\n",
      "Epoch 109/500\n",
      "15/15 [==============================] - 0s 23ms/step - loss: 0.4124 - accuracy: 0.8143 - val_loss: 0.6499 - val_accuracy: 0.6415\n",
      "Epoch 110/500\n",
      "15/15 [==============================] - 0s 24ms/step - loss: 0.4077 - accuracy: 0.8121 - val_loss: 0.6543 - val_accuracy: 0.6604\n",
      "Epoch 111/500\n",
      "15/15 [==============================] - 0s 26ms/step - loss: 0.4152 - accuracy: 0.8207 - val_loss: 0.6541 - val_accuracy: 0.6415\n",
      "Epoch 112/500\n",
      "15/15 [==============================] - 0s 25ms/step - loss: 0.4136 - accuracy: 0.8272 - val_loss: 0.6574 - val_accuracy: 0.6415\n",
      "Epoch 113/500\n",
      "15/15 [==============================] - 0s 24ms/step - loss: 0.4058 - accuracy: 0.8402 - val_loss: 0.6656 - val_accuracy: 0.6415\n",
      "Epoch 114/500\n",
      "15/15 [==============================] - 0s 24ms/step - loss: 0.4037 - accuracy: 0.8337 - val_loss: 0.6581 - val_accuracy: 0.6604\n",
      "Epoch 115/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15/15 [==============================] - 0s 22ms/step - loss: 0.4055 - accuracy: 0.8164 - val_loss: 0.6615 - val_accuracy: 0.6226\n",
      "Epoch 116/500\n",
      "15/15 [==============================] - 0s 25ms/step - loss: 0.4000 - accuracy: 0.8359 - val_loss: 0.6597 - val_accuracy: 0.6415\n",
      "Epoch 117/500\n",
      "15/15 [==============================] - 0s 22ms/step - loss: 0.4021 - accuracy: 0.8380 - val_loss: 0.6558 - val_accuracy: 0.6415\n",
      "Epoch 118/500\n",
      "15/15 [==============================] - 0s 23ms/step - loss: 0.4011 - accuracy: 0.8380 - val_loss: 0.6705 - val_accuracy: 0.6415\n",
      "Epoch 119/500\n",
      "15/15 [==============================] - 0s 25ms/step - loss: 0.3958 - accuracy: 0.8272 - val_loss: 0.6701 - val_accuracy: 0.6415\n",
      "Epoch 120/500\n",
      "15/15 [==============================] - 0s 25ms/step - loss: 0.3966 - accuracy: 0.8359 - val_loss: 0.6684 - val_accuracy: 0.6038\n",
      "Epoch 121/500\n",
      "15/15 [==============================] - 0s 24ms/step - loss: 0.3969 - accuracy: 0.8402 - val_loss: 0.6639 - val_accuracy: 0.6415\n",
      "Epoch 122/500\n",
      "15/15 [==============================] - 0s 23ms/step - loss: 0.3857 - accuracy: 0.8337 - val_loss: 0.6708 - val_accuracy: 0.6226\n",
      "Epoch 123/500\n",
      "15/15 [==============================] - 0s 22ms/step - loss: 0.3883 - accuracy: 0.8402 - val_loss: 0.6587 - val_accuracy: 0.6415\n",
      "Epoch 124/500\n",
      "15/15 [==============================] - 0s 25ms/step - loss: 0.3789 - accuracy: 0.8467 - val_loss: 0.6623 - val_accuracy: 0.6415\n",
      "Epoch 125/500\n",
      "15/15 [==============================] - 0s 23ms/step - loss: 0.3847 - accuracy: 0.8423 - val_loss: 0.6633 - val_accuracy: 0.6415\n",
      "Epoch 126/500\n",
      "15/15 [==============================] - 0s 23ms/step - loss: 0.3774 - accuracy: 0.8423 - val_loss: 0.6696 - val_accuracy: 0.6226\n",
      "Epoch 127/500\n",
      "15/15 [==============================] - 0s 24ms/step - loss: 0.3834 - accuracy: 0.8445 - val_loss: 0.6702 - val_accuracy: 0.6604\n",
      "Epoch 128/500\n",
      "15/15 [==============================] - 0s 25ms/step - loss: 0.3796 - accuracy: 0.8531 - val_loss: 0.6723 - val_accuracy: 0.6415\n",
      "Epoch 129/500\n",
      "15/15 [==============================] - 0s 23ms/step - loss: 0.3674 - accuracy: 0.8445 - val_loss: 0.6638 - val_accuracy: 0.6038\n",
      "Epoch 130/500\n",
      "15/15 [==============================] - 0s 22ms/step - loss: 0.3795 - accuracy: 0.8445 - val_loss: 0.6873 - val_accuracy: 0.6038\n",
      "Epoch 131/500\n",
      "15/15 [==============================] - 0s 23ms/step - loss: 0.3714 - accuracy: 0.8359 - val_loss: 0.6912 - val_accuracy: 0.6415\n",
      "Epoch 132/500\n",
      "15/15 [==============================] - 0s 26ms/step - loss: 0.3644 - accuracy: 0.8618 - val_loss: 0.6790 - val_accuracy: 0.6226\n",
      "Epoch 133/500\n",
      "15/15 [==============================] - 0s 24ms/step - loss: 0.3590 - accuracy: 0.8488 - val_loss: 0.6803 - val_accuracy: 0.6226\n",
      "Epoch 134/500\n",
      "15/15 [==============================] - 0s 23ms/step - loss: 0.3775 - accuracy: 0.8402 - val_loss: 0.6801 - val_accuracy: 0.6226\n",
      "Epoch 135/500\n",
      "15/15 [==============================] - 0s 25ms/step - loss: 0.3565 - accuracy: 0.8467 - val_loss: 0.6840 - val_accuracy: 0.6226\n",
      "Epoch 136/500\n",
      "15/15 [==============================] - 0s 26ms/step - loss: 0.3663 - accuracy: 0.8531 - val_loss: 0.6807 - val_accuracy: 0.5849\n",
      "Epoch 137/500\n",
      "15/15 [==============================] - 0s 24ms/step - loss: 0.3647 - accuracy: 0.8639 - val_loss: 0.6918 - val_accuracy: 0.6415\n",
      "Epoch 138/500\n",
      "15/15 [==============================] - 0s 24ms/step - loss: 0.3649 - accuracy: 0.8488 - val_loss: 0.6697 - val_accuracy: 0.5849\n",
      "Epoch 139/500\n",
      "15/15 [==============================] - 0s 25ms/step - loss: 0.3477 - accuracy: 0.8790 - val_loss: 0.6735 - val_accuracy: 0.6226\n",
      "Epoch 140/500\n",
      "15/15 [==============================] - 0s 24ms/step - loss: 0.3459 - accuracy: 0.8639 - val_loss: 0.6769 - val_accuracy: 0.6226\n",
      "Epoch 141/500\n",
      "15/15 [==============================] - 0s 24ms/step - loss: 0.3535 - accuracy: 0.8575 - val_loss: 0.6836 - val_accuracy: 0.6226\n",
      "Epoch 142/500\n",
      "15/15 [==============================] - 0s 26ms/step - loss: 0.3406 - accuracy: 0.8683 - val_loss: 0.6857 - val_accuracy: 0.6226\n",
      "Epoch 143/500\n",
      "15/15 [==============================] - 0s 24ms/step - loss: 0.3541 - accuracy: 0.8683 - val_loss: 0.6922 - val_accuracy: 0.6415\n",
      "Epoch 144/500\n",
      "15/15 [==============================] - 0s 25ms/step - loss: 0.3421 - accuracy: 0.8531 - val_loss: 0.6906 - val_accuracy: 0.6415\n",
      "Epoch 145/500\n",
      "15/15 [==============================] - 0s 26ms/step - loss: 0.3386 - accuracy: 0.8683 - val_loss: 0.6913 - val_accuracy: 0.6226\n",
      "Epoch 146/500\n",
      "15/15 [==============================] - 0s 24ms/step - loss: 0.3518 - accuracy: 0.8531 - val_loss: 0.6896 - val_accuracy: 0.6226\n",
      "Epoch 147/500\n",
      "15/15 [==============================] - 0s 22ms/step - loss: 0.3460 - accuracy: 0.8639 - val_loss: 0.6879 - val_accuracy: 0.6415\n",
      "Epoch 148/500\n",
      "15/15 [==============================] - 0s 25ms/step - loss: 0.3430 - accuracy: 0.8726 - val_loss: 0.6869 - val_accuracy: 0.6415\n",
      "Epoch 149/500\n",
      "15/15 [==============================] - 0s 22ms/step - loss: 0.3361 - accuracy: 0.8747 - val_loss: 0.6755 - val_accuracy: 0.6226\n",
      "Epoch 150/500\n",
      "15/15 [==============================] - 0s 24ms/step - loss: 0.3382 - accuracy: 0.8726 - val_loss: 0.6869 - val_accuracy: 0.6415\n",
      "Epoch 151/500\n",
      "15/15 [==============================] - 0s 23ms/step - loss: 0.3356 - accuracy: 0.8769 - val_loss: 0.7001 - val_accuracy: 0.6415\n",
      "Epoch 152/500\n",
      "15/15 [==============================] - 0s 25ms/step - loss: 0.3295 - accuracy: 0.8618 - val_loss: 0.6899 - val_accuracy: 0.6415\n",
      "Epoch 153/500\n",
      "15/15 [==============================] - 0s 25ms/step - loss: 0.3386 - accuracy: 0.8531 - val_loss: 0.6897 - val_accuracy: 0.6226\n",
      "Epoch 154/500\n",
      "15/15 [==============================] - 0s 24ms/step - loss: 0.3291 - accuracy: 0.8834 - val_loss: 0.6791 - val_accuracy: 0.6415\n",
      "Epoch 155/500\n",
      "15/15 [==============================] - 0s 23ms/step - loss: 0.3180 - accuracy: 0.8812 - val_loss: 0.6756 - val_accuracy: 0.6415\n",
      "Epoch 156/500\n",
      "15/15 [==============================] - 0s 24ms/step - loss: 0.3228 - accuracy: 0.8747 - val_loss: 0.7213 - val_accuracy: 0.6415\n",
      "Epoch 157/500\n",
      "15/15 [==============================] - 0s 26ms/step - loss: 0.3282 - accuracy: 0.8726 - val_loss: 0.6963 - val_accuracy: 0.6604\n",
      "Epoch 158/500\n",
      "15/15 [==============================] - 0s 25ms/step - loss: 0.3303 - accuracy: 0.8683 - val_loss: 0.7039 - val_accuracy: 0.6415\n",
      "Epoch 159/500\n",
      "15/15 [==============================] - 0s 25ms/step - loss: 0.3190 - accuracy: 0.8942 - val_loss: 0.6821 - val_accuracy: 0.6415\n",
      "Epoch 160/500\n",
      "15/15 [==============================] - 0s 25ms/step - loss: 0.3150 - accuracy: 0.8855 - val_loss: 0.6759 - val_accuracy: 0.6038\n",
      "Epoch 161/500\n",
      "15/15 [==============================] - 0s 22ms/step - loss: 0.2998 - accuracy: 0.8877 - val_loss: 0.6779 - val_accuracy: 0.5849\n",
      "Epoch 162/500\n",
      "15/15 [==============================] - 0s 24ms/step - loss: 0.3115 - accuracy: 0.8877 - val_loss: 0.6938 - val_accuracy: 0.6415\n",
      "Epoch 163/500\n",
      "15/15 [==============================] - 0s 25ms/step - loss: 0.3169 - accuracy: 0.8683 - val_loss: 0.7018 - val_accuracy: 0.6415\n",
      "Epoch 164/500\n",
      "15/15 [==============================] - 0s 24ms/step - loss: 0.3123 - accuracy: 0.8704 - val_loss: 0.6859 - val_accuracy: 0.6415\n",
      "Epoch 165/500\n",
      "15/15 [==============================] - 0s 23ms/step - loss: 0.2968 - accuracy: 0.9006 - val_loss: 0.7053 - val_accuracy: 0.6415\n",
      "Epoch 166/500\n",
      "15/15 [==============================] - 0s 25ms/step - loss: 0.2976 - accuracy: 0.9071 - val_loss: 0.6879 - val_accuracy: 0.6604\n",
      "Epoch 167/500\n",
      "15/15 [==============================] - 0s 25ms/step - loss: 0.3029 - accuracy: 0.8963 - val_loss: 0.7026 - val_accuracy: 0.6604\n",
      "Epoch 168/500\n",
      "15/15 [==============================] - 0s 25ms/step - loss: 0.3039 - accuracy: 0.8963 - val_loss: 0.7084 - val_accuracy: 0.6415\n",
      "Epoch 169/500\n",
      "15/15 [==============================] - 0s 24ms/step - loss: 0.3014 - accuracy: 0.8920 - val_loss: 0.6939 - val_accuracy: 0.6038\n",
      "Epoch 170/500\n",
      "15/15 [==============================] - 0s 24ms/step - loss: 0.2936 - accuracy: 0.8963 - val_loss: 0.7013 - val_accuracy: 0.6415\n",
      "Epoch 171/500\n",
      "15/15 [==============================] - 0s 23ms/step - loss: 0.2870 - accuracy: 0.8942 - val_loss: 0.6984 - val_accuracy: 0.6415\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 172/500\n",
      "15/15 [==============================] - 0s 27ms/step - loss: 0.2894 - accuracy: 0.9050 - val_loss: 0.7146 - val_accuracy: 0.6226\n",
      "Epoch 173/500\n",
      "15/15 [==============================] - 0s 26ms/step - loss: 0.2879 - accuracy: 0.8985 - val_loss: 0.6879 - val_accuracy: 0.6415\n",
      "Epoch 174/500\n",
      "15/15 [==============================] - 0s 26ms/step - loss: 0.2842 - accuracy: 0.9028 - val_loss: 0.7058 - val_accuracy: 0.6415\n",
      "Epoch 175/500\n",
      "15/15 [==============================] - 0s 25ms/step - loss: 0.2904 - accuracy: 0.9071 - val_loss: 0.7026 - val_accuracy: 0.6415\n",
      "Epoch 176/500\n",
      "15/15 [==============================] - 0s 26ms/step - loss: 0.2760 - accuracy: 0.9222 - val_loss: 0.7082 - val_accuracy: 0.6604\n",
      "Epoch 177/500\n",
      "15/15 [==============================] - 0s 25ms/step - loss: 0.2747 - accuracy: 0.8963 - val_loss: 0.7003 - val_accuracy: 0.6604\n",
      "Epoch 178/500\n",
      "15/15 [==============================] - 0s 26ms/step - loss: 0.2859 - accuracy: 0.8942 - val_loss: 0.7113 - val_accuracy: 0.6792\n",
      "Epoch 179/500\n",
      "15/15 [==============================] - 0s 26ms/step - loss: 0.2812 - accuracy: 0.9028 - val_loss: 0.7135 - val_accuracy: 0.6415\n",
      "Epoch 180/500\n",
      "15/15 [==============================] - 0s 25ms/step - loss: 0.2750 - accuracy: 0.9050 - val_loss: 0.7053 - val_accuracy: 0.6038\n",
      "Epoch 181/500\n",
      "15/15 [==============================] - 0s 26ms/step - loss: 0.2802 - accuracy: 0.9028 - val_loss: 0.7018 - val_accuracy: 0.6038\n",
      "Epoch 182/500\n",
      "15/15 [==============================] - 0s 27ms/step - loss: 0.2684 - accuracy: 0.9071 - val_loss: 0.7097 - val_accuracy: 0.6604\n",
      "Epoch 183/500\n",
      "15/15 [==============================] - 0s 26ms/step - loss: 0.2618 - accuracy: 0.9006 - val_loss: 0.7066 - val_accuracy: 0.6415\n",
      "Epoch 184/500\n",
      "15/15 [==============================] - 0s 25ms/step - loss: 0.2764 - accuracy: 0.8920 - val_loss: 0.7152 - val_accuracy: 0.6415\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f81100267c0>"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_1.compile(optimizer=opt1, \n",
    "                loss='categorical_crossentropy', \n",
    "                metrics=['accuracy'])\n",
    "#Here we use cross-entropy as the criteria for loss.\n",
    "model_1.fit(X_train, y_train, \n",
    "            validation_data=(X_val, y_val), \n",
    "            epochs=500, verbose=True, \n",
    "            callbacks=[early_stopping_monitor])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "b34d771b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 8ms/step - loss: 0.5927 - accuracy: 0.7458\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.6293 - accuracy: 0.6792\n"
     ]
    }
   ],
   "source": [
    "m1_eval_test = model_1.evaluate(X_test, y_test)\n",
    "m1_eval_val = model_1.evaluate(X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "32be6f13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f81f26b6040> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "roc auc score:  0.7180451127819549\n",
      "average precision score:  0.7061168466336795\n"
     ]
    }
   ],
   "source": [
    "pred = model_1.predict(X_test)\n",
    "roc_value = roc_auc_score(y_test, pred)\n",
    "ap_score = average_precision_score(y_test, pred)\n",
    "print('roc auc score: ', roc_value)\n",
    "print('average precision score: ', ap_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "245e5e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = pred\n",
    "y_c = (y_pred > 0.5).astype(\"int32\")\n",
    "y_test_np = y_test.to_numpy()\n",
    "y_test_np = y_test_np.astype('int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "25c26ad1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 5ms/step - loss: 0.5927 - accuracy: 0.7458\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "cf_matrix = confusion_matrix(y_test_np.argmax(axis=1), y_c.argmax(axis=1)) \n",
    "#cf_matrix = confusion_matrix(y_test_np[:, 1], y_c[:, 1]) \n",
    "\n",
    "ax = sns.heatmap(cf_matrix, annot=True, cmap='Blues')\n",
    "\n",
    "ax.set_title('Seaborn Confusion Matrix with labels\\n\\n');\n",
    "ax.set_xlabel('\\nPredicted Values')\n",
    "ax.set_ylabel('Actual Values ');\n",
    "\n",
    "## Ticket labels - List must be in alphabetical order\n",
    "ax.xaxis.set_ticklabels(['Non-Progressor','Progressor'])\n",
    "ax.yaxis.set_ticklabels(['Non-Progressor','Progressor'])\n",
    "\n",
    "## Display the visualization of the Confusion Matrix.\n",
    "plt.savefig('M1_GRI_test.png')\n",
    "m1_eval_test = model_1.evaluate(X_test, y_test)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba04a204",
   "metadata": {},
   "source": [
    "**For validation set:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "740c2b33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 9ms/step\n",
      "roc auc score:  0.6412698412698412\n",
      "average precision score:  0.5987637903380651\n"
     ]
    }
   ],
   "source": [
    "pred = model_1.predict(X_val)\n",
    "roc_value = roc_auc_score(y_val, pred)\n",
    "ap_score = average_precision_score(y_val, pred)\n",
    "print('roc auc score: ', roc_value)\n",
    "print('average precision score: ', ap_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "4ae992ca",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.65164775, 0.34835228],\n",
       "       [0.41271812, 0.5872818 ],\n",
       "       [0.87526125, 0.12473873],\n",
       "       [0.72000605, 0.27999398],\n",
       "       [0.86457425, 0.1354257 ],\n",
       "       [0.6742969 , 0.32570317],\n",
       "       [0.49222475, 0.5077752 ],\n",
       "       [0.85723984, 0.14276013],\n",
       "       [0.8022537 , 0.19774629],\n",
       "       [0.40801024, 0.5919898 ],\n",
       "       [0.57469654, 0.42530346],\n",
       "       [0.6181532 , 0.3818468 ],\n",
       "       [0.78752077, 0.21247931],\n",
       "       [0.8657124 , 0.13428752],\n",
       "       [0.56509924, 0.4349008 ],\n",
       "       [0.6007261 , 0.3992739 ],\n",
       "       [0.93552244, 0.06447752],\n",
       "       [0.8328599 , 0.16714007],\n",
       "       [0.7071036 , 0.29289633],\n",
       "       [0.8574806 , 0.14251947],\n",
       "       [0.50772214, 0.49227783],\n",
       "       [0.4465445 , 0.55345553],\n",
       "       [0.8560331 , 0.14396694],\n",
       "       [0.85172397, 0.14827603],\n",
       "       [0.3874886 , 0.61251146],\n",
       "       [0.46836576, 0.5316342 ],\n",
       "       [0.67935246, 0.32064754],\n",
       "       [0.9201101 , 0.07988991],\n",
       "       [0.8643532 , 0.13564683],\n",
       "       [0.5796716 , 0.4203284 ],\n",
       "       [0.33382565, 0.66617435],\n",
       "       [0.59296304, 0.40703696],\n",
       "       [0.69794357, 0.30205646],\n",
       "       [0.77173954, 0.22826047],\n",
       "       [0.76706296, 0.23293704],\n",
       "       [0.6451219 , 0.35487813],\n",
       "       [0.5459938 , 0.45400625],\n",
       "       [0.5936649 , 0.40633518],\n",
       "       [0.8895121 , 0.11048794],\n",
       "       [0.5460202 , 0.45397985],\n",
       "       [0.64096195, 0.35903805],\n",
       "       [0.90548027, 0.09451977],\n",
       "       [0.93197054, 0.06802945],\n",
       "       [0.6080144 , 0.39198563],\n",
       "       [0.7046825 , 0.29531744],\n",
       "       [0.8581386 , 0.14186135],\n",
       "       [0.9377522 , 0.0622478 ],\n",
       "       [0.48984256, 0.5101574 ],\n",
       "       [0.3954431 , 0.6045568 ],\n",
       "       [0.8063159 , 0.1936841 ],\n",
       "       [0.7249388 , 0.2750611 ],\n",
       "       [0.89702946, 0.10297048],\n",
       "       [0.6301715 , 0.36982852]], dtype=float32)"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred\n",
    "#No predicted progressors with a high prob in second column ('Progressor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "bc27e140",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = pred\n",
    "y_c = (y_pred > 0.5).astype(\"int32\")\n",
    "y_val_np = y_val.to_numpy()\n",
    "y_val_np = y_val_np.astype('int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "4687afec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 7ms/step - loss: 0.6293 - accuracy: 0.6792\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "cf_matrix = confusion_matrix(y_val_np.argmax(axis=1), y_c.argmax(axis=1)) \n",
    "#cf_matrix = confusion_matrix(y_test_np[:, 1], y_c[:, 1]) \n",
    "\n",
    "ax = sns.heatmap(cf_matrix, annot=True, cmap='Blues')\n",
    "\n",
    "ax.set_title('Seaborn Confusion Matrix with labels (validation set)\\n\\n');\n",
    "ax.set_xlabel('\\nPredicted Values')\n",
    "ax.set_ylabel('Actual Values ');\n",
    "\n",
    "## Ticket labels - List must be in alphabetical order\n",
    "ax.xaxis.set_ticklabels(['Non-Progressor','Progressor'])\n",
    "ax.yaxis.set_ticklabels(['Non-Progressor','Progressor'])\n",
    "\n",
    "## Display the visualization of the Confusion Matrix.\n",
    "plt.savefig('M1_GRI_test.png')\n",
    "m1_eval_test = model_1.evaluate(X_val, y_val)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baebbd1c",
   "metadata": {},
   "source": [
    "**Still we can see the training model cannot provide a good prediction for the real progressor.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8df3f8d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e86b4f6e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcb5e646",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e75e6246",
   "metadata": {},
   "source": [
    "### Idea 2: Using 'RNFLT 1 to 768' as the predictors, 'Y_combined' as the dependent variable, with resampling, CNN method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "b5abc528",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "       0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0,\n",
       "       0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0,\n",
       "       1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "       0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
       "       1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0,\n",
       "       1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1,\n",
       "       0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0,\n",
       "       0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1,\n",
       "       0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1,\n",
       "       0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1,\n",
       "       1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1,\n",
       "       1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1,\n",
       "       1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1,\n",
       "       1], dtype=uint8)"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_progressor = np.array(y_train)[:,1]\n",
    "y_progressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "dd96bcd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "463"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_progressor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "fdcbce1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(463, 768, 1)"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "07350ebf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(463, 768)"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_2d = np.reshape(X_train, (X_train.shape[0], X_train.shape[1]))\n",
    "X_train_2d.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "a6358fcb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(642, 768)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "       0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0,\n",
       "       0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0,\n",
       "       1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "       0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
       "       1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0,\n",
       "       1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1,\n",
       "       0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0,\n",
       "       0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1,\n",
       "       0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1,\n",
       "       0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1,\n",
       "       1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1,\n",
       "       1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1,\n",
       "       1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1], dtype=uint8)"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from imblearn.over_sampling import RandomOverSampler\n",
    "oversample = RandomOverSampler(sampling_strategy = 'minority')\n",
    "X_train_over, y_train_over = oversample.fit_resample(X_train_2d, y_progressor)\n",
    "print(X_train_over.shape)\n",
    "y_train_over"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "84a067e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>637</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>638</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>639</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>640</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>641</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>642 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     0  1\n",
       "0    1  0\n",
       "1    0  1\n",
       "2    1  0\n",
       "3    1  0\n",
       "4    1  0\n",
       "..  .. ..\n",
       "637  0  1\n",
       "638  0  1\n",
       "639  0  1\n",
       "640  0  1\n",
       "641  0  1\n",
       "\n",
       "[642 rows x 2 columns]"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_over = pd.get_dummies(y_train_over)\n",
    "y_train_over"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "01be6471",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non-Progressor  Progressor\n",
      "0               1             321\n",
      "1               0             321\n",
      "dtype: int64 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_train_over=y_train_over.rename(columns={0: \"Non-Progressor\", 1: \"Progressor\"})\n",
    "print(y_train_over.value_counts(), '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "406257d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non-Progressor  Progressor\n",
      "1               0             321\n",
      "0               1             142\n",
      "dtype: int64 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(y_train.value_counts(), '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "cd635496",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Non-Progressor</th>\n",
       "      <th>Progressor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>637</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>638</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>639</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>640</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>641</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>642 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Non-Progressor  Progressor\n",
       "0                 1           0\n",
       "1                 0           1\n",
       "2                 1           0\n",
       "3                 1           0\n",
       "4                 1           0\n",
       "..              ...         ...\n",
       "637               0           1\n",
       "638               0           1\n",
       "639               0           1\n",
       "640               0           1\n",
       "641               0           1\n",
       "\n",
       "[642 rows x 2 columns]"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_over"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "776a36c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(642, 768)"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_over.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "f1628b92",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.12335958],\n",
       "        [0.12335958],\n",
       "        [0.12073491],\n",
       "        ...,\n",
       "        [0.12598425],\n",
       "        [0.12598425],\n",
       "        [0.12335958]],\n",
       "\n",
       "       [[0.18372703],\n",
       "        [0.18635171],\n",
       "        [0.18897638],\n",
       "        ...,\n",
       "        [0.17322835],\n",
       "        [0.17585302],\n",
       "        [0.18110236]],\n",
       "\n",
       "       [[0.11548556],\n",
       "        [0.11811024],\n",
       "        [0.11811024],\n",
       "        ...,\n",
       "        [0.11811024],\n",
       "        [0.11811024],\n",
       "        [0.11548556]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[0.14435696],\n",
       "        [0.14698163],\n",
       "        [0.14698163],\n",
       "        ...,\n",
       "        [0.14698163],\n",
       "        [0.14698163],\n",
       "        [0.14435696]],\n",
       "\n",
       "       [[0.46981627],\n",
       "        [0.46456693],\n",
       "        [0.45931759],\n",
       "        ...,\n",
       "        [0.48293963],\n",
       "        [0.48031496],\n",
       "        [0.47506562]],\n",
       "\n",
       "       [[0.08136483],\n",
       "        [0.08136483],\n",
       "        [0.0839895 ],\n",
       "        ...,\n",
       "        [0.07874016],\n",
       "        [0.07874016],\n",
       "        [0.07874016]]])"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_over = np.reshape(X_train_over, (X_train_over.shape[0], X_train_over.shape[1], 1))\n",
    "X_train_over"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "740fd214",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv1d_8 (Conv1D)           (None, 766, 64)           256       \n",
      "                                                                 \n",
      " max_pooling1d_8 (MaxPooling  (None, 255, 64)          0         \n",
      " 1D)                                                             \n",
      "                                                                 \n",
      " dropout_8 (Dropout)         (None, 255, 64)           0         \n",
      "                                                                 \n",
      " flatten_8 (Flatten)         (None, 16320)             0         \n",
      "                                                                 \n",
      " dense_24 (Dense)            (None, 64)                1044544   \n",
      "                                                                 \n",
      " dense_25 (Dense)            (None, 16)                1040      \n",
      "                                                                 \n",
      " dense_26 (Dense)            (None, 2)                 34        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,045,874\n",
      "Trainable params: 1,045,874\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#create model2\n",
    "model_2 = Sequential()\n",
    "\n",
    "#add layers\n",
    "model_2.add(Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(768,1)))\n",
    "model_2.add(MaxPooling1D(pool_size=3))\n",
    "# model_1.add(Conv1D(filters=32, kernel_size=3, activation='relu'))\n",
    "model_2.add(Dropout(0.5))\n",
    "# model_1.add(MaxPooling1D(pool_size=2))\n",
    "model_2.add(Flatten())\n",
    "model_2.add(Dense(64, activation='relu'))\n",
    "model_2.add(Dense(16, activation='relu'))\n",
    "model_2.add(Dense(2, activation='softmax'))\n",
    "model_2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "43ad051d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "early_stopping_monitor = EarlyStopping(\n",
    "    monitor='val_accuracy',\n",
    "    min_delta=0,\n",
    "    patience=100,\n",
    "    verbose=0,\n",
    "    mode='auto',\n",
    "    baseline=None,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "opt1 = keras.optimizers.Adam(learning_rate = 1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "7bf60ad2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "21/21 [==============================] - 1s 32ms/step - loss: 0.6940 - accuracy: 0.4875 - val_loss: 0.6866 - val_accuracy: 0.6604\n",
      "Epoch 2/500\n",
      "21/21 [==============================] - 1s 25ms/step - loss: 0.6917 - accuracy: 0.5031 - val_loss: 0.6721 - val_accuracy: 0.6604\n",
      "Epoch 3/500\n",
      "21/21 [==============================] - 1s 26ms/step - loss: 0.6912 - accuracy: 0.5000 - val_loss: 0.6756 - val_accuracy: 0.6604\n",
      "Epoch 4/500\n",
      "21/21 [==============================] - 1s 28ms/step - loss: 0.6900 - accuracy: 0.5016 - val_loss: 0.6752 - val_accuracy: 0.6604\n",
      "Epoch 5/500\n",
      "21/21 [==============================] - 1s 25ms/step - loss: 0.6897 - accuracy: 0.5234 - val_loss: 0.6808 - val_accuracy: 0.6604\n",
      "Epoch 6/500\n",
      "21/21 [==============================] - 1s 27ms/step - loss: 0.6895 - accuracy: 0.5888 - val_loss: 0.6734 - val_accuracy: 0.6792\n",
      "Epoch 7/500\n",
      "21/21 [==============================] - 1s 26ms/step - loss: 0.6873 - accuracy: 0.5576 - val_loss: 0.6788 - val_accuracy: 0.6981\n",
      "Epoch 8/500\n",
      "21/21 [==============================] - 1s 26ms/step - loss: 0.6849 - accuracy: 0.6184 - val_loss: 0.6731 - val_accuracy: 0.7170\n",
      "Epoch 9/500\n",
      "21/21 [==============================] - 1s 25ms/step - loss: 0.6862 - accuracy: 0.5156 - val_loss: 0.6527 - val_accuracy: 0.6604\n",
      "Epoch 10/500\n",
      "21/21 [==============================] - 1s 27ms/step - loss: 0.6839 - accuracy: 0.5779 - val_loss: 0.6575 - val_accuracy: 0.6604\n",
      "Epoch 11/500\n",
      "21/21 [==============================] - 1s 26ms/step - loss: 0.6758 - accuracy: 0.5935 - val_loss: 0.6691 - val_accuracy: 0.6792\n",
      "Epoch 12/500\n",
      "21/21 [==============================] - 1s 28ms/step - loss: 0.6759 - accuracy: 0.5857 - val_loss: 0.6572 - val_accuracy: 0.6981\n",
      "Epoch 13/500\n",
      "21/21 [==============================] - 1s 27ms/step - loss: 0.6688 - accuracy: 0.6168 - val_loss: 0.6570 - val_accuracy: 0.6981\n",
      "Epoch 14/500\n",
      "21/21 [==============================] - 1s 26ms/step - loss: 0.6663 - accuracy: 0.6355 - val_loss: 0.6334 - val_accuracy: 0.7358\n",
      "Epoch 15/500\n",
      "21/21 [==============================] - 1s 28ms/step - loss: 0.6626 - accuracy: 0.5919 - val_loss: 0.6415 - val_accuracy: 0.6981\n",
      "Epoch 16/500\n",
      "21/21 [==============================] - 1s 25ms/step - loss: 0.6616 - accuracy: 0.6184 - val_loss: 0.6214 - val_accuracy: 0.7358\n",
      "Epoch 17/500\n",
      "21/21 [==============================] - 1s 25ms/step - loss: 0.6585 - accuracy: 0.6012 - val_loss: 0.6534 - val_accuracy: 0.6981\n",
      "Epoch 18/500\n",
      "21/21 [==============================] - 1s 25ms/step - loss: 0.6566 - accuracy: 0.6433 - val_loss: 0.6181 - val_accuracy: 0.7358\n",
      "Epoch 19/500\n",
      "21/21 [==============================] - 1s 26ms/step - loss: 0.6539 - accuracy: 0.6106 - val_loss: 0.6409 - val_accuracy: 0.7358\n",
      "Epoch 20/500\n",
      "21/21 [==============================] - 1s 26ms/step - loss: 0.6597 - accuracy: 0.6355 - val_loss: 0.6285 - val_accuracy: 0.6792\n",
      "Epoch 21/500\n",
      "21/21 [==============================] - 1s 26ms/step - loss: 0.6506 - accuracy: 0.6184 - val_loss: 0.6377 - val_accuracy: 0.7358\n",
      "Epoch 22/500\n",
      "21/21 [==============================] - 1s 27ms/step - loss: 0.6468 - accuracy: 0.6386 - val_loss: 0.6255 - val_accuracy: 0.6792\n",
      "Epoch 23/500\n",
      "21/21 [==============================] - 1s 26ms/step - loss: 0.6445 - accuracy: 0.6262 - val_loss: 0.6366 - val_accuracy: 0.7170\n",
      "Epoch 24/500\n",
      "21/21 [==============================] - 1s 27ms/step - loss: 0.6415 - accuracy: 0.6293 - val_loss: 0.6308 - val_accuracy: 0.7170\n",
      "Epoch 25/500\n",
      "21/21 [==============================] - 1s 26ms/step - loss: 0.6468 - accuracy: 0.6231 - val_loss: 0.6292 - val_accuracy: 0.7170\n",
      "Epoch 26/500\n",
      "21/21 [==============================] - 1s 25ms/step - loss: 0.6451 - accuracy: 0.6511 - val_loss: 0.6206 - val_accuracy: 0.6792\n",
      "Epoch 27/500\n",
      "21/21 [==============================] - 1s 28ms/step - loss: 0.6387 - accuracy: 0.6246 - val_loss: 0.6245 - val_accuracy: 0.7170\n",
      "Epoch 28/500\n",
      "21/21 [==============================] - 1s 26ms/step - loss: 0.6353 - accuracy: 0.6495 - val_loss: 0.6269 - val_accuracy: 0.7170\n",
      "Epoch 29/500\n",
      "21/21 [==============================] - 1s 26ms/step - loss: 0.6317 - accuracy: 0.6480 - val_loss: 0.6172 - val_accuracy: 0.6981\n",
      "Epoch 30/500\n",
      "21/21 [==============================] - 1s 26ms/step - loss: 0.6288 - accuracy: 0.6573 - val_loss: 0.6206 - val_accuracy: 0.7358\n",
      "Epoch 31/500\n",
      "21/21 [==============================] - 1s 25ms/step - loss: 0.6266 - accuracy: 0.6464 - val_loss: 0.6134 - val_accuracy: 0.7170\n",
      "Epoch 32/500\n",
      "21/21 [==============================] - 1s 26ms/step - loss: 0.6262 - accuracy: 0.6371 - val_loss: 0.6133 - val_accuracy: 0.7170\n",
      "Epoch 33/500\n",
      "21/21 [==============================] - 1s 27ms/step - loss: 0.6223 - accuracy: 0.6449 - val_loss: 0.6237 - val_accuracy: 0.7170\n",
      "Epoch 34/500\n",
      "21/21 [==============================] - 1s 27ms/step - loss: 0.6187 - accuracy: 0.6604 - val_loss: 0.6119 - val_accuracy: 0.7170\n",
      "Epoch 35/500\n",
      "21/21 [==============================] - 1s 28ms/step - loss: 0.6175 - accuracy: 0.6402 - val_loss: 0.6200 - val_accuracy: 0.7358\n",
      "Epoch 36/500\n",
      "21/21 [==============================] - 1s 29ms/step - loss: 0.6143 - accuracy: 0.6558 - val_loss: 0.6232 - val_accuracy: 0.6981\n",
      "Epoch 37/500\n",
      "21/21 [==============================] - 1s 27ms/step - loss: 0.6132 - accuracy: 0.6449 - val_loss: 0.6181 - val_accuracy: 0.6981\n",
      "Epoch 38/500\n",
      "21/21 [==============================] - 1s 25ms/step - loss: 0.6112 - accuracy: 0.6885 - val_loss: 0.6238 - val_accuracy: 0.6792\n",
      "Epoch 39/500\n",
      "21/21 [==============================] - 1s 25ms/step - loss: 0.6073 - accuracy: 0.6558 - val_loss: 0.6409 - val_accuracy: 0.6792\n",
      "Epoch 40/500\n",
      "21/21 [==============================] - 1s 27ms/step - loss: 0.6055 - accuracy: 0.6854 - val_loss: 0.6224 - val_accuracy: 0.6792\n",
      "Epoch 41/500\n",
      "21/21 [==============================] - 1s 26ms/step - loss: 0.6097 - accuracy: 0.6558 - val_loss: 0.6524 - val_accuracy: 0.6604\n",
      "Epoch 42/500\n",
      "21/21 [==============================] - 1s 28ms/step - loss: 0.6006 - accuracy: 0.6807 - val_loss: 0.6427 - val_accuracy: 0.6604\n",
      "Epoch 43/500\n",
      "21/21 [==============================] - 0s 24ms/step - loss: 0.5968 - accuracy: 0.6838 - val_loss: 0.6163 - val_accuracy: 0.7358\n",
      "Epoch 44/500\n",
      "21/21 [==============================] - 1s 27ms/step - loss: 0.5959 - accuracy: 0.6729 - val_loss: 0.6381 - val_accuracy: 0.6604\n",
      "Epoch 45/500\n",
      "21/21 [==============================] - 1s 28ms/step - loss: 0.5993 - accuracy: 0.6776 - val_loss: 0.6528 - val_accuracy: 0.6604\n",
      "Epoch 46/500\n",
      "21/21 [==============================] - 1s 26ms/step - loss: 0.5895 - accuracy: 0.7009 - val_loss: 0.6277 - val_accuracy: 0.6792\n",
      "Epoch 47/500\n",
      "21/21 [==============================] - 1s 26ms/step - loss: 0.5837 - accuracy: 0.6947 - val_loss: 0.6484 - val_accuracy: 0.6792\n",
      "Epoch 48/500\n",
      "21/21 [==============================] - 1s 27ms/step - loss: 0.5831 - accuracy: 0.6947 - val_loss: 0.6326 - val_accuracy: 0.6792\n",
      "Epoch 49/500\n",
      "21/21 [==============================] - 1s 25ms/step - loss: 0.5852 - accuracy: 0.6885 - val_loss: 0.6123 - val_accuracy: 0.7170\n",
      "Epoch 50/500\n",
      "21/21 [==============================] - 1s 28ms/step - loss: 0.5817 - accuracy: 0.6822 - val_loss: 0.6435 - val_accuracy: 0.6415\n",
      "Epoch 51/500\n",
      "21/21 [==============================] - 1s 29ms/step - loss: 0.5819 - accuracy: 0.6994 - val_loss: 0.6324 - val_accuracy: 0.6604\n",
      "Epoch 52/500\n",
      "21/21 [==============================] - 1s 26ms/step - loss: 0.5726 - accuracy: 0.6994 - val_loss: 0.6387 - val_accuracy: 0.6604\n",
      "Epoch 53/500\n",
      "21/21 [==============================] - 1s 26ms/step - loss: 0.5665 - accuracy: 0.7352 - val_loss: 0.6331 - val_accuracy: 0.6604\n",
      "Epoch 54/500\n",
      "21/21 [==============================] - 1s 27ms/step - loss: 0.5701 - accuracy: 0.7040 - val_loss: 0.6534 - val_accuracy: 0.6226\n",
      "Epoch 55/500\n",
      "21/21 [==============================] - 1s 27ms/step - loss: 0.5635 - accuracy: 0.7212 - val_loss: 0.6345 - val_accuracy: 0.6981\n",
      "Epoch 56/500\n",
      "21/21 [==============================] - 1s 26ms/step - loss: 0.5620 - accuracy: 0.7181 - val_loss: 0.6409 - val_accuracy: 0.6604\n",
      "Epoch 57/500\n",
      "21/21 [==============================] - 1s 24ms/step - loss: 0.5565 - accuracy: 0.7227 - val_loss: 0.6348 - val_accuracy: 0.6415\n",
      "Epoch 58/500\n",
      "21/21 [==============================] - 1s 27ms/step - loss: 0.5549 - accuracy: 0.7165 - val_loss: 0.6438 - val_accuracy: 0.6415\n",
      "Epoch 59/500\n",
      "21/21 [==============================] - 1s 29ms/step - loss: 0.5482 - accuracy: 0.7352 - val_loss: 0.6554 - val_accuracy: 0.6415\n",
      "Epoch 60/500\n",
      "21/21 [==============================] - 1s 26ms/step - loss: 0.5507 - accuracy: 0.7259 - val_loss: 0.6307 - val_accuracy: 0.6792\n",
      "Epoch 61/500\n",
      "21/21 [==============================] - 1s 27ms/step - loss: 0.5416 - accuracy: 0.7305 - val_loss: 0.6588 - val_accuracy: 0.6226\n",
      "Epoch 62/500\n",
      "21/21 [==============================] - 1s 27ms/step - loss: 0.5436 - accuracy: 0.7212 - val_loss: 0.6300 - val_accuracy: 0.6604\n",
      "Epoch 63/500\n",
      "21/21 [==============================] - 1s 28ms/step - loss: 0.5419 - accuracy: 0.7321 - val_loss: 0.6528 - val_accuracy: 0.6226\n",
      "Epoch 64/500\n",
      "21/21 [==============================] - 1s 26ms/step - loss: 0.5345 - accuracy: 0.7461 - val_loss: 0.6548 - val_accuracy: 0.6415\n",
      "Epoch 65/500\n",
      "21/21 [==============================] - 1s 25ms/step - loss: 0.5361 - accuracy: 0.7368 - val_loss: 0.6755 - val_accuracy: 0.6226\n",
      "Epoch 66/500\n",
      "21/21 [==============================] - 1s 28ms/step - loss: 0.5311 - accuracy: 0.7586 - val_loss: 0.6545 - val_accuracy: 0.6604\n",
      "Epoch 67/500\n",
      "21/21 [==============================] - 1s 25ms/step - loss: 0.5355 - accuracy: 0.7368 - val_loss: 0.6539 - val_accuracy: 0.6604\n",
      "Epoch 68/500\n",
      "21/21 [==============================] - 1s 27ms/step - loss: 0.5249 - accuracy: 0.7414 - val_loss: 0.7021 - val_accuracy: 0.6038\n",
      "Epoch 69/500\n",
      "21/21 [==============================] - 1s 28ms/step - loss: 0.5219 - accuracy: 0.7461 - val_loss: 0.6425 - val_accuracy: 0.6415\n",
      "Epoch 70/500\n",
      "21/21 [==============================] - 1s 29ms/step - loss: 0.5157 - accuracy: 0.7508 - val_loss: 0.7127 - val_accuracy: 0.5849\n",
      "Epoch 71/500\n",
      "21/21 [==============================] - 1s 27ms/step - loss: 0.5152 - accuracy: 0.7430 - val_loss: 0.6533 - val_accuracy: 0.6415\n",
      "Epoch 72/500\n",
      "21/21 [==============================] - 1s 27ms/step - loss: 0.5161 - accuracy: 0.7523 - val_loss: 0.6900 - val_accuracy: 0.6415\n",
      "Epoch 73/500\n",
      "21/21 [==============================] - 1s 25ms/step - loss: 0.5072 - accuracy: 0.7601 - val_loss: 0.6994 - val_accuracy: 0.6604\n",
      "Epoch 74/500\n",
      "21/21 [==============================] - 1s 27ms/step - loss: 0.5071 - accuracy: 0.7477 - val_loss: 0.6642 - val_accuracy: 0.6226\n",
      "Epoch 75/500\n",
      "21/21 [==============================] - 1s 26ms/step - loss: 0.5015 - accuracy: 0.7664 - val_loss: 0.7124 - val_accuracy: 0.6226\n",
      "Epoch 76/500\n",
      "21/21 [==============================] - 1s 26ms/step - loss: 0.5001 - accuracy: 0.7710 - val_loss: 0.6820 - val_accuracy: 0.6604\n",
      "Epoch 77/500\n",
      "21/21 [==============================] - 1s 25ms/step - loss: 0.5042 - accuracy: 0.7617 - val_loss: 0.6601 - val_accuracy: 0.6038\n",
      "Epoch 78/500\n",
      "21/21 [==============================] - 1s 26ms/step - loss: 0.4997 - accuracy: 0.7757 - val_loss: 0.6874 - val_accuracy: 0.6415\n",
      "Epoch 79/500\n",
      "21/21 [==============================] - 0s 24ms/step - loss: 0.4900 - accuracy: 0.7897 - val_loss: 0.6947 - val_accuracy: 0.6415\n",
      "Epoch 80/500\n",
      "21/21 [==============================] - 1s 24ms/step - loss: 0.4874 - accuracy: 0.7741 - val_loss: 0.6789 - val_accuracy: 0.6604\n",
      "Epoch 81/500\n",
      "21/21 [==============================] - 1s 26ms/step - loss: 0.4838 - accuracy: 0.7788 - val_loss: 0.6749 - val_accuracy: 0.6038\n",
      "Epoch 82/500\n",
      "21/21 [==============================] - 1s 24ms/step - loss: 0.4790 - accuracy: 0.7944 - val_loss: 0.7066 - val_accuracy: 0.6226\n",
      "Epoch 83/500\n",
      "21/21 [==============================] - 0s 23ms/step - loss: 0.4766 - accuracy: 0.7975 - val_loss: 0.7104 - val_accuracy: 0.6226\n",
      "Epoch 84/500\n",
      "21/21 [==============================] - 1s 27ms/step - loss: 0.4724 - accuracy: 0.7991 - val_loss: 0.6975 - val_accuracy: 0.6415\n",
      "Epoch 85/500\n",
      "21/21 [==============================] - 1s 25ms/step - loss: 0.4790 - accuracy: 0.7835 - val_loss: 0.6954 - val_accuracy: 0.6226\n",
      "Epoch 86/500\n",
      "21/21 [==============================] - 0s 24ms/step - loss: 0.4634 - accuracy: 0.7960 - val_loss: 0.7394 - val_accuracy: 0.6226\n",
      "Epoch 87/500\n",
      "21/21 [==============================] - 0s 24ms/step - loss: 0.4667 - accuracy: 0.7882 - val_loss: 0.7124 - val_accuracy: 0.6226\n",
      "Epoch 88/500\n",
      "21/21 [==============================] - 1s 27ms/step - loss: 0.4619 - accuracy: 0.8146 - val_loss: 0.6988 - val_accuracy: 0.6038\n",
      "Epoch 89/500\n",
      "21/21 [==============================] - 1s 26ms/step - loss: 0.4615 - accuracy: 0.7975 - val_loss: 0.7674 - val_accuracy: 0.5660\n",
      "Epoch 90/500\n",
      "21/21 [==============================] - 0s 24ms/step - loss: 0.4567 - accuracy: 0.7991 - val_loss: 0.7055 - val_accuracy: 0.6226\n",
      "Epoch 91/500\n",
      "21/21 [==============================] - 1s 26ms/step - loss: 0.4515 - accuracy: 0.8084 - val_loss: 0.7089 - val_accuracy: 0.5660\n",
      "Epoch 92/500\n",
      "21/21 [==============================] - 1s 28ms/step - loss: 0.4667 - accuracy: 0.7928 - val_loss: 0.7695 - val_accuracy: 0.5849\n",
      "Epoch 93/500\n",
      "21/21 [==============================] - 1s 26ms/step - loss: 0.4497 - accuracy: 0.8162 - val_loss: 0.7557 - val_accuracy: 0.6226\n",
      "Epoch 94/500\n",
      "21/21 [==============================] - 1s 25ms/step - loss: 0.4652 - accuracy: 0.7897 - val_loss: 0.7303 - val_accuracy: 0.5660\n",
      "Epoch 95/500\n",
      "21/21 [==============================] - 1s 28ms/step - loss: 0.4465 - accuracy: 0.7975 - val_loss: 0.7140 - val_accuracy: 0.5472\n",
      "Epoch 96/500\n",
      "21/21 [==============================] - 1s 25ms/step - loss: 0.4449 - accuracy: 0.8146 - val_loss: 0.7171 - val_accuracy: 0.5660\n",
      "Epoch 97/500\n",
      "21/21 [==============================] - 1s 25ms/step - loss: 0.4452 - accuracy: 0.8115 - val_loss: 0.7439 - val_accuracy: 0.6038\n",
      "Epoch 98/500\n",
      "21/21 [==============================] - 1s 24ms/step - loss: 0.4324 - accuracy: 0.8084 - val_loss: 0.7239 - val_accuracy: 0.5849\n",
      "Epoch 99/500\n",
      "21/21 [==============================] - 1s 25ms/step - loss: 0.4291 - accuracy: 0.8224 - val_loss: 0.7147 - val_accuracy: 0.5849\n",
      "Epoch 100/500\n",
      "21/21 [==============================] - 1s 25ms/step - loss: 0.4301 - accuracy: 0.8162 - val_loss: 0.7558 - val_accuracy: 0.6038\n",
      "Epoch 101/500\n",
      "21/21 [==============================] - 1s 25ms/step - loss: 0.4169 - accuracy: 0.8333 - val_loss: 0.7366 - val_accuracy: 0.5849\n",
      "Epoch 102/500\n",
      "21/21 [==============================] - 1s 24ms/step - loss: 0.4149 - accuracy: 0.8364 - val_loss: 0.7541 - val_accuracy: 0.5849\n",
      "Epoch 103/500\n",
      "21/21 [==============================] - 1s 24ms/step - loss: 0.4055 - accuracy: 0.8349 - val_loss: 0.7546 - val_accuracy: 0.5849\n",
      "Epoch 104/500\n",
      "21/21 [==============================] - 1s 28ms/step - loss: 0.4034 - accuracy: 0.8396 - val_loss: 0.7503 - val_accuracy: 0.5849\n",
      "Epoch 105/500\n",
      "21/21 [==============================] - 1s 26ms/step - loss: 0.4188 - accuracy: 0.8396 - val_loss: 0.7586 - val_accuracy: 0.5849\n",
      "Epoch 106/500\n",
      "21/21 [==============================] - 1s 27ms/step - loss: 0.4084 - accuracy: 0.8318 - val_loss: 0.7392 - val_accuracy: 0.5283\n",
      "Epoch 107/500\n",
      "21/21 [==============================] - 1s 24ms/step - loss: 0.4051 - accuracy: 0.8146 - val_loss: 0.7566 - val_accuracy: 0.5849\n",
      "Epoch 108/500\n",
      "21/21 [==============================] - 1s 25ms/step - loss: 0.3976 - accuracy: 0.8442 - val_loss: 0.7622 - val_accuracy: 0.5849\n",
      "Epoch 109/500\n",
      "21/21 [==============================] - 1s 25ms/step - loss: 0.4044 - accuracy: 0.8427 - val_loss: 0.7544 - val_accuracy: 0.5472\n",
      "Epoch 110/500\n",
      "21/21 [==============================] - 1s 26ms/step - loss: 0.3985 - accuracy: 0.8458 - val_loss: 0.7668 - val_accuracy: 0.5283\n",
      "Epoch 111/500\n",
      "21/21 [==============================] - 0s 23ms/step - loss: 0.3906 - accuracy: 0.8551 - val_loss: 0.7514 - val_accuracy: 0.5283\n",
      "Epoch 112/500\n",
      "21/21 [==============================] - 1s 26ms/step - loss: 0.3986 - accuracy: 0.8442 - val_loss: 0.7710 - val_accuracy: 0.5094\n",
      "Epoch 113/500\n",
      "21/21 [==============================] - 1s 25ms/step - loss: 0.3872 - accuracy: 0.8364 - val_loss: 0.7836 - val_accuracy: 0.5094\n",
      "Epoch 114/500\n",
      "21/21 [==============================] - 0s 23ms/step - loss: 0.3870 - accuracy: 0.8520 - val_loss: 0.7963 - val_accuracy: 0.5472\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f8120eec670>"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_2.compile(optimizer=opt1, \n",
    "                loss='categorical_crossentropy', \n",
    "                metrics=['accuracy'])\n",
    "#Here we use cross-entropy as the criteria for loss.\n",
    "model_2.fit(X_train_over, y_train_over, \n",
    "            validation_data=(X_val, y_val), \n",
    "            epochs=500, verbose=True, \n",
    "            callbacks=[early_stopping_monitor])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "a7121c70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 7ms/step - loss: 0.6296 - accuracy: 0.7119\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.6334 - accuracy: 0.7358\n"
     ]
    }
   ],
   "source": [
    "m2_eval_test = model_2.evaluate(X_test, y_test)\n",
    "m2_eval_val = model_2.evaluate(X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "a944c3d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 12ms/step\n",
      "roc auc score:  0.7418546365914787\n",
      "average precision score:  0.7452541555026557\n"
     ]
    }
   ],
   "source": [
    "pred = model_2.predict(X_test)\n",
    "roc_value = roc_auc_score(y_test, pred)\n",
    "ap_score = average_precision_score(y_test, pred)\n",
    "print('roc auc score: ', roc_value)\n",
    "print('average precision score: ', ap_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "2712c38c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.63769215, 0.3623079 ],\n",
       "       [0.61531943, 0.38468057],\n",
       "       [0.45191416, 0.54808587],\n",
       "       [0.47715497, 0.5228451 ],\n",
       "       [0.5386943 , 0.46130568],\n",
       "       [0.42581838, 0.5741817 ],\n",
       "       [0.46023387, 0.53976613],\n",
       "       [0.5591939 , 0.44080612],\n",
       "       [0.45377207, 0.54622793],\n",
       "       [0.61740714, 0.3825929 ],\n",
       "       [0.5756217 , 0.4243783 ],\n",
       "       [0.5253378 , 0.47466215],\n",
       "       [0.46313027, 0.53686976],\n",
       "       [0.56586444, 0.43413553],\n",
       "       [0.5519507 , 0.44804937],\n",
       "       [0.65226966, 0.34773028],\n",
       "       [0.6507214 , 0.34927866],\n",
       "       [0.5483357 , 0.45166433],\n",
       "       [0.5034891 , 0.4965109 ],\n",
       "       [0.60293835, 0.39706168],\n",
       "       [0.6268571 , 0.3731429 ],\n",
       "       [0.4906781 , 0.5093218 ],\n",
       "       [0.47852984, 0.5214701 ],\n",
       "       [0.5101047 , 0.48989522],\n",
       "       [0.6149709 , 0.3850291 ],\n",
       "       [0.56360155, 0.4363985 ],\n",
       "       [0.56548727, 0.43451267],\n",
       "       [0.52956575, 0.47043425],\n",
       "       [0.64718634, 0.35281366],\n",
       "       [0.5306633 , 0.46933672],\n",
       "       [0.6196213 , 0.38037875],\n",
       "       [0.63537574, 0.3646242 ],\n",
       "       [0.4644603 , 0.5355397 ],\n",
       "       [0.46009612, 0.5399038 ],\n",
       "       [0.6350331 , 0.3649669 ],\n",
       "       [0.42378584, 0.57621413],\n",
       "       [0.4896292 , 0.5103708 ],\n",
       "       [0.59907633, 0.40092367],\n",
       "       [0.60337317, 0.39662686],\n",
       "       [0.57498354, 0.42501646],\n",
       "       [0.56276274, 0.4372372 ],\n",
       "       [0.50536513, 0.49463487],\n",
       "       [0.5276257 , 0.47237432],\n",
       "       [0.50734776, 0.49265218],\n",
       "       [0.6045907 , 0.3954093 ],\n",
       "       [0.42104477, 0.57895523],\n",
       "       [0.42375705, 0.576243  ],\n",
       "       [0.48238567, 0.5176143 ],\n",
       "       [0.43955514, 0.5604449 ],\n",
       "       [0.5332629 , 0.4667371 ],\n",
       "       [0.58211017, 0.41788983],\n",
       "       [0.5422136 , 0.45778638],\n",
       "       [0.5240023 , 0.47599772],\n",
       "       [0.62227786, 0.37772208],\n",
       "       [0.5858725 , 0.41412744],\n",
       "       [0.47059414, 0.52940583],\n",
       "       [0.465028  , 0.5349721 ],\n",
       "       [0.42757806, 0.5724219 ],\n",
       "       [0.42432812, 0.5756719 ]], dtype=float32)"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "f81f2ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = pred\n",
    "y_c = (y_pred > 0.5).astype(\"int32\")\n",
    "y_test_np = y_test.to_numpy()\n",
    "y_test_np = y_test_np.astype('int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "937047d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 8ms/step - loss: 0.6296 - accuracy: 0.7119\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "cf_matrix = confusion_matrix(y_test_np.argmax(axis=1), y_c.argmax(axis=1)) \n",
    "#cf_matrix = confusion_matrix(y_test_np[:, 1], y_c[:, 1]) \n",
    "\n",
    "ax = sns.heatmap(cf_matrix, annot=True, cmap='Blues')\n",
    "\n",
    "ax.set_title('Seaborn Confusion Matrix with labels\\n\\n');\n",
    "ax.set_xlabel('\\nPredicted Values')\n",
    "ax.set_ylabel('Actual Values ');\n",
    "\n",
    "## Ticket labels - List must be in alphabetical order\n",
    "ax.xaxis.set_ticklabels(['Non-Progressor','Progressor'])\n",
    "ax.yaxis.set_ticklabels(['Non-Progressor','Progressor'])\n",
    "\n",
    "## Display the visualization of the Confusion Matrix.\n",
    "plt.savefig('M1_GRI_test.png')\n",
    "m2_eval_test = model_2.evaluate(X_test, y_test)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa6e4ce0",
   "metadata": {},
   "source": [
    "**Run 100, 200, 500 rounds (patience = 100, 200, 500)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "daf7cf4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_9\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv1d_9 (Conv1D)           (None, 766, 64)           256       \n",
      "                                                                 \n",
      " max_pooling1d_9 (MaxPooling  (None, 255, 64)          0         \n",
      " 1D)                                                             \n",
      "                                                                 \n",
      " dropout_9 (Dropout)         (None, 255, 64)           0         \n",
      "                                                                 \n",
      " flatten_9 (Flatten)         (None, 16320)             0         \n",
      "                                                                 \n",
      " dense_27 (Dense)            (None, 64)                1044544   \n",
      "                                                                 \n",
      " dense_28 (Dense)            (None, 16)                1040      \n",
      "                                                                 \n",
      " dense_29 (Dense)            (None, 2)                 34        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,045,874\n",
      "Trainable params: 1,045,874\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#create model2_nocall\n",
    "model_2_100 = Sequential()\n",
    "\n",
    "#add layers\n",
    "model_2_100.add(Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(768,1)))\n",
    "model_2_100.add(MaxPooling1D(pool_size=3))\n",
    "# model_1.add(Conv1D(filters=32, kernel_size=3, activation='relu'))\n",
    "model_2_100.add(Dropout(0.5))\n",
    "# model_1.add(MaxPooling1D(pool_size=2))\n",
    "model_2_100.add(Flatten())\n",
    "model_2_100.add(Dense(64, activation='relu'))\n",
    "model_2_100.add(Dense(16, activation='relu'))\n",
    "model_2_100.add(Dense(2, activation='softmax'))\n",
    "model_2_100.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "a304f0bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "early_stopping_monitor = EarlyStopping(\n",
    "    monitor='val_accuracy',\n",
    "    min_delta=0,\n",
    "    patience=100,\n",
    "    verbose=0,\n",
    "    mode='auto',\n",
    "    baseline=None,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "opt1 = keras.optimizers.Adam(learning_rate = 1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "cbd51148",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "21/21 [==============================] - 1s 32ms/step - loss: 0.6934 - accuracy: 0.5016 - val_loss: 0.6856 - val_accuracy: 0.6604\n",
      "Epoch 2/500\n",
      "21/21 [==============================] - 1s 26ms/step - loss: 0.6914 - accuracy: 0.4969 - val_loss: 0.6721 - val_accuracy: 0.6604\n",
      "Epoch 3/500\n",
      "21/21 [==============================] - 1s 25ms/step - loss: 0.6885 - accuracy: 0.4969 - val_loss: 0.6645 - val_accuracy: 0.6604\n",
      "Epoch 4/500\n",
      "21/21 [==============================] - 1s 27ms/step - loss: 0.6857 - accuracy: 0.4969 - val_loss: 0.6701 - val_accuracy: 0.7170\n",
      "Epoch 5/500\n",
      "21/21 [==============================] - 1s 25ms/step - loss: 0.6841 - accuracy: 0.5498 - val_loss: 0.6627 - val_accuracy: 0.7170\n",
      "Epoch 6/500\n",
      "21/21 [==============================] - 1s 26ms/step - loss: 0.6796 - accuracy: 0.5639 - val_loss: 0.6629 - val_accuracy: 0.7358\n",
      "Epoch 7/500\n",
      "21/21 [==============================] - 1s 28ms/step - loss: 0.6765 - accuracy: 0.5966 - val_loss: 0.6547 - val_accuracy: 0.7358\n",
      "Epoch 8/500\n",
      "21/21 [==============================] - 1s 27ms/step - loss: 0.6739 - accuracy: 0.6386 - val_loss: 0.6467 - val_accuracy: 0.7358\n",
      "Epoch 9/500\n",
      "21/21 [==============================] - 1s 26ms/step - loss: 0.6700 - accuracy: 0.5794 - val_loss: 0.6433 - val_accuracy: 0.7358\n",
      "Epoch 10/500\n",
      "21/21 [==============================] - 1s 27ms/step - loss: 0.6654 - accuracy: 0.6340 - val_loss: 0.6335 - val_accuracy: 0.7358\n",
      "Epoch 11/500\n",
      "21/21 [==============================] - 1s 25ms/step - loss: 0.6597 - accuracy: 0.6246 - val_loss: 0.6475 - val_accuracy: 0.6981\n",
      "Epoch 12/500\n",
      "21/21 [==============================] - 1s 26ms/step - loss: 0.6632 - accuracy: 0.6075 - val_loss: 0.6193 - val_accuracy: 0.7170\n",
      "Epoch 13/500\n",
      "21/21 [==============================] - 1s 28ms/step - loss: 0.6522 - accuracy: 0.6402 - val_loss: 0.6264 - val_accuracy: 0.6981\n",
      "Epoch 14/500\n",
      "21/21 [==============================] - 1s 27ms/step - loss: 0.6481 - accuracy: 0.6417 - val_loss: 0.6592 - val_accuracy: 0.6226\n",
      "Epoch 15/500\n",
      "21/21 [==============================] - 1s 26ms/step - loss: 0.6462 - accuracy: 0.6262 - val_loss: 0.6148 - val_accuracy: 0.6792\n",
      "Epoch 16/500\n",
      "21/21 [==============================] - 1s 27ms/step - loss: 0.6559 - accuracy: 0.5981 - val_loss: 0.6035 - val_accuracy: 0.7170\n",
      "Epoch 17/500\n",
      "21/21 [==============================] - 1s 26ms/step - loss: 0.6457 - accuracy: 0.6417 - val_loss: 0.6283 - val_accuracy: 0.7170\n",
      "Epoch 18/500\n",
      "21/21 [==============================] - 1s 28ms/step - loss: 0.6444 - accuracy: 0.6371 - val_loss: 0.7190 - val_accuracy: 0.4906\n",
      "Epoch 19/500\n",
      "21/21 [==============================] - 1s 27ms/step - loss: 0.6589 - accuracy: 0.6106 - val_loss: 0.6054 - val_accuracy: 0.7358\n",
      "Epoch 20/500\n",
      "21/21 [==============================] - 1s 25ms/step - loss: 0.6370 - accuracy: 0.6402 - val_loss: 0.6281 - val_accuracy: 0.6981\n",
      "Epoch 21/500\n",
      "21/21 [==============================] - 1s 26ms/step - loss: 0.6339 - accuracy: 0.6464 - val_loss: 0.6492 - val_accuracy: 0.6604\n",
      "Epoch 22/500\n",
      "21/21 [==============================] - 1s 28ms/step - loss: 0.6287 - accuracy: 0.6636 - val_loss: 0.6069 - val_accuracy: 0.7170\n",
      "Epoch 23/500\n",
      "21/21 [==============================] - 1s 27ms/step - loss: 0.6311 - accuracy: 0.6495 - val_loss: 0.6289 - val_accuracy: 0.6792\n",
      "Epoch 24/500\n",
      "21/21 [==============================] - 1s 25ms/step - loss: 0.6200 - accuracy: 0.6526 - val_loss: 0.5863 - val_accuracy: 0.7170\n",
      "Epoch 25/500\n",
      "21/21 [==============================] - 1s 25ms/step - loss: 0.6273 - accuracy: 0.6511 - val_loss: 0.6366 - val_accuracy: 0.6981\n",
      "Epoch 26/500\n",
      "21/21 [==============================] - 1s 26ms/step - loss: 0.6127 - accuracy: 0.6713 - val_loss: 0.6252 - val_accuracy: 0.7170\n",
      "Epoch 27/500\n",
      "21/21 [==============================] - 1s 27ms/step - loss: 0.6179 - accuracy: 0.6729 - val_loss: 0.6376 - val_accuracy: 0.6981\n",
      "Epoch 28/500\n",
      "21/21 [==============================] - 1s 25ms/step - loss: 0.6090 - accuracy: 0.6745 - val_loss: 0.6083 - val_accuracy: 0.6792\n",
      "Epoch 29/500\n",
      "21/21 [==============================] - 1s 25ms/step - loss: 0.6085 - accuracy: 0.6729 - val_loss: 0.6354 - val_accuracy: 0.6981\n",
      "Epoch 30/500\n",
      "21/21 [==============================] - 1s 26ms/step - loss: 0.6021 - accuracy: 0.6682 - val_loss: 0.6493 - val_accuracy: 0.6981\n",
      "Epoch 31/500\n",
      "21/21 [==============================] - 1s 26ms/step - loss: 0.5970 - accuracy: 0.6807 - val_loss: 0.5981 - val_accuracy: 0.6792\n",
      "Epoch 32/500\n",
      "21/21 [==============================] - 1s 25ms/step - loss: 0.5966 - accuracy: 0.6729 - val_loss: 0.6210 - val_accuracy: 0.6792\n",
      "Epoch 33/500\n",
      "21/21 [==============================] - 1s 27ms/step - loss: 0.5994 - accuracy: 0.6900 - val_loss: 0.6636 - val_accuracy: 0.6226\n",
      "Epoch 34/500\n",
      "21/21 [==============================] - 1s 26ms/step - loss: 0.5878 - accuracy: 0.7072 - val_loss: 0.6154 - val_accuracy: 0.6792\n",
      "Epoch 35/500\n",
      "21/21 [==============================] - 1s 27ms/step - loss: 0.5884 - accuracy: 0.6947 - val_loss: 0.6321 - val_accuracy: 0.6981\n",
      "Epoch 36/500\n",
      "21/21 [==============================] - 1s 28ms/step - loss: 0.5848 - accuracy: 0.6978 - val_loss: 0.6675 - val_accuracy: 0.6226\n",
      "Epoch 37/500\n",
      "21/21 [==============================] - 1s 27ms/step - loss: 0.5842 - accuracy: 0.6978 - val_loss: 0.5931 - val_accuracy: 0.6604\n",
      "Epoch 38/500\n",
      "21/21 [==============================] - 1s 26ms/step - loss: 0.5914 - accuracy: 0.6931 - val_loss: 0.7044 - val_accuracy: 0.5849\n",
      "Epoch 39/500\n",
      "21/21 [==============================] - 1s 25ms/step - loss: 0.5700 - accuracy: 0.7243 - val_loss: 0.6382 - val_accuracy: 0.6415\n",
      "Epoch 40/500\n",
      "21/21 [==============================] - 1s 25ms/step - loss: 0.5704 - accuracy: 0.7150 - val_loss: 0.6884 - val_accuracy: 0.6226\n",
      "Epoch 41/500\n",
      "21/21 [==============================] - 0s 24ms/step - loss: 0.5703 - accuracy: 0.7243 - val_loss: 0.6273 - val_accuracy: 0.6792\n",
      "Epoch 42/500\n",
      "21/21 [==============================] - 1s 26ms/step - loss: 0.5653 - accuracy: 0.7259 - val_loss: 0.6512 - val_accuracy: 0.6415\n",
      "Epoch 43/500\n",
      "21/21 [==============================] - 1s 27ms/step - loss: 0.5635 - accuracy: 0.7274 - val_loss: 0.6726 - val_accuracy: 0.5849\n",
      "Epoch 44/500\n",
      "21/21 [==============================] - 1s 25ms/step - loss: 0.5679 - accuracy: 0.6978 - val_loss: 0.6365 - val_accuracy: 0.6604\n",
      "Epoch 45/500\n",
      "21/21 [==============================] - 0s 23ms/step - loss: 0.5534 - accuracy: 0.7321 - val_loss: 0.6650 - val_accuracy: 0.6226\n",
      "Epoch 46/500\n",
      "21/21 [==============================] - 1s 27ms/step - loss: 0.5525 - accuracy: 0.7430 - val_loss: 0.6846 - val_accuracy: 0.6038\n",
      "Epoch 47/500\n",
      "21/21 [==============================] - 1s 25ms/step - loss: 0.5519 - accuracy: 0.7259 - val_loss: 0.6162 - val_accuracy: 0.6981\n",
      "Epoch 48/500\n",
      "21/21 [==============================] - 1s 25ms/step - loss: 0.5461 - accuracy: 0.7305 - val_loss: 0.6976 - val_accuracy: 0.6038\n",
      "Epoch 49/500\n",
      "21/21 [==============================] - 1s 25ms/step - loss: 0.5586 - accuracy: 0.7118 - val_loss: 0.6388 - val_accuracy: 0.6792\n",
      "Epoch 50/500\n",
      "21/21 [==============================] - 1s 25ms/step - loss: 0.5482 - accuracy: 0.7259 - val_loss: 0.7039 - val_accuracy: 0.5660\n",
      "Epoch 51/500\n",
      "21/21 [==============================] - 1s 25ms/step - loss: 0.5293 - accuracy: 0.7586 - val_loss: 0.6459 - val_accuracy: 0.6604\n",
      "Epoch 52/500\n",
      "21/21 [==============================] - 1s 26ms/step - loss: 0.5275 - accuracy: 0.7336 - val_loss: 0.6452 - val_accuracy: 0.6415\n",
      "Epoch 53/500\n",
      "21/21 [==============================] - 1s 25ms/step - loss: 0.5289 - accuracy: 0.7492 - val_loss: 0.6475 - val_accuracy: 0.6415\n",
      "Epoch 54/500\n",
      "21/21 [==============================] - 1s 27ms/step - loss: 0.5293 - accuracy: 0.7492 - val_loss: 0.7135 - val_accuracy: 0.5472\n",
      "Epoch 55/500\n",
      "21/21 [==============================] - 1s 26ms/step - loss: 0.5211 - accuracy: 0.7539 - val_loss: 0.6679 - val_accuracy: 0.6604\n",
      "Epoch 56/500\n",
      "21/21 [==============================] - 1s 25ms/step - loss: 0.5259 - accuracy: 0.7555 - val_loss: 0.7220 - val_accuracy: 0.5660\n",
      "Epoch 57/500\n",
      "21/21 [==============================] - 0s 24ms/step - loss: 0.5169 - accuracy: 0.7601 - val_loss: 0.6638 - val_accuracy: 0.6792\n",
      "Epoch 58/500\n",
      "21/21 [==============================] - 1s 25ms/step - loss: 0.5060 - accuracy: 0.7897 - val_loss: 0.6646 - val_accuracy: 0.6604\n",
      "Epoch 59/500\n",
      "21/21 [==============================] - 1s 25ms/step - loss: 0.5058 - accuracy: 0.7648 - val_loss: 0.6782 - val_accuracy: 0.6415\n",
      "Epoch 60/500\n",
      "21/21 [==============================] - 1s 24ms/step - loss: 0.5088 - accuracy: 0.7679 - val_loss: 0.6559 - val_accuracy: 0.6604\n",
      "Epoch 61/500\n",
      "21/21 [==============================] - 1s 26ms/step - loss: 0.5009 - accuracy: 0.7773 - val_loss: 0.7145 - val_accuracy: 0.5472\n",
      "Epoch 62/500\n",
      "21/21 [==============================] - 1s 26ms/step - loss: 0.4937 - accuracy: 0.7897 - val_loss: 0.6758 - val_accuracy: 0.6604\n",
      "Epoch 63/500\n",
      "21/21 [==============================] - 1s 28ms/step - loss: 0.4953 - accuracy: 0.7726 - val_loss: 0.7191 - val_accuracy: 0.5849\n",
      "Epoch 64/500\n",
      "21/21 [==============================] - 0s 23ms/step - loss: 0.4863 - accuracy: 0.7850 - val_loss: 0.6486 - val_accuracy: 0.6792\n",
      "Epoch 65/500\n",
      "21/21 [==============================] - 1s 27ms/step - loss: 0.5034 - accuracy: 0.7586 - val_loss: 0.7353 - val_accuracy: 0.5660\n",
      "Epoch 66/500\n",
      "21/21 [==============================] - 1s 28ms/step - loss: 0.4807 - accuracy: 0.7991 - val_loss: 0.6773 - val_accuracy: 0.6415\n",
      "Epoch 67/500\n",
      "21/21 [==============================] - 1s 27ms/step - loss: 0.4790 - accuracy: 0.7882 - val_loss: 0.6667 - val_accuracy: 0.6415\n",
      "Epoch 68/500\n",
      "21/21 [==============================] - 1s 26ms/step - loss: 0.4869 - accuracy: 0.7710 - val_loss: 0.7761 - val_accuracy: 0.4906\n",
      "Epoch 69/500\n",
      "21/21 [==============================] - 1s 26ms/step - loss: 0.4860 - accuracy: 0.7741 - val_loss: 0.7211 - val_accuracy: 0.6038\n",
      "Epoch 70/500\n",
      "21/21 [==============================] - 1s 25ms/step - loss: 0.4895 - accuracy: 0.7757 - val_loss: 0.6876 - val_accuracy: 0.6415\n",
      "Epoch 71/500\n",
      "21/21 [==============================] - 1s 29ms/step - loss: 0.4711 - accuracy: 0.7944 - val_loss: 0.6712 - val_accuracy: 0.6415\n",
      "Epoch 72/500\n",
      "21/21 [==============================] - 0s 23ms/step - loss: 0.4704 - accuracy: 0.8084 - val_loss: 0.7356 - val_accuracy: 0.6038\n",
      "Epoch 73/500\n",
      "21/21 [==============================] - 1s 26ms/step - loss: 0.4672 - accuracy: 0.8006 - val_loss: 0.6900 - val_accuracy: 0.6226\n",
      "Epoch 74/500\n",
      "21/21 [==============================] - 1s 25ms/step - loss: 0.4580 - accuracy: 0.8006 - val_loss: 0.6876 - val_accuracy: 0.6604\n",
      "Epoch 75/500\n",
      "21/21 [==============================] - 0s 22ms/step - loss: 0.4624 - accuracy: 0.7944 - val_loss: 0.6942 - val_accuracy: 0.6415\n",
      "Epoch 76/500\n",
      "21/21 [==============================] - 1s 26ms/step - loss: 0.4519 - accuracy: 0.8131 - val_loss: 0.7071 - val_accuracy: 0.6415\n",
      "Epoch 77/500\n",
      "21/21 [==============================] - 1s 24ms/step - loss: 0.4655 - accuracy: 0.7804 - val_loss: 0.6925 - val_accuracy: 0.6038\n",
      "Epoch 78/500\n",
      "21/21 [==============================] - 1s 26ms/step - loss: 0.4418 - accuracy: 0.8084 - val_loss: 0.6988 - val_accuracy: 0.6038\n",
      "Epoch 79/500\n",
      "21/21 [==============================] - 1s 25ms/step - loss: 0.4377 - accuracy: 0.8255 - val_loss: 0.7125 - val_accuracy: 0.6415\n",
      "Epoch 80/500\n",
      "21/21 [==============================] - 0s 23ms/step - loss: 0.4316 - accuracy: 0.8162 - val_loss: 0.6858 - val_accuracy: 0.6415\n",
      "Epoch 81/500\n",
      "21/21 [==============================] - 1s 25ms/step - loss: 0.4622 - accuracy: 0.7835 - val_loss: 0.7531 - val_accuracy: 0.5660\n",
      "Epoch 82/500\n",
      "21/21 [==============================] - 1s 25ms/step - loss: 0.4345 - accuracy: 0.8193 - val_loss: 0.6889 - val_accuracy: 0.6038\n",
      "Epoch 83/500\n",
      "21/21 [==============================] - 1s 25ms/step - loss: 0.4226 - accuracy: 0.8240 - val_loss: 0.7052 - val_accuracy: 0.6038\n",
      "Epoch 84/500\n",
      "21/21 [==============================] - 1s 27ms/step - loss: 0.4190 - accuracy: 0.8271 - val_loss: 0.7310 - val_accuracy: 0.6038\n",
      "Epoch 85/500\n",
      "21/21 [==============================] - 1s 24ms/step - loss: 0.4283 - accuracy: 0.8224 - val_loss: 0.7264 - val_accuracy: 0.5849\n",
      "Epoch 86/500\n",
      "21/21 [==============================] - 0s 23ms/step - loss: 0.4214 - accuracy: 0.8302 - val_loss: 0.7394 - val_accuracy: 0.6038\n",
      "Epoch 87/500\n",
      "21/21 [==============================] - 1s 25ms/step - loss: 0.4146 - accuracy: 0.8349 - val_loss: 0.7339 - val_accuracy: 0.6038\n",
      "Epoch 88/500\n",
      "21/21 [==============================] - 1s 28ms/step - loss: 0.4132 - accuracy: 0.8380 - val_loss: 0.7466 - val_accuracy: 0.5849\n",
      "Epoch 89/500\n",
      "21/21 [==============================] - 1s 25ms/step - loss: 0.4090 - accuracy: 0.8162 - val_loss: 0.8175 - val_accuracy: 0.5660\n",
      "Epoch 90/500\n",
      "21/21 [==============================] - 1s 26ms/step - loss: 0.4515 - accuracy: 0.7882 - val_loss: 0.7260 - val_accuracy: 0.6038\n",
      "Epoch 91/500\n",
      "21/21 [==============================] - 1s 24ms/step - loss: 0.4045 - accuracy: 0.8380 - val_loss: 0.7005 - val_accuracy: 0.6038\n",
      "Epoch 92/500\n",
      "21/21 [==============================] - 0s 24ms/step - loss: 0.3907 - accuracy: 0.8536 - val_loss: 0.7484 - val_accuracy: 0.6415\n",
      "Epoch 93/500\n",
      "21/21 [==============================] - 1s 25ms/step - loss: 0.3978 - accuracy: 0.8333 - val_loss: 0.7176 - val_accuracy: 0.6038\n",
      "Epoch 94/500\n",
      "21/21 [==============================] - 1s 25ms/step - loss: 0.3943 - accuracy: 0.8442 - val_loss: 0.7672 - val_accuracy: 0.6226\n",
      "Epoch 95/500\n",
      "21/21 [==============================] - 1s 25ms/step - loss: 0.4073 - accuracy: 0.8224 - val_loss: 0.6887 - val_accuracy: 0.6415\n",
      "Epoch 96/500\n",
      "21/21 [==============================] - 1s 25ms/step - loss: 0.3942 - accuracy: 0.8411 - val_loss: 0.7053 - val_accuracy: 0.6226\n",
      "Epoch 97/500\n",
      "21/21 [==============================] - 0s 24ms/step - loss: 0.3920 - accuracy: 0.8396 - val_loss: 0.7516 - val_accuracy: 0.6226\n",
      "Epoch 98/500\n",
      "21/21 [==============================] - 1s 27ms/step - loss: 0.3918 - accuracy: 0.8349 - val_loss: 0.7337 - val_accuracy: 0.6226\n",
      "Epoch 99/500\n",
      "21/21 [==============================] - 1s 25ms/step - loss: 0.3762 - accuracy: 0.8427 - val_loss: 0.7031 - val_accuracy: 0.6226\n",
      "Epoch 100/500\n",
      "21/21 [==============================] - 1s 26ms/step - loss: 0.3854 - accuracy: 0.8333 - val_loss: 0.7433 - val_accuracy: 0.6415\n",
      "Epoch 101/500\n",
      "21/21 [==============================] - 0s 23ms/step - loss: 0.3739 - accuracy: 0.8474 - val_loss: 0.7404 - val_accuracy: 0.6415\n",
      "Epoch 102/500\n",
      "21/21 [==============================] - 1s 26ms/step - loss: 0.3754 - accuracy: 0.8489 - val_loss: 0.7246 - val_accuracy: 0.6415\n",
      "Epoch 103/500\n",
      "21/21 [==============================] - 1s 25ms/step - loss: 0.3693 - accuracy: 0.8505 - val_loss: 0.7344 - val_accuracy: 0.6415\n",
      "Epoch 104/500\n",
      "21/21 [==============================] - 1s 26ms/step - loss: 0.3611 - accuracy: 0.8567 - val_loss: 0.7705 - val_accuracy: 0.6226\n",
      "Epoch 105/500\n",
      "21/21 [==============================] - 1s 25ms/step - loss: 0.3559 - accuracy: 0.8660 - val_loss: 0.7383 - val_accuracy: 0.5849\n",
      "Epoch 106/500\n",
      "21/21 [==============================] - 1s 27ms/step - loss: 0.3571 - accuracy: 0.8692 - val_loss: 0.7456 - val_accuracy: 0.5472\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f8120f231c0>"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_2_100.compile(optimizer=opt1, \n",
    "                loss='categorical_crossentropy', \n",
    "                metrics=['accuracy'])\n",
    "#Here we use cross-entropy as the criteria for loss.\n",
    "model_2_100.fit(X_train_over, y_train_over, \n",
    "            validation_data=(X_val, y_val), \n",
    "            epochs=500, verbose=True, \n",
    "            callbacks=[early_stopping_monitor])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "43c1879e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 4ms/step - loss: 0.6623 - accuracy: 0.6949\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.6629 - accuracy: 0.7358\n"
     ]
    }
   ],
   "source": [
    "m2_eval_test = model_2_100.evaluate(X_test, y_test)\n",
    "m2_eval_val = model_2_100.evaluate(X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "31a60e64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 8ms/step\n",
      "roc auc score:  0.7092731829573935\n",
      "average precision score:  0.679132020550737\n"
     ]
    }
   ],
   "source": [
    "pred = model_2_100.predict(X_test)\n",
    "roc_value = roc_auc_score(y_test, pred)\n",
    "ap_score = average_precision_score(y_test, pred)\n",
    "print('roc auc score: ', roc_value)\n",
    "print('average precision score: ', ap_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "af8be52b",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = pred\n",
    "y_c = (y_pred > 0.5).astype(\"int32\")\n",
    "y_test_np = y_test.to_numpy()\n",
    "y_test_np = y_test_np.astype('int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "ea545ac6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 8ms/step - loss: 0.6623 - accuracy: 0.6949\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXIAAAFACAYAAAChlvevAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAw+0lEQVR4nO3dd7xcRd3H8c83BRIgdIiEJi34IJBQpQiCWDAiKh1BimBARQFBAbGA5XlUBKVL6GAMLVSNEqQFkBJCh4RiCBITOiEBQkn4PX/MXFkuu/fu3uy5d0/yfed1XndPm5kt+e3snDkzigjMzKy8evV0AczMbN44kJuZlZwDuZlZyTmQm5mVnAO5mVnJOZCbmZWcA3kTSTpO0p96uhxFkPRVSc9Kel3SBvOQzqOStmleybqfpK0kPV5wHq9LWr2D/VMkfabOtPaTdHudx3b5Mzw/f/5b3QIZyCV9UtI/Jb0m6RVJd0japKfLNa8krSDpXEnTJc2SNEnS8ZIWbULyvwMOiYjFIuL+riYSER+PiFuaUJ4PkHSLpJA0pN32q/P2bepMJySt2dExEXFbRKzd9dJ2Lr/Ok3OZLpD0yyLzs3Jb4AK5pMWBvwCnAksDKwLHA2/3ZLnak9S7weOXBu4E+gObR8QA4LPAksAaTSjSqsCjTUinSE8A+7StSFoG2Ax4sVkZSOrTrLTMmmWBC+TAYICIGBURcyNidkSMjYiH2g6Q9A1JEyW9Kul6SatW7Ds5NzHMlDRB0lbt0u8n6dJcI76vsoYo6X9yzXFGbmLYsWLfBZLOlDRG0hvAtvnn85GSHsq/Hi6V1K/G8/o+MAvYOyKm5Of4bEQc2vbcJG0haXxOa7ykLSryv0XSL/Kvk1mSxkpaVtLCkl4HegMPSvpXPv4DNdfKWmM+7y/5eb4i6TZJvfK+/zYJ5LT/IGlaXv4gaeG8bxtJUyUdIemF/Ctj/07e25HA7hVfgnsCVwHvVJRzU0l35rJNl3SapIXyvnH5sAdz08buFeU4StJzwPlt2/I5a+TnuGFeHyTppWq/ACTtL+m6ivWnJF1Wsf6spKGVr6+k4cBewA9zma6rSHJonZ+N9uWYl8/wIEmjJb0o6WlJ36uRRz9Jf5L0cn6tx0saWE/5rHELYiB/Apgr6UJJX5C0VOVOSV8BfgTsBCwH3AaMqjhkPDCUVJv/M3B5u/9AXwYur9h/taS+kvoC1wFjgeWB7wIjJVX+RP8a8CtgANDWprkbsD2wGrA+sF+N5/UZ4MqIeK/aTqUa+1+BU4BlgJOAvyrVWivz3z+XbyHgyIh4OyIWy/uHREQ9tfsjgKmk128g6fWsNhbEsaQa81BgCLAp8OOK/R8BliD9ajoAOL39+9XONOAx4HN5fR/gonbHzAUOB5YFNge2A74NEBFb52OG5KaNSyvKsTTpV8nwysQi4l/AUaT3chHgfOCCGs1HtwJbSeolaQWgL7AlgFJ7+GLAQ5UnRMQI0hfUb3OZvlSxu97PRntd/Qz3In2GHyS9J9sBh0n6fJU89iW9dyuTPm8HA7PrLJ81aIEL5BExE/gkKbCcDbwo6dqK2sJBwP9FxMSImAP8L6nms2o+/08R8XJEzImIE4GFgcpgPCEiroiId0nBsh8pWG1G+o/664h4JyJuIjXx7Flx7jURcUdEvBcRb+Vtp0TEtIh4hfSfaGiNp7YMML2Dp/5F4MmIuDiXfRQwCagMDOdHxBMRMRu4rIO8OvMusAKwakS8m9uUqwXyvYCfR8QLEfEiqYnr6+3S+XlOYwzwOh98rau5CNgnf0EuGRF3Vu6MiAkRcVd+DaYAZwGf6iTN94Cf5S+1DwWjiDgbeBK4Oz/vY6slktu8Z5Fe108B1wP/kfSxvH5brS/iGur9bLQvR1c/w5sAy0XEz/NneDLp/9AeVbJ5l/SZXDP/8p2Q/+9ZARa4QA6Qg/R+EbESsC4wCPhD3r0qcHL+OTgDeAUQqQZC/qk/Mf+cnUGqdSxbkfyzFfm8R6qZDsrLs+3+oz7Tlm77cys8V/H4TdKXQTUvk4JILYNyfpXa519vXp05AXgKGCtpsqSj6yzTM3lbm5fzl2kjZboS+DTpF8/F7XdKGpybfZ6TNJP0Rb1s++PaebHii7WWs0mfpVMjoqPrLbcC2wBb58e3kIL4p/J6I7r0fs3DZ3hVYFDb/4187o9Iv7rau5j0RXVJbjb7bf5VagVYIAN5pYiYBFxA+k8I6UN8UEQsWbH0j4h/5rbEo0g/aZeKiCWB10iBvs3KbQ/yT9GVSD/5pwErt7UVZ6sA/6kszjw8lX8AX22XfqVppP+Ildrn34g3gUUq1j/S9iAiZkXEERGxOqnG/31J29VRplXyti6LiDeBvwHfokogB84k/RJZKyIWJwUiVTnuA8l2tFPSYqSKwLnAcbkZq5a2QL5VfnwrnQfypg1ROo+f4WeBp9v93xgQEcM+VOD0K+r4iFgH2ALYgYoL0dZcC1wgl/SxXCNZKa+vTGreuCsf8kfgGEkfz/uXkLRr3jcAmEPqBdFH0k+BxdtlsZGknZR6NxxG6g1zF+ln9xuki1Z988WwLwGXNOmpnZTLcmFbM5CkFSWdJGl9YAwwWNLXJPWRtDuwDql5pyseAL4mqbek7alonpC0Q75QJ2AmqV16bpU0RgE/lrScpGWBnwLN6If8I+BTbRd92xmQy/R6btL4Vrv9zwM1+2/XcDKpOeJA0nWIP3Zw7K3AtkD/iJhKugazPakZola3zq6UqZZ5+QzfA8xUuvDbP7/366pK111J20paT+nC80xSU0u1z4A1wQIXyEltlJ8A7lbqHXIX8AjpAh0RcRXwG9JPwpl53xfyudeTantPkJoB3uLDzSHXALsDr5Lae3fKtZN3gB1zWi8BZwD75F8E8yy3k25B+g9zt6RZwI2k2tZTEfEyqVZ0BKkZ5ofADhHxUhezPJT0RTSD1NZ9dcW+tUi/EF4ndYk8o8bFv18C95Iu8D0M3Je3zZPcblzrBpgjSRd1Z5GaQy5tt/840pfhDEm7dZaXpC+TAvHBedP3gQ0l7VWjbE+QXpfb8vpMYDJwR0TUCnTnAuvkMl3dWZk6MS+f4bmk93wo8DTpc3wOqWmmvY8AV5CC+ETSF5hvFiqIql+DMjOzslgQa+RmZvMVB3Izs5JzIDczKzkHcjOzknMgNzMrOQdyM7OScyA3Mys5B3Izs5JzIDczKzkHcjOzknMgNzMrOQdyM7OScyA3Mys5B3Izs5JzIDczKzkHcjOzknMgNzMrOQdyM7OScyA3Mys5B3Izs5JzIDczKzkHcjOzknMgNzMrOQdyM7OScyA3Mys5B3Izs5JzIDczKzkHcjOzknMgNzMrOQdyM7OScyA3Mys5B3Izs5JzIDczKzkHcjOzknMgNzMruT49XYBa+m9wSPR0Gaz1vDr+tJ4ugrWgfn3QvKbRSMyZff9p85xfM7VsIDcz61a9evd0CbrMgdzMDEDlbWl2IDczA1BLtZY0xIHczAxcIzczKz3XyM3MSs41cjOzknOvFTOzknPTiplZyblpxcys5FwjNzMrOdfIzcxKzoHczKzkejen14qklYGLgI8A7wEjIuJkSZcCa+fDlgRmRMTQKudPAWYBc4E5EbFxZ3k6kJuZQTPbyOcAR0TEfZIGABMk3RARu7+flU4EXusgjW0j4qV6M3QgNzODpjWtRMR0YHp+PEvSRGBF4DEASQJ2Az7dlAzxxBJmZolU9yJpuKR7K5bh1ZPUR4ENgLsrNm8FPB8RT9YoSQBjJU2olW57rpGbmUFDNfKIGAGM6DA5aTFgNHBYRMys2LUnMKqDU7eMiGmSlgdukDQpIsZ1lFdhNXJJvSRtUVT6ZmZN1at3/UsnJPUlBfGREXFlxfY+wE7ApbXOjYhp+e8LwFXApp0WvdMSdVFEvAecWFT6ZmZN1UDTSsfJSMC5wMSIOKnd7s8AkyJiao1zF80XSJG0KPA54JHOil50G/lYSTvnJ2Zm1rrUq/6lY1sCXwc+LemBvAzL+/agXbOKpEGSxuTVgcDtkh4E7gH+GhF/7yzDotvIvw8sCsyVNBsQEBGxeMH5mpk1pkn1zYi4HapPBh0R+1XZNg0Ylh9PBoY0mmehgTwiBhSZvplZ0/jOztok7QhsnVdviYi/FJ2nmVnDHMirk/RrYBNgZN50qKRPRsTRReZrZtYwTyxR0zBgaO7BgqQLgfsBB3Izay0l7pPRHTcELQm8kh8v0Q35mZk1zk0rNf0fcL+km0lXcbcGjik4TzOzxrlGXl1EjJJ0C6mdXMBREfFckXmamXVFmW93KfS3hKQtgZkRcS0wAPihpFWLzNPMrCvUS3UvraboRqEzgTclDQF+ADxDGnDdzKylKI1qWNfSaooO5HMiIoAvA6dExMmkmrmZWUspcyAv+mLnLEnHAHsDW0vqDfQtOE8zs4a1YoCuV9E18t2Bt4ED8kXOFYETCs7TzKxhrpHXNgs4OSLmShoMfIyOB1Q3M+sZrRef61Z0jXwcsLCkFYEbgf2BCwrO08ysYb169ap7aTVFl0gR8SZpRoxTI+KrwMcLztPMrGFuWqlNkjYH9gIOyNvKOzKNmc23WjFA16voQH4Y6Zb8qyLiUUmrAzcXnKeZWePKG8cLv0X/VuDWPPdc2+wX3ysyTzOzrihzjbzoW/Q3l/QYMDGvD5F0RpF5mpl1RbPayCWtLOlmSRMlPSrp0Lz9OEn/qTKPZ/vzt5f0uKSnJNU15HfRTSt/AD4PXAsQEQ9K2rrDM8zMekATx1CZAxwREfdJGgBMkHRD3vf7iPhdzTKkmyZPBz4LTAXGS7o2Ih7rKMPCxyOPiGfbfYPNLTpPM7NGNatpJSKmA9Pz41mSJpJuhqzHpsBTuRkaSZeQhjjpMJAX3f3wWUlbACFpIUlHkptZzMxaSSNNK5KGS7q3YhleI82PAhsAd+dNh0h6SNJ5kpaqcsqKwLMV61Op40ug6EB+MPCdXJCpwNC8bmbWUhoJ5BExIiI2rlhGVElvMWA0cFhEzCSNBrsGKQ5OB06sVowq26KzshfWtJLbev4QEXsVlYeZWbM0s9eKpL6kID4yIq4EiIjnK/afDfylyqlTgZUr1lcCpnWWX2E18oiYCywnaaGi8jAza5ZmTSyh9I1wLjAxIk6q2L5CxWFfBR6pcvp4YC1Jq+XYuQe5s0hHir7YOQW4Q9K1wBttGyufnJlZK2hijXxL4OvAw5IeyNt+BOwpaSipqWQKcFDOdxBwTkQMi4g5kg4BrifdBX9eRDzaWYZFB/JpeemFJ5QwsxbWxF4rt1O9rXtMjeOnAcMq1sfUOraWou/sPL7I9M3Mmqa8N3YWG8glXceHr7i+BtwLnBURbxWZfxmtNHBJzvnFPgxcZnHei+C80Xdw+qhbWG/wipx67B4s2n9hnpn2MvsfeyGz3vDLt6AaefGFjL7iciKCnXfZlb332a+ni1R6vkW/tsnA68DZeZkJPA8MzuvWzpy573H0SVeywc6/5FP7/I6Ddt+aj63+Ec786df48SnXsMlu/8u1Nz/I4ftu19NFtR7y5JNPMPqKyxl5yeVcfuU1jLv1Fp55ZkpPF6v0yjyMbdGBfIOI+FpEXJeXvYFNI+I7wIYF511Kz700kwcmTQXg9TffZtLTzzFouSVZa9XluX3CUwDcdNckvrLd0B4spfWkpyf/i/WHDKF///706dOHjTbehJv+cUPnJ1qHPLFEbctJWqVtJT9eNq++U3DepbfKCkszdO2VGP/IFB7713R22GY9AHb67IasNLDaTWG2IFhzzcFMuPdeZsx4ldmzZ3P7beN47rnnerpY5acGlhZTdK+VI4DbJf2L9PRXA76dh7W9sP3B+TbX4QB9VtqGPssuuJMJLdp/IUb97kB+8LvRzHrjLQ46biQn/nAXjvnmF/jrrQ/zzrsesmZBtfoaa7D/AQdy0IHfYJFFFmHw2mvTp7fna5lXrdhkUq+ie62MkbQWadJlAZMqLnD+ocrxI4ARAP03OKTT21LnV3369GLU777JpX+7l2tuehCAJ6Y8z5e+fToAa66yPF/YasH9kjPYaedd2WnnXQE45Q8nMXDgwB4uUfmVOZAXPR55X1Kn958APwYOzNusA3/82V48/vRznPKnm/67bbmlFgPSh+3ob36es6+4vaeKZy3g5ZdfBmD6tGnc+I+xfGHYDj1covKT6l9aTdFNK2cCfYG2ySS+nrcdWHC+pbXF0NXZa4dP8PAT/+GuS9KY8j877VrWXHl5Dto9DeV+zU0PcNE1d/VkMa2HHXHYd3ltxgz69OnDj378MxZfYomeLlLplblGrojiWjAkPRgRQzrbVs2C3LRitb06/rSeLoK1oH595v0S5NpHXV93zHn8N59vqahfdK+VuZLWaFvJky/7Kp2ZtRw3rdR2JHCzpMmki52rAvsXnKeZWcN6NW+qt25X9HjkQ4C1gLV5v9fK20XlaWbWVa1Y065X0eOR7xgRb0fEQxHxoIO4mbWqMt+iX3TTyj8lnQZcygfHI7+v4HzNzBrippXatsh/f16xLYBPF5yvmVlDWrGmXa+iA/muEfFSwXmYmc2zEsfxYtrIJX1J0ovAQ5KmStqi05PMzHpQmdvIi7rY+Stgq4gYBOwM/F9B+ZiZNUWz+pFLWlnSzZImSnpU0qF5+wmSJkl6SNJVkpascf4USQ9LekDSvfWUvahAPiciJgFExN14vk4za3FNrJHPAY6IiP8BNgO+I2kd4AZg3YhYH3gCOKaDNLaNiKERsXE9ZS+qjXx5Sd+vtR4RJxWUr5lZlzSr10pETAem58ezJE0EVoyIsRWH3QXs0pQMKa5GfjapFt62tF83M2spjTStSBou6d6KZXj1NPVRYAPg7na7vgH8rUZRAhgraUKtdNsrpEYeEccXka6ZWVEauYhZOXdCB+ktBowGDouImRXbjyU1v4ysceqWETFN0vLADZImRcS4jvLqtsnnJPkmIDNrWc0cNCvPuzAaGBkRV1Zs3xfYAdgragw9GxHT8t8XgKuATTvLrztnEW29PjtmZlmzLnYqHXAuMLHyeqCk7YGjSEOXvFnj3EUlDWh7DHwOeKSzshd9Q1Clv3ZjXmZmDWli9/AtSZPoPCzpgbztR8ApwMKk5hKAuyLiYEmDgHMiYhgwELgq7+8D/Dki/t5Zht0WyCPix92Vl5lZo5rYa+V2qrdAjKlx/DRgWH48mTRqbEOKnrNzJ0lPSnpN0kxJsyTN7PxMM7PuVeY7O4uukf8W+FJETCw4HzOzedKKAbpendbIJf1W0uKS+kq6UdJLkvauM/3nHcTNrAzm96nePhcRP5T0VWAqsCtwM/CnOs69V9KlwNXAfyeVqOyOY2bWCspcI68nkPfNf4cBoyLilQae8OLAm6QuNG0CcCA3s5Yyv08scZ2kScBs4NuSlgPeqifxiPBEy2ZWCiWukHfeRh4RRwObAxtHxLukGvaX60lc0kp5uMYXJD0vabSkleatyGZmzddLqntpNfVc7FwE+A5wZt40CKhraEXgfODafM6KwHV5m5lZSynzxc56+pGfD7zD+/NvTgV+WWf6y0XE+RExJy8XAMs1Xkwzs2KVuR95PYF8jYj4LfAuQETMpv5xU16StLek3nnZG3i5i2U1MytML9W/tJp6Avk7kvqTepsgaQ0quhJ24hvAbsBzpIHWd8nbzMxaSq9eqntpNfX0WvkZ8HdgZUkjSQPC7FdP4hHxb2DHLpfOzKybqMQDtHYayCPihjyW+GakJpVDI+Kljs6R9NOOk4xfNFZMM7NitWBFu26dBnJJW+eHs/LfdSTRyYwVb1TZtihwALAM4EBuZi2lFS9i1queppUfVDzuR5qtYgLw6VonRMSJbY/zIOmHAvsDlwAn1jrPzKynlDiO19W08qXKdUkrk0Y17JCkpYHvA3sBFwIbRsSrXSynmVmhepe4baUrw9hOBdbt6ABJJwA7kSYnXS8iXu9CPmZm3Wa+blqRdCq56yGpu+JQ4MFOTjuC1EXxx8CxFS+QSBc7F+9KYc3MilLiOF5XjfzeisdzSCMg3tHRCRHRnZM6m5nNs2aNoZKbny8CPgK8B4yIiJNzc/OlwEeBKcBu1Zqb8yTNJwO9SXN5/rqzPOtpI7+wgedgZlZKTayQzwGOiIj7cmePCZJuIN1/c2NE/FrS0cDRwFEfKIPUGzgd+CypGXu8pGsj4rGOMqwZyCU9zPtNKh/YRWoeWb/+52Vm1tqa1UYeEdNJd7ITEbMkTSQNGvhlYJt82IXALbQL5KRegU/lSZiRdEk+r2uBHNihseKbmZVXEb1WJH0U2AC4GxiYgzwRMV3S8lVOWRF4tmJ9KvCJzvKpGcgj4plGCmxmVmaNVMglDQeGV2waEREj2h2zGDAaOCwiZtZZ4692ULWWkQ+op9fKZsCpwP8AC5Ea4N9wzxMzm5800rSSg/aIWvsl9SUF8ZEVcxQ/L2mFXBtfAXihyqlTgZUr1lcCpnVWnnp6l5wG7Ak8CfQHDiQFdjOz+UazhrFV+kY4F5gYESdV7LoW2Dc/3he4psrp44G1JK0maSFgj3xex2Xv/OlBRDwF9I6IuRFxPrBtPeeZmZVFEyeW2BL4OvBpSQ/kZRjwa+Czkp4k9Ur5dc53kKQxABExBzgEuB6YCFwWEY92lmE9/cjfzN8MD0j6Lelq7KJ1nGdmVhrNutQZEbd3kNx2VY6fBgyrWB8DjGkkz5o1cklt83J+PR93CGlUw5WBnRvJxMys1fXupbqXVtNRjfzsfNV1FHBJ7pB+fPcUy8yse5V5rJWaNfKI2IDUl3wucEVu5zlK0qrdVjozs24i1b+0mg4vdkbE4xFxfESsQ7rKuiRwk6QOx1oxMyubXlLdS6upaxhbSb2A5YGBpAudLxZZKDOz7taC8bluHQZySVuR+pB/BXiENMPP4RHxWtEFe3X8aUVnYSX075ff7OkiWAsaPHCReU6jd4kjeUeDZj0L/JsUvI+PiOe7rVRmZt2szBc7O6qRf9LjrZjZgqIFexXWzYNmmZkxnwZyM7MFyfzatGJmtsCYL2vk7SZd/pCI+F4hJTIz6wGteOt9vTqqkd/bwT4zs/lKmWeM7+hipyddNrMFRombyOuaIWg50gSh6wD92rZHxKcLLJeZWbdqxVvv61XPr4mRpAHOVyONfjiFNIuFmdl8Y74dNCtbJiLOBd6NiFsj4hvAZgWXy8ysWzVrqreeUE/3w3fz3+mSvkiaCHSl4opkZtb95tdeK21+KWkJ4AjSpMuLA4cXWiozs27WzDgu6TzSfA4vRMS6edulwNr5kCWBGRExtMq5U4BZpLkg5kTExu2Paa/TQB4Rf8kPX8OTLpvZfEpNm7UTgAuA04CL2jZExO7/zUs6kRRTa9k2Il6qN7N6eq2cT5Ubg3JbuZnZfKGZNfKIGCfpo9X2KY0FsBvQtJ5/9TSt/KXicT/gq6R2cjOz+UY3NpFvBTwfEU/W2B/AWEkBnBURIzpLsJ6mldGV65JGAf+oo7BmZqXRyMVOScOB4RWbRtQTcLM9SZPa17JlREyTtDxwg6RJETGuowS7MmjWWsAqXTjPzKxlNdI/PAftegN3RR7qA+wEbNRB2tPy3xckXQVsCsxbIJc0iw+2kT9HutPTzGy+0U13dn4GmBQRU6vtlLQo0CsiZuXHnwN+3lmi9TStDGi0pGZmZdPk7oejgG2AZSVNBX6Wb6zcg3bNKpIGAedExDDSBPdX5bHR+wB/joi/d5ZfPTXyGyNiu862mZmVWTMr5BGxZ43t+1XZNg0Ylh9PBoY0ml9H45H3AxYhfaMsBf/tZLk4MKjRjMzMWlmv5vYj71Yd1cgPAg4jBe0JvB/IZwKnF1ssM7Pu1bvEA5J3NB75ycDJkr4bEad2Y5nMzLrd/D6M7XuSlmxbkbSUpG8XVyQzs+43vw9j+82ImNG2EhGvAt8srERmZj2gl1T30mrquSGolyRFRABI6g0sVGyxzMy6VwvG57rVE8ivBy6T9EfSjUEHA532azQzK5MSX+usK5AfRRpT4FuknitjgbOLLJSZWXdrxSaTenX6JRQR70XEHyNil4jYGXiUNMGEmdl8o8xt5HX9mpA0VNJv8swVvwAm1XFOb0l/msfymZl1CzWwtJqO7uwcTBoXYE/gZeBSQBFR1yxBETFX0nKSFoqId5pSWjOzgrRgRbtuHbWRTwJuA74UEU8BSGp0rs4pwB2SrgXeaNsYESc1mI6ZWaFU4kjeUSDfmVQjv1nS34FLaPxXxbS89AI8iqKZtaze82Mgj4irSMMpLgp8BTgcGCjpTOCqiBjbWeIRcTyApAFpNV5vSqnNzJqsvGG8vl4rb0TEyIjYAVgJeAA4up7EJa0r6X7gEeBRSRMkfXxeCmxmVgRJdS+tpqE+8BHxSkScFRH1zv48Avh+RKwaEasCR+A+6GbWgno1sLSarszZ2YhFI+LmtpWIuCU31ZiZtZRWrGnXq+hAPlnST4CL8/rewNMF52lm1rDyhvHifyV8A1gOuBK4ClgW2L/gPM3MGtZbqnvpjKTzJL0g6ZGKbcdJ+o+kB/IyrMa520t6XNJTkuq6HllojTwPefu9XLjepKaWmUXmaWbWFU1uWbkAOA24qN3230fE72qXQb1JM7B9FpgKjJd0bUQ81lFmhdbIJf1Z0uK5XfxR4HFJPygyTzOzrlAD/zoTEeOAV7pQjE2BpyJicr4j/hLgy52dVHTTyjq5Bv4VYAywCvD1gvM0M2tYN80QdIikh3LTy1JV9q8IPFuxPjVv61DRgbyvpL6kQH5NRLxLGtPczKyl9EJ1L5KGS7q3YhleRxZnAmsAQ4HpwIlVjqn2NdFpzCy618pZpPFWHgTGSVoVcBu5mbWcXg1UayNiBOk+mUbOeb7tsaSzgb9UOWwqsHLF+kqkYU46VGiNPCJOiYgVI2JYJM8AdY2eaGbWnZrZRl41fWmFitWvku54b288sJak1SQtRBrv6trO0i76Yueh+WKnJJ0r6T6g3rtCzcy6TS/Vv3RG0ijgTmBtSVMlHQD8VtLDkh4iVWgPz8cOkjQGICLmAIeQpticCFwWEY92ml+eU7kQkh6MiCGSPg98B/gJcH5EbNjZuW/NcVu6fdi/X36zp4tgLWjwwEXmufPgTZNerjvmfPpjy7TU/UNFt5G3PdlhpAD+oMp8H6yZzbfKHJmKDuQTJI0FVgOOycPZvldwnvOVkRdfyOgrLici2HmXXdl7n/16ukjWA07+9XGM/+c4llhqaU6/8AoAzjvj99zzz3H07dOXj6y4EocefTyLDfCw/13V1bbvVlB098MDSEPebhIRbwIL4Vv06/bkk08w+orLGXnJ5Vx+5TWMu/UWnnlmSk8Xy3rAdtt/ieNOOP0D24ZuvBmnX3A5p15wGSuutCpX/Om8Hird/KGZt+h3t6IDeQDrkG/TBxYF+hWc53zj6cn/Yv0hQ+jfvz99+vRho4034aZ/3NDTxbIesO7QjRiw+BIf2LbhppvTu0/6Ub32x9fjpRefr3aq1ambbggqRNGB/Axgc9IEzgCzSOMIWB3WXHMwE+69lxkzXmX27Nncfts4nnvuuZ4ulrWgG8Zcw0abbdnTxSg1NbC0mqLbyD8RERvmWYKIiFdz30irw+prrMH+BxzIQQd+g0UWWYTBa69Nn969e7pY1mIuvegcevfuzTafrTqYntWpVytWtetUdI383TyaVwBIWo4OLnZW3vZ67tkN3TQ139pp51259IqrOP+ikSyxxJKssuqqPV0kayE3/u1axt85jiN+8qtST4zQClwjr+0U0jjky0v6FbAL8ONaB1fe9up+5MnLL7/MMsssw/Rp07jxH2O5eOSlPV0kaxET7r6D0X++gP879Rz69evf08Upv1aM0HUq7IYgSb2AzUhDOW5HeplujIiJ9ZzvQJ7s9/Wv8dqMGfTp04cjjzqGT2y2eU8XqUctqDcEnXD80Tx8/wRmvjaDJZdemq/tfzBXjDyfd995hwFLpIuga6+zHt85smY9ab7WjBuC7pn8Wt0xZ9PVl2ipsF/0nZ13RkSXIo8DuVWzoAZy61gzAvn4BgL5Ji0WyItuIx8raWffzWlmLa/EjeRFt5F/n9R3fI6kt0gvQUTE4gXna2bWkDLf2Vn0nJ2+X9jMSqHM7QaFBnJJ1UY5fA14Jg/XaGbWEhzIazsD2BB4OK+vR5otaBlJB0fE2ILzNzOrS5mbVoq+2DkF2CAiNoqIjUhz1T0CfAb4bcF5m5nVrcxjrRRdI/9Y5ewWEfGYpA0iYrI7sphZKylzRCo6kD8u6Uzgkry+O/CEpIWBdwvO28ysfiWO5EUH8v2AbwOHkV6m24EjSUHckzCbWcsocxt50d0PZ0s6FRhLGjjr8Yhoq4m/XmTeZmaNqGdS5XpJOg/YAXghItbN204AvgS8A/wL2D8iZlQ5dwppyO+5wJyI2Liz/Aq92ClpG+BJ4DRSD5YnJG1dZJ5mZl3S3Ds7LwC2b7ftBmDdiFgfeAI4poPzt42IofUEcSi+aeVE4HMR8TiApMHAKGCjgvM1M2tIM5tWImKcpI+221bZ3fou0miwTVF098O+bUEcICKeAPoWnKeZWcMa6X5YOXdCXoY3mN03gL/V2Bekcaom1Jtu0TXyCZLOBS7O63sBEwrO08ysYY3UxyvnTmg4H+lYYA4wssYhW0bENEnLAzdImhQR4zpKs+ga+cHAo6TJlw8FHsvbzMxaSzeMfihpX9JF0L2ixhjiETEt/32BNDHPpp2lW1iNPE8sMSFfsT2pqHzMzJqh6Dk7JW0PHAV8KiKqDqwvaVGgV0TMyo8/B/y8s7QLq5FHxHvAg5JWKSoPM7NmaWaFXNIo4E5gbUlTJR1A6r03gNRc8oCkP+ZjB0kak08dCNwu6UHgHuCvEfH3TvMreIagm4BNcoHeaNseETt2dq5nCLJqPEOQVdOMGYKeeP7NumNOM/JrpqIvdh5fcPpmZk3hOzvbkdSPdFFzTdIQtud6/HEza2VlHsevqBr5haTxVG4DvgCsQ+q1YmbWkhzIP2ydiFgPIPcjv6egfMzMmsJNKx/23yFqI2KOxx43s1ZX5jBVVCAfImlmfiygf14XEBGxeEH5mpl1SYnjeDGBPCJ6F5GumVlhShzJi+5+aGZWCm4jNzMruWZOLNHdHMjNzPDFTjOz+UB5I7kDuZkZrpGbmZVeieO4A7mZGbhGbmZWemW+A92B3MwMN62YmZVeiSvkDuRmZlDuOzsLm7PTzKxUmjhpp6TzJL0g6ZGKbUtLukHSk/nvUjXO3V7S45KeknR0PUV3IDczI92iX+9ShwuA7dttOxq4MSLWAm7M6x8gqTdwOu9PyLOnpHU6LXtdRTIzm8+pgX+diYhxwCvtNn+ZNHsa+e9Xqpy6KfBUREyOiHeAS/J5HXIgNzMjXeysf9FwSfdWLMPryGJgREwHyH+Xr3LMisCzFetT87YO+WKnmVmDImIEMKKApKtV96Ozk1wjNzOjsRp5Fz0vaYWUl1YAXqhyzFRg5Yr1lYBpnSXsQG5mRnPbyGu4Ftg3P94XuKbKMeOBtSStJmkhYI98XoccyM3MaG6vFUmjgDuBtSVNlXQA8Gvgs5KeBD6b15E0SNIYSJPVA4cA1wMTgcsi4tFO84votPmlR7w1p/N2IVvw/PvlN3u6CNaCBg9cZJ7v5pn19nt1x5wBC7fWfEK+2GlmRrnv7HQgNzPDY62YmZVeieO4A7mZGVDqSO5AbmYG9Cpx20rL9lqx90kanu8kM/svfy6sjfuRl0M94zjYgsefCwMcyM3MSs+B3Mys5BzIy8HtoFaNPxcG+GKnmVnpuUZuZlZyDuRmZiXnQN6OpJB0YsX6kZKOa1Lax0n6j6QHJD0iacdmpGutR9Lcivf5ckmL9HSZbP7lQP5hbwM7SVq2oPR/HxFDgV2B8yR94D2QNE93287r+Q3m1bu78iqh2RExNCLWBd4BDq7c2YzXrrte/+78TFnXOJB/2BxSb4DD2++QtKqkGyU9lP+ukrdfIOkUSf+UNFnSLp1lEhETc17LSrpF0v9KuhU4VNJ2ku6X9LCk8yQtnPMZJmmSpNtzfn/J24+TNELSWOAiSctJGi1pfF62zMd9KtcSH8jpD5C0gqRxFbXHrfKxe+b8H5H0m4rX4HVJP5d0N7D5PL7WC4rbgDUlbSPpZkl/Bh6W1E/S+fl1vl/StgCSFpF0Wf6cXSrpbkkb530feP0l7S3pnvz+nSWpd14uyO/dw5IOz+d+T9JjOd1L8ralJV2dt90laf28/QOfqZ540awBEeGlYgFeBxYHpgBLAEcCx+V91wH75sffAK7Ojy8ALid9Ma4DPFUj7eOAI/PjT5Dm4hNwC3BG3t6PNIv24Lx+EXBYxfbV8vZRwF8q0p0A9M/rfwY+mR+vAkysKP+W+fFipLF2jgCOzdt6AwOAQcC/geXyMTcBX8nHBLBbT79Prb4Ar+e/fUhTen0L2AZ4o+I9PAI4Pz/+WH7N++XP3Fl5+7qkL/yN27/+wP/k97RvXj8D2AfYCLihoixL5r/TgIXbbTsV+Fl+/GnggWqfKS+tvbhGXkVEzCQF0O+127U5KUgCXAx8smLf1RHxXkQ8BgzsIPnDJT0A/A7YPfL/GuDS/Hdt4OmIeCKvXwhsTfqPPjkins7bR7VL99qImJ0ffwY4LedzLbC4pAHAHcBJkr5H+o88hzRH4P75OsB6ETEL2AS4JSJezMeMzGUAmAuM7uD5WdI/v/73kgL0uXn7PRXv4SdJnyMiYhLwDDA4b78kb38EeKgi3crXfztS0B6f89oOWB2YDKwu6VRJ2wMz8/EPASMl7U36cmhfhpuAZSQtkfdVfqashbntq7Y/APcB53dwTGUn/LcrHgtA0q+ALwJEaheH1Eb+uyppvVF5bhWdDc32RsXjXsDmVf4T/lrSX4FhwF2SPhMR4yRtnct5saQTeP8/fjVvRcTcTspiuY28coPS6HqV71NX3uvK11/AhRFxzIcSkIYAnwe+A+xG+gX5RdIX8o7ATyR9vEZebZ/rN6rssxbkGnkNEfEKcBlwQMXmf5JmtQbYC7i9kzSOjXTBa2gDWU8CPippzbz+deDWvH11SR/N23fvII2xpAlcAZA0NP9dIyIejojfkGqKH5O0KvBCRJxNqjVuCNwNfErSsvmC2p65DNZc40ifIyQNJjWDPU76XO2Wt68DrFfj/BuBXSQtn49dOl/HWRboFRGjgZ8AG+aL6itHxM3AD4ElSc1rlWXYBngp/yK1EnGNvGMnUhEQSU0t50n6AfAisH+zM4yItyTtD1yeewuMB/4YEW9L+jbwd0kvAfd0kMz3gNMlPUR6j8eRek0cli+ozQUeA/5G+mL6gaR3SdcH9omI6ZKOAW4m1djGRMQ1zX6uxhnAHyU9TGrq2C+/z2cAF+b3735Sk8hr7U+OiMck/RgYmwP1u6Qa+GzgfL3fI+oY0vWPP+VmE5F+Gc7ITWrn57zeBPYt8PlaQXyLfolIWiwiXlf6jX468GRE/L6ny2XNlX8F9c1f6muQat6DI+KdHi6atSjXyMvlm5L2BRYi1dTO6uHyWDEWAW6W1JdUe/6Wg7h1xDVyM7OS88VOM7OScyA3Mys5B3Izs5JzIDczKzkHcjOzknMgNzMrOQdyM7OScyA3Mys5B3Izs5JzIDczKzkHcjOzknMgNzMrOQdyM7OScyA3Mys5B3L7AElzJT0g6RFJl0taZB7SukDSLvnxOXnaslrHbiNpiy7kMSVPbdY+34PabfuKpDH1lNWsbBzIrb3ZeZ7RdYF3SFPE/VeevaZhEXFgRDzWwSHbAA0H8hpG8f7cqm32yNvN5jsO5NaR24A1c235Zkl/Bh6W1FvSCZLGS3qorfar5DRJj0n6K7B8W0KSbpG0cX68vaT7JD0o6cY8ofTBwOH518BWkpaTNDrnMV7SlvncZSSNlXS/pLOoPgv8P0gTS6+Qz1kE+AxwtaSf5vQekTQiT5v3AZW1fEkbS7olP15U0nn5/PslfTlv/7ike3LZH5K0VjNefLN6OZBbVXni5y8AD+dNmwLHRsQ6wAHAaxGxCbAJaQq61YCvAmuTZn3/JlVq2JKWA84Gdo6IIcCuETEF+CNpQuChEXEbcHJe3wTYGTgnJ/Ez4PaI2AC4ljTz/AdExFzgSvJM9MCOwM0RMQs4LSI2yb84+gM7NPCyHAvclMu0LXCCpEVJX0InR8RQYGNgagNpms0zz9lp7fWX9EB+fBtwLikg3xMRT+ftnwPWr2hTXgJYC9gaGJUD6TRJN1VJfzNgXFtaEfFKjXJ8BlinosK8uKQBOY+d8rl/lfRqjfNHASeQvhD2AC7K27eV9EPSvJhLA48C19VIo73PATtKOjKv9yN9kdwJHCtpJeDKiHiyzvTMmsKB3NqbnWuW/5WD6RuVm4DvRsT17Y4bBnQ2CazqOAbSr8XNI2J2lbLUc/4dwAqShpC+iPaQ1A84A9g4Ip6VdBwpGLc3h/d/rVbuF+mXxOPtjp8o6W7gi8D1kg6MiGpfYmaFcNOKdcX1wLfyLO9IGpybGMaRAmbv3D69bZVz7wQ+lZtikLR03j4LGFBx3FjgkLYVSUPzw3HAXnnbF4ClqhUw0qzilwEXAmMi4i3eD8ovSVoMqNVLZQqwUX68c7vn/d22dnVJG+S/qwOTI+IUUnPP+jXSNSuEA7l1xTnAY8B9kh4BziL9ursKeJLUrn4mcGv7EyPiRWA4cKWkB4FL867rgK+2XewEvgdsnC8ePsb7vWeOB7aWdB+pqePfHZRzFDAEuCTnPYPUPv8wcDUwvsZ5xwMnS7oNmFux/RdAX+Ch/Lx/kbfvDjySm6Q+xvvNOGbdQqniYmZmZeUauZlZyTmQm5mVnAO5mVnJOZCbmZWcA7mZWck5kJuZlZwDuZlZyTmQm5mV3P8DlAJ69IBZLgkAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "cf_matrix = confusion_matrix(y_test_np.argmax(axis=1), y_c.argmax(axis=1)) \n",
    "#cf_matrix = confusion_matrix(y_test_np[:, 1], y_c[:, 1]) \n",
    "\n",
    "ax = sns.heatmap(cf_matrix, annot=True, cmap='Blues')\n",
    "\n",
    "ax.set_title('Seaborn Confusion Matrix with labels\\n\\n');\n",
    "ax.set_xlabel('\\nPredicted Values')\n",
    "ax.set_ylabel('Actual Values ');\n",
    "\n",
    "## Ticket labels - List must be in alphabetical order\n",
    "ax.xaxis.set_ticklabels(['Non-Progressor','Progressor'])\n",
    "ax.yaxis.set_ticklabels(['Non-Progressor','Progressor'])\n",
    "\n",
    "## Display the visualization of the Confusion Matrix.\n",
    "plt.savefig('M1_GRI_test.png')\n",
    "m2_eval_test = model_2_100.evaluate(X_test, y_test)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd5ecfab",
   "metadata": {},
   "source": [
    "200:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "a5f5afef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_10\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv1d_10 (Conv1D)          (None, 766, 64)           256       \n",
      "                                                                 \n",
      " max_pooling1d_10 (MaxPoolin  (None, 255, 64)          0         \n",
      " g1D)                                                            \n",
      "                                                                 \n",
      " dropout_10 (Dropout)        (None, 255, 64)           0         \n",
      "                                                                 \n",
      " flatten_10 (Flatten)        (None, 16320)             0         \n",
      "                                                                 \n",
      " dense_30 (Dense)            (None, 64)                1044544   \n",
      "                                                                 \n",
      " dense_31 (Dense)            (None, 16)                1040      \n",
      "                                                                 \n",
      " dense_32 (Dense)            (None, 2)                 34        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,045,874\n",
      "Trainable params: 1,045,874\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#create model2_nocall\n",
    "model_2_200 = Sequential()\n",
    "\n",
    "#add layers\n",
    "model_2_200.add(Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(768,1)))\n",
    "model_2_200.add(MaxPooling1D(pool_size=3))\n",
    "# model_1.add(Conv1D(filters=32, kernel_size=3, activation='relu'))\n",
    "model_2_200.add(Dropout(0.5))\n",
    "# model_1.add(MaxPooling1D(pool_size=2))\n",
    "model_2_200.add(Flatten())\n",
    "model_2_200.add(Dense(64, activation='relu'))\n",
    "model_2_200.add(Dense(16, activation='relu'))\n",
    "model_2_200.add(Dense(2, activation='softmax'))\n",
    "model_2_200.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "e5d0e012",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "early_stopping_monitor = EarlyStopping(\n",
    "    monitor='val_accuracy',\n",
    "    min_delta=0,\n",
    "    patience=200,\n",
    "    verbose=0,\n",
    "    mode='auto',\n",
    "    baseline=None,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "opt1 = keras.optimizers.Adam(learning_rate = 1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "cb77562f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "21/21 [==============================] - 1s 33ms/step - loss: 0.6942 - accuracy: 0.5093 - val_loss: 0.6745 - val_accuracy: 0.6604\n",
      "Epoch 2/500\n",
      "21/21 [==============================] - 1s 25ms/step - loss: 0.6932 - accuracy: 0.5031 - val_loss: 0.6650 - val_accuracy: 0.6604\n",
      "Epoch 3/500\n",
      "21/21 [==============================] - 1s 26ms/step - loss: 0.6900 - accuracy: 0.5265 - val_loss: 0.6747 - val_accuracy: 0.6604\n",
      "Epoch 4/500\n",
      "21/21 [==============================] - 1s 26ms/step - loss: 0.6867 - accuracy: 0.5576 - val_loss: 0.6681 - val_accuracy: 0.6792\n",
      "Epoch 5/500\n",
      "21/21 [==============================] - 1s 26ms/step - loss: 0.6836 - accuracy: 0.5966 - val_loss: 0.6535 - val_accuracy: 0.6792\n",
      "Epoch 6/500\n",
      "21/21 [==============================] - 1s 27ms/step - loss: 0.6802 - accuracy: 0.6090 - val_loss: 0.6541 - val_accuracy: 0.6604\n",
      "Epoch 7/500\n",
      "21/21 [==============================] - 1s 27ms/step - loss: 0.6796 - accuracy: 0.5841 - val_loss: 0.6741 - val_accuracy: 0.6792\n",
      "Epoch 8/500\n",
      "21/21 [==============================] - 1s 25ms/step - loss: 0.6775 - accuracy: 0.5810 - val_loss: 0.6409 - val_accuracy: 0.6604\n",
      "Epoch 9/500\n",
      "21/21 [==============================] - 1s 28ms/step - loss: 0.6695 - accuracy: 0.5966 - val_loss: 0.6990 - val_accuracy: 0.4906\n",
      "Epoch 10/500\n",
      "21/21 [==============================] - 1s 27ms/step - loss: 0.6704 - accuracy: 0.5857 - val_loss: 0.6425 - val_accuracy: 0.7170\n",
      "Epoch 11/500\n",
      "21/21 [==============================] - 1s 26ms/step - loss: 0.6612 - accuracy: 0.6028 - val_loss: 0.6448 - val_accuracy: 0.6981\n",
      "Epoch 12/500\n",
      "21/21 [==============================] - 1s 26ms/step - loss: 0.6593 - accuracy: 0.6262 - val_loss: 0.6267 - val_accuracy: 0.7170\n",
      "Epoch 13/500\n",
      "21/21 [==============================] - 1s 27ms/step - loss: 0.6566 - accuracy: 0.6075 - val_loss: 0.6496 - val_accuracy: 0.6981\n",
      "Epoch 14/500\n",
      "21/21 [==============================] - 1s 28ms/step - loss: 0.6540 - accuracy: 0.6199 - val_loss: 0.6139 - val_accuracy: 0.7358\n",
      "Epoch 15/500\n",
      "21/21 [==============================] - 1s 25ms/step - loss: 0.6545 - accuracy: 0.6106 - val_loss: 0.6403 - val_accuracy: 0.7358\n",
      "Epoch 16/500\n",
      "21/21 [==============================] - 1s 25ms/step - loss: 0.6524 - accuracy: 0.6308 - val_loss: 0.6244 - val_accuracy: 0.6981\n",
      "Epoch 17/500\n",
      "21/21 [==============================] - 1s 25ms/step - loss: 0.6500 - accuracy: 0.6246 - val_loss: 0.6235 - val_accuracy: 0.7170\n",
      "Epoch 18/500\n",
      "21/21 [==============================] - 1s 27ms/step - loss: 0.6486 - accuracy: 0.6262 - val_loss: 0.6185 - val_accuracy: 0.7170\n",
      "Epoch 19/500\n",
      "21/21 [==============================] - 1s 25ms/step - loss: 0.6423 - accuracy: 0.6246 - val_loss: 0.6233 - val_accuracy: 0.7170\n",
      "Epoch 20/500\n",
      "21/21 [==============================] - 1s 24ms/step - loss: 0.6456 - accuracy: 0.6371 - val_loss: 0.6051 - val_accuracy: 0.6981\n",
      "Epoch 21/500\n",
      "21/21 [==============================] - 1s 26ms/step - loss: 0.6404 - accuracy: 0.6246 - val_loss: 0.6712 - val_accuracy: 0.6226\n",
      "Epoch 22/500\n",
      "21/21 [==============================] - 1s 26ms/step - loss: 0.6474 - accuracy: 0.6199 - val_loss: 0.6083 - val_accuracy: 0.7358\n",
      "Epoch 23/500\n",
      "21/21 [==============================] - 1s 27ms/step - loss: 0.6392 - accuracy: 0.6231 - val_loss: 0.6227 - val_accuracy: 0.6981\n",
      "Epoch 24/500\n",
      "21/21 [==============================] - 1s 26ms/step - loss: 0.6340 - accuracy: 0.6511 - val_loss: 0.6291 - val_accuracy: 0.7170\n",
      "Epoch 25/500\n",
      "21/21 [==============================] - 1s 24ms/step - loss: 0.6294 - accuracy: 0.6495 - val_loss: 0.6310 - val_accuracy: 0.7358\n",
      "Epoch 26/500\n",
      "21/21 [==============================] - 1s 29ms/step - loss: 0.6272 - accuracy: 0.6542 - val_loss: 0.6130 - val_accuracy: 0.7170\n",
      "Epoch 27/500\n",
      "21/21 [==============================] - 1s 26ms/step - loss: 0.6287 - accuracy: 0.6293 - val_loss: 0.6311 - val_accuracy: 0.7358\n",
      "Epoch 28/500\n",
      "21/21 [==============================] - 1s 25ms/step - loss: 0.6229 - accuracy: 0.6620 - val_loss: 0.6066 - val_accuracy: 0.6981\n",
      "Epoch 29/500\n",
      "21/21 [==============================] - 1s 25ms/step - loss: 0.6280 - accuracy: 0.6495 - val_loss: 0.5996 - val_accuracy: 0.6415\n",
      "Epoch 30/500\n",
      "21/21 [==============================] - 1s 24ms/step - loss: 0.6421 - accuracy: 0.6090 - val_loss: 0.6590 - val_accuracy: 0.6604\n",
      "Epoch 31/500\n",
      "21/21 [==============================] - 0s 23ms/step - loss: 0.6198 - accuracy: 0.6464 - val_loss: 0.6040 - val_accuracy: 0.6792\n",
      "Epoch 32/500\n",
      "21/21 [==============================] - 1s 24ms/step - loss: 0.6157 - accuracy: 0.6636 - val_loss: 0.6362 - val_accuracy: 0.6792\n",
      "Epoch 33/500\n",
      "21/21 [==============================] - 1s 24ms/step - loss: 0.6099 - accuracy: 0.6745 - val_loss: 0.6097 - val_accuracy: 0.7170\n",
      "Epoch 34/500\n",
      "21/21 [==============================] - 1s 26ms/step - loss: 0.6086 - accuracy: 0.6760 - val_loss: 0.6071 - val_accuracy: 0.6981\n",
      "Epoch 35/500\n",
      "21/21 [==============================] - 0s 24ms/step - loss: 0.6080 - accuracy: 0.6682 - val_loss: 0.6322 - val_accuracy: 0.7170\n",
      "Epoch 36/500\n",
      "21/21 [==============================] - 1s 25ms/step - loss: 0.6048 - accuracy: 0.6620 - val_loss: 0.6098 - val_accuracy: 0.7170\n",
      "Epoch 37/500\n",
      "21/21 [==============================] - 0s 24ms/step - loss: 0.6101 - accuracy: 0.6542 - val_loss: 0.6298 - val_accuracy: 0.7170\n",
      "Epoch 38/500\n",
      "21/21 [==============================] - 1s 25ms/step - loss: 0.6002 - accuracy: 0.6636 - val_loss: 0.6135 - val_accuracy: 0.7358\n",
      "Epoch 39/500\n",
      "21/21 [==============================] - 1s 26ms/step - loss: 0.5960 - accuracy: 0.6869 - val_loss: 0.6183 - val_accuracy: 0.7170\n",
      "Epoch 40/500\n",
      "21/21 [==============================] - 0s 22ms/step - loss: 0.6007 - accuracy: 0.6682 - val_loss: 0.6263 - val_accuracy: 0.6981\n",
      "Epoch 41/500\n",
      "21/21 [==============================] - 0s 24ms/step - loss: 0.5986 - accuracy: 0.6713 - val_loss: 0.6175 - val_accuracy: 0.6981\n",
      "Epoch 42/500\n",
      "21/21 [==============================] - 1s 26ms/step - loss: 0.6000 - accuracy: 0.6822 - val_loss: 0.6107 - val_accuracy: 0.6792\n",
      "Epoch 43/500\n",
      "21/21 [==============================] - 0s 24ms/step - loss: 0.5909 - accuracy: 0.6807 - val_loss: 0.6295 - val_accuracy: 0.6981\n",
      "Epoch 44/500\n",
      "21/21 [==============================] - 0s 24ms/step - loss: 0.5849 - accuracy: 0.6947 - val_loss: 0.6334 - val_accuracy: 0.7170\n",
      "Epoch 45/500\n",
      "21/21 [==============================] - 1s 25ms/step - loss: 0.5773 - accuracy: 0.7118 - val_loss: 0.6175 - val_accuracy: 0.6981\n",
      "Epoch 46/500\n",
      "21/21 [==============================] - 0s 23ms/step - loss: 0.5778 - accuracy: 0.6978 - val_loss: 0.6003 - val_accuracy: 0.6792\n",
      "Epoch 47/500\n",
      "21/21 [==============================] - 0s 23ms/step - loss: 0.5877 - accuracy: 0.6900 - val_loss: 0.6477 - val_accuracy: 0.6226\n",
      "Epoch 48/500\n",
      "21/21 [==============================] - 0s 24ms/step - loss: 0.5771 - accuracy: 0.7227 - val_loss: 0.6249 - val_accuracy: 0.7170\n",
      "Epoch 49/500\n",
      "21/21 [==============================] - 0s 24ms/step - loss: 0.5703 - accuracy: 0.7072 - val_loss: 0.6395 - val_accuracy: 0.6981\n",
      "Epoch 50/500\n",
      "21/21 [==============================] - 0s 23ms/step - loss: 0.5642 - accuracy: 0.7196 - val_loss: 0.6415 - val_accuracy: 0.6415\n",
      "Epoch 51/500\n",
      "21/21 [==============================] - 1s 25ms/step - loss: 0.5610 - accuracy: 0.7243 - val_loss: 0.6203 - val_accuracy: 0.7170\n",
      "Epoch 52/500\n",
      "21/21 [==============================] - 1s 26ms/step - loss: 0.5646 - accuracy: 0.7072 - val_loss: 0.6318 - val_accuracy: 0.6981\n",
      "Epoch 53/500\n",
      "21/21 [==============================] - 0s 23ms/step - loss: 0.5583 - accuracy: 0.7321 - val_loss: 0.6061 - val_accuracy: 0.6981\n",
      "Epoch 54/500\n",
      "21/21 [==============================] - 1s 24ms/step - loss: 0.5579 - accuracy: 0.7103 - val_loss: 0.6792 - val_accuracy: 0.5849\n",
      "Epoch 55/500\n",
      "21/21 [==============================] - 1s 26ms/step - loss: 0.5556 - accuracy: 0.7305 - val_loss: 0.6303 - val_accuracy: 0.6604\n",
      "Epoch 56/500\n",
      "21/21 [==============================] - 1s 25ms/step - loss: 0.5516 - accuracy: 0.7352 - val_loss: 0.6280 - val_accuracy: 0.6792\n",
      "Epoch 57/500\n",
      "21/21 [==============================] - 1s 25ms/step - loss: 0.5458 - accuracy: 0.7290 - val_loss: 0.6514 - val_accuracy: 0.6415\n",
      "Epoch 58/500\n",
      "21/21 [==============================] - 0s 24ms/step - loss: 0.5360 - accuracy: 0.7414 - val_loss: 0.6935 - val_accuracy: 0.6038\n",
      "Epoch 59/500\n",
      "21/21 [==============================] - 1s 26ms/step - loss: 0.5579 - accuracy: 0.7072 - val_loss: 0.6300 - val_accuracy: 0.6604\n",
      "Epoch 60/500\n",
      "21/21 [==============================] - 1s 25ms/step - loss: 0.5345 - accuracy: 0.7477 - val_loss: 0.6492 - val_accuracy: 0.6604\n",
      "Epoch 61/500\n",
      "21/21 [==============================] - 1s 26ms/step - loss: 0.5297 - accuracy: 0.7477 - val_loss: 0.6427 - val_accuracy: 0.6604\n",
      "Epoch 62/500\n",
      "21/21 [==============================] - 1s 25ms/step - loss: 0.5300 - accuracy: 0.7399 - val_loss: 0.6514 - val_accuracy: 0.6604\n",
      "Epoch 63/500\n",
      "21/21 [==============================] - 1s 25ms/step - loss: 0.5259 - accuracy: 0.7492 - val_loss: 0.6352 - val_accuracy: 0.6604\n",
      "Epoch 64/500\n",
      "21/21 [==============================] - 0s 23ms/step - loss: 0.5184 - accuracy: 0.7539 - val_loss: 0.6790 - val_accuracy: 0.6038\n",
      "Epoch 65/500\n",
      "21/21 [==============================] - 1s 26ms/step - loss: 0.5179 - accuracy: 0.7461 - val_loss: 0.6470 - val_accuracy: 0.6415\n",
      "Epoch 66/500\n",
      "21/21 [==============================] - 1s 25ms/step - loss: 0.5114 - accuracy: 0.7586 - val_loss: 0.6681 - val_accuracy: 0.6415\n",
      "Epoch 67/500\n",
      "21/21 [==============================] - 1s 24ms/step - loss: 0.5092 - accuracy: 0.7773 - val_loss: 0.6650 - val_accuracy: 0.6226\n",
      "Epoch 68/500\n",
      "21/21 [==============================] - 0s 23ms/step - loss: 0.5016 - accuracy: 0.7679 - val_loss: 0.6603 - val_accuracy: 0.6226\n",
      "Epoch 69/500\n",
      "21/21 [==============================] - 1s 25ms/step - loss: 0.5005 - accuracy: 0.8006 - val_loss: 0.6799 - val_accuracy: 0.6415\n",
      "Epoch 70/500\n",
      "21/21 [==============================] - 1s 25ms/step - loss: 0.5039 - accuracy: 0.7804 - val_loss: 0.6811 - val_accuracy: 0.6604\n",
      "Epoch 71/500\n",
      "21/21 [==============================] - 0s 22ms/step - loss: 0.4954 - accuracy: 0.7850 - val_loss: 0.6708 - val_accuracy: 0.6415\n",
      "Epoch 72/500\n",
      "21/21 [==============================] - 1s 24ms/step - loss: 0.5020 - accuracy: 0.7773 - val_loss: 0.6475 - val_accuracy: 0.6981\n",
      "Epoch 73/500\n",
      "21/21 [==============================] - 0s 24ms/step - loss: 0.4942 - accuracy: 0.7757 - val_loss: 0.6970 - val_accuracy: 0.6415\n",
      "Epoch 74/500\n",
      "21/21 [==============================] - 0s 24ms/step - loss: 0.4848 - accuracy: 0.7897 - val_loss: 0.6979 - val_accuracy: 0.6604\n",
      "Epoch 75/500\n",
      "21/21 [==============================] - 0s 23ms/step - loss: 0.4898 - accuracy: 0.7710 - val_loss: 0.6774 - val_accuracy: 0.6226\n",
      "Epoch 76/500\n",
      "21/21 [==============================] - 0s 23ms/step - loss: 0.4849 - accuracy: 0.7866 - val_loss: 0.6881 - val_accuracy: 0.6226\n",
      "Epoch 77/500\n",
      "21/21 [==============================] - 0s 23ms/step - loss: 0.4734 - accuracy: 0.7835 - val_loss: 0.6596 - val_accuracy: 0.6604\n",
      "Epoch 78/500\n",
      "21/21 [==============================] - 0s 21ms/step - loss: 0.4812 - accuracy: 0.7991 - val_loss: 0.6898 - val_accuracy: 0.6226\n",
      "Epoch 79/500\n",
      "21/21 [==============================] - 0s 23ms/step - loss: 0.4732 - accuracy: 0.7882 - val_loss: 0.7020 - val_accuracy: 0.6226\n",
      "Epoch 80/500\n",
      "21/21 [==============================] - 0s 24ms/step - loss: 0.4835 - accuracy: 0.7897 - val_loss: 0.6593 - val_accuracy: 0.6226\n",
      "Epoch 81/500\n",
      "21/21 [==============================] - 1s 25ms/step - loss: 0.4790 - accuracy: 0.7835 - val_loss: 0.7341 - val_accuracy: 0.6038\n",
      "Epoch 82/500\n",
      "21/21 [==============================] - 1s 25ms/step - loss: 0.4657 - accuracy: 0.7835 - val_loss: 0.6875 - val_accuracy: 0.6038\n",
      "Epoch 83/500\n",
      "21/21 [==============================] - 1s 25ms/step - loss: 0.4616 - accuracy: 0.8069 - val_loss: 0.6994 - val_accuracy: 0.6038\n",
      "Epoch 84/500\n",
      "21/21 [==============================] - 0s 24ms/step - loss: 0.4643 - accuracy: 0.7960 - val_loss: 0.7240 - val_accuracy: 0.6226\n",
      "Epoch 85/500\n",
      "21/21 [==============================] - 0s 24ms/step - loss: 0.4588 - accuracy: 0.7928 - val_loss: 0.6811 - val_accuracy: 0.6038\n",
      "Epoch 86/500\n",
      "21/21 [==============================] - 0s 23ms/step - loss: 0.4564 - accuracy: 0.7913 - val_loss: 0.7107 - val_accuracy: 0.6038\n",
      "Epoch 87/500\n",
      "21/21 [==============================] - 0s 23ms/step - loss: 0.4505 - accuracy: 0.8287 - val_loss: 0.6957 - val_accuracy: 0.6226\n",
      "Epoch 88/500\n",
      "21/21 [==============================] - 1s 25ms/step - loss: 0.4479 - accuracy: 0.8022 - val_loss: 0.7077 - val_accuracy: 0.6226\n",
      "Epoch 89/500\n",
      "21/21 [==============================] - 0s 22ms/step - loss: 0.4449 - accuracy: 0.7960 - val_loss: 0.7112 - val_accuracy: 0.6226\n",
      "Epoch 90/500\n",
      "21/21 [==============================] - 0s 23ms/step - loss: 0.4380 - accuracy: 0.8115 - val_loss: 0.7136 - val_accuracy: 0.6038\n",
      "Epoch 91/500\n",
      "21/21 [==============================] - 0s 23ms/step - loss: 0.4387 - accuracy: 0.8115 - val_loss: 0.6886 - val_accuracy: 0.6226\n",
      "Epoch 92/500\n",
      "21/21 [==============================] - 0s 23ms/step - loss: 0.4292 - accuracy: 0.8271 - val_loss: 0.6972 - val_accuracy: 0.6038\n",
      "Epoch 93/500\n",
      "21/21 [==============================] - 0s 23ms/step - loss: 0.4246 - accuracy: 0.8302 - val_loss: 0.7797 - val_accuracy: 0.6038\n",
      "Epoch 94/500\n",
      "21/21 [==============================] - 0s 22ms/step - loss: 0.4356 - accuracy: 0.8069 - val_loss: 0.6933 - val_accuracy: 0.6415\n",
      "Epoch 95/500\n",
      "21/21 [==============================] - 0s 22ms/step - loss: 0.4240 - accuracy: 0.8146 - val_loss: 0.7026 - val_accuracy: 0.6226\n",
      "Epoch 96/500\n",
      "21/21 [==============================] - 0s 23ms/step - loss: 0.4259 - accuracy: 0.8178 - val_loss: 0.7212 - val_accuracy: 0.6038\n",
      "Epoch 97/500\n",
      "21/21 [==============================] - 0s 24ms/step - loss: 0.4180 - accuracy: 0.8162 - val_loss: 0.7137 - val_accuracy: 0.5849\n",
      "Epoch 98/500\n",
      "21/21 [==============================] - 0s 22ms/step - loss: 0.4156 - accuracy: 0.8349 - val_loss: 0.7407 - val_accuracy: 0.6038\n",
      "Epoch 99/500\n",
      "21/21 [==============================] - 0s 23ms/step - loss: 0.4164 - accuracy: 0.8178 - val_loss: 0.8257 - val_accuracy: 0.5283\n",
      "Epoch 100/500\n",
      "21/21 [==============================] - 0s 22ms/step - loss: 0.4095 - accuracy: 0.8178 - val_loss: 0.7199 - val_accuracy: 0.5849\n",
      "Epoch 101/500\n",
      "21/21 [==============================] - 0s 22ms/step - loss: 0.4006 - accuracy: 0.8489 - val_loss: 0.7343 - val_accuracy: 0.6038\n",
      "Epoch 102/500\n",
      "21/21 [==============================] - 0s 23ms/step - loss: 0.4141 - accuracy: 0.8209 - val_loss: 0.7256 - val_accuracy: 0.6038\n",
      "Epoch 103/500\n",
      "21/21 [==============================] - 1s 24ms/step - loss: 0.4114 - accuracy: 0.8255 - val_loss: 0.7285 - val_accuracy: 0.6226\n",
      "Epoch 104/500\n",
      "21/21 [==============================] - 1s 25ms/step - loss: 0.3910 - accuracy: 0.8380 - val_loss: 0.7441 - val_accuracy: 0.6038\n",
      "Epoch 105/500\n",
      "21/21 [==============================] - 1s 24ms/step - loss: 0.3899 - accuracy: 0.8396 - val_loss: 0.7665 - val_accuracy: 0.5849\n",
      "Epoch 106/500\n",
      "21/21 [==============================] - 0s 21ms/step - loss: 0.3973 - accuracy: 0.8271 - val_loss: 0.7293 - val_accuracy: 0.5849\n",
      "Epoch 107/500\n",
      "21/21 [==============================] - 0s 23ms/step - loss: 0.3884 - accuracy: 0.8489 - val_loss: 0.7129 - val_accuracy: 0.6038\n",
      "Epoch 108/500\n",
      "21/21 [==============================] - 1s 24ms/step - loss: 0.3908 - accuracy: 0.8474 - val_loss: 0.7163 - val_accuracy: 0.6226\n",
      "Epoch 109/500\n",
      "21/21 [==============================] - 0s 23ms/step - loss: 0.3812 - accuracy: 0.8489 - val_loss: 0.7674 - val_accuracy: 0.5849\n",
      "Epoch 110/500\n",
      "21/21 [==============================] - 0s 22ms/step - loss: 0.3869 - accuracy: 0.8411 - val_loss: 0.7581 - val_accuracy: 0.5849\n",
      "Epoch 111/500\n",
      "21/21 [==============================] - 1s 24ms/step - loss: 0.3725 - accuracy: 0.8629 - val_loss: 0.7469 - val_accuracy: 0.6038\n",
      "Epoch 112/500\n",
      "21/21 [==============================] - 0s 23ms/step - loss: 0.3762 - accuracy: 0.8474 - val_loss: 0.7654 - val_accuracy: 0.5660\n",
      "Epoch 113/500\n",
      "21/21 [==============================] - 0s 22ms/step - loss: 0.3702 - accuracy: 0.8551 - val_loss: 0.7728 - val_accuracy: 0.5849\n",
      "Epoch 114/500\n",
      "21/21 [==============================] - 0s 23ms/step - loss: 0.3694 - accuracy: 0.8520 - val_loss: 0.7515 - val_accuracy: 0.6038\n",
      "Epoch 115/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21/21 [==============================] - 0s 24ms/step - loss: 0.3592 - accuracy: 0.8769 - val_loss: 0.7507 - val_accuracy: 0.6038\n",
      "Epoch 116/500\n",
      "21/21 [==============================] - 1s 24ms/step - loss: 0.3562 - accuracy: 0.8551 - val_loss: 0.7526 - val_accuracy: 0.6415\n",
      "Epoch 117/500\n",
      "21/21 [==============================] - 1s 25ms/step - loss: 0.3520 - accuracy: 0.8567 - val_loss: 0.7707 - val_accuracy: 0.5849\n",
      "Epoch 118/500\n",
      "21/21 [==============================] - 0s 24ms/step - loss: 0.3532 - accuracy: 0.8707 - val_loss: 0.8014 - val_accuracy: 0.5849\n",
      "Epoch 119/500\n",
      "21/21 [==============================] - 0s 24ms/step - loss: 0.3477 - accuracy: 0.8676 - val_loss: 0.7586 - val_accuracy: 0.6226\n",
      "Epoch 120/500\n",
      "21/21 [==============================] - 1s 25ms/step - loss: 0.3616 - accuracy: 0.8614 - val_loss: 0.7743 - val_accuracy: 0.6038\n",
      "Epoch 121/500\n",
      "21/21 [==============================] - 1s 25ms/step - loss: 0.3450 - accuracy: 0.8723 - val_loss: 0.7550 - val_accuracy: 0.6226\n",
      "Epoch 122/500\n",
      "21/21 [==============================] - 1s 24ms/step - loss: 0.3450 - accuracy: 0.8536 - val_loss: 0.7551 - val_accuracy: 0.6038\n",
      "Epoch 123/500\n",
      "21/21 [==============================] - 1s 25ms/step - loss: 0.3356 - accuracy: 0.8692 - val_loss: 0.8204 - val_accuracy: 0.5283\n",
      "Epoch 124/500\n",
      "21/21 [==============================] - 1s 24ms/step - loss: 0.3443 - accuracy: 0.8551 - val_loss: 0.7749 - val_accuracy: 0.6038\n",
      "Epoch 125/500\n",
      "21/21 [==============================] - 1s 24ms/step - loss: 0.3362 - accuracy: 0.8847 - val_loss: 0.8030 - val_accuracy: 0.5472\n",
      "Epoch 126/500\n",
      "21/21 [==============================] - 0s 23ms/step - loss: 0.3277 - accuracy: 0.8925 - val_loss: 0.8226 - val_accuracy: 0.5472\n",
      "Epoch 127/500\n",
      "21/21 [==============================] - 1s 24ms/step - loss: 0.3335 - accuracy: 0.8676 - val_loss: 0.7904 - val_accuracy: 0.5660\n",
      "Epoch 128/500\n",
      "21/21 [==============================] - 1s 26ms/step - loss: 0.3353 - accuracy: 0.8676 - val_loss: 0.7604 - val_accuracy: 0.6038\n",
      "Epoch 129/500\n",
      "21/21 [==============================] - 0s 22ms/step - loss: 0.3277 - accuracy: 0.8801 - val_loss: 0.7891 - val_accuracy: 0.6226\n",
      "Epoch 130/500\n",
      "21/21 [==============================] - 1s 24ms/step - loss: 0.3344 - accuracy: 0.8723 - val_loss: 0.8045 - val_accuracy: 0.5849\n",
      "Epoch 131/500\n",
      "21/21 [==============================] - 0s 24ms/step - loss: 0.3394 - accuracy: 0.8832 - val_loss: 0.7843 - val_accuracy: 0.5849\n",
      "Epoch 132/500\n",
      "21/21 [==============================] - 1s 26ms/step - loss: 0.3136 - accuracy: 0.8863 - val_loss: 0.7944 - val_accuracy: 0.5472\n",
      "Epoch 133/500\n",
      "21/21 [==============================] - 1s 26ms/step - loss: 0.3049 - accuracy: 0.8941 - val_loss: 0.8170 - val_accuracy: 0.5660\n",
      "Epoch 134/500\n",
      "21/21 [==============================] - 1s 25ms/step - loss: 0.3310 - accuracy: 0.8769 - val_loss: 0.8937 - val_accuracy: 0.5094\n",
      "Epoch 135/500\n",
      "21/21 [==============================] - 1s 25ms/step - loss: 0.3151 - accuracy: 0.8832 - val_loss: 0.8245 - val_accuracy: 0.5849\n",
      "Epoch 136/500\n",
      "21/21 [==============================] - 1s 25ms/step - loss: 0.3099 - accuracy: 0.8894 - val_loss: 0.8228 - val_accuracy: 0.5472\n",
      "Epoch 137/500\n",
      "21/21 [==============================] - 0s 23ms/step - loss: 0.2960 - accuracy: 0.8956 - val_loss: 0.8065 - val_accuracy: 0.5472\n",
      "Epoch 138/500\n",
      "21/21 [==============================] - 0s 24ms/step - loss: 0.3143 - accuracy: 0.8863 - val_loss: 0.8050 - val_accuracy: 0.5660\n",
      "Epoch 139/500\n",
      "21/21 [==============================] - 0s 23ms/step - loss: 0.3017 - accuracy: 0.9050 - val_loss: 0.8642 - val_accuracy: 0.5283\n",
      "Epoch 140/500\n",
      "21/21 [==============================] - 0s 23ms/step - loss: 0.2937 - accuracy: 0.9019 - val_loss: 0.8799 - val_accuracy: 0.5283\n",
      "Epoch 141/500\n",
      "21/21 [==============================] - 0s 23ms/step - loss: 0.3230 - accuracy: 0.8598 - val_loss: 0.8277 - val_accuracy: 0.5849\n",
      "Epoch 142/500\n",
      "21/21 [==============================] - 1s 25ms/step - loss: 0.2918 - accuracy: 0.8941 - val_loss: 0.8039 - val_accuracy: 0.5849\n",
      "Epoch 143/500\n",
      "21/21 [==============================] - 0s 23ms/step - loss: 0.2852 - accuracy: 0.8972 - val_loss: 0.8626 - val_accuracy: 0.5472\n",
      "Epoch 144/500\n",
      "21/21 [==============================] - 0s 23ms/step - loss: 0.2923 - accuracy: 0.8879 - val_loss: 0.8725 - val_accuracy: 0.5094\n",
      "Epoch 145/500\n",
      "21/21 [==============================] - 0s 23ms/step - loss: 0.2778 - accuracy: 0.8988 - val_loss: 0.8209 - val_accuracy: 0.6415\n",
      "Epoch 146/500\n",
      "21/21 [==============================] - 1s 26ms/step - loss: 0.2765 - accuracy: 0.9081 - val_loss: 0.8111 - val_accuracy: 0.6038\n",
      "Epoch 147/500\n",
      "21/21 [==============================] - 1s 24ms/step - loss: 0.2696 - accuracy: 0.9081 - val_loss: 0.8167 - val_accuracy: 0.5849\n",
      "Epoch 148/500\n",
      "21/21 [==============================] - 0s 24ms/step - loss: 0.2738 - accuracy: 0.9206 - val_loss: 0.8335 - val_accuracy: 0.6226\n",
      "Epoch 149/500\n",
      "21/21 [==============================] - 1s 25ms/step - loss: 0.2727 - accuracy: 0.9019 - val_loss: 0.8232 - val_accuracy: 0.5849\n",
      "Epoch 150/500\n",
      "21/21 [==============================] - 0s 24ms/step - loss: 0.2689 - accuracy: 0.9097 - val_loss: 0.8497 - val_accuracy: 0.6226\n",
      "Epoch 151/500\n",
      "21/21 [==============================] - 0s 23ms/step - loss: 0.2690 - accuracy: 0.8972 - val_loss: 0.8340 - val_accuracy: 0.6038\n",
      "Epoch 152/500\n",
      "21/21 [==============================] - 0s 22ms/step - loss: 0.2878 - accuracy: 0.8894 - val_loss: 0.8100 - val_accuracy: 0.6415\n",
      "Epoch 153/500\n",
      "21/21 [==============================] - 1s 25ms/step - loss: 0.2741 - accuracy: 0.8988 - val_loss: 0.8484 - val_accuracy: 0.5849\n",
      "Epoch 154/500\n",
      "21/21 [==============================] - 0s 24ms/step - loss: 0.3187 - accuracy: 0.8629 - val_loss: 0.9462 - val_accuracy: 0.5094\n",
      "Epoch 155/500\n",
      "21/21 [==============================] - 0s 22ms/step - loss: 0.2951 - accuracy: 0.8832 - val_loss: 0.9016 - val_accuracy: 0.5472\n",
      "Epoch 156/500\n",
      "21/21 [==============================] - 1s 24ms/step - loss: 0.2706 - accuracy: 0.9019 - val_loss: 0.8154 - val_accuracy: 0.6415\n",
      "Epoch 157/500\n",
      "21/21 [==============================] - 0s 23ms/step - loss: 0.2575 - accuracy: 0.9050 - val_loss: 0.8254 - val_accuracy: 0.5849\n",
      "Epoch 158/500\n",
      "21/21 [==============================] - 1s 24ms/step - loss: 0.2577 - accuracy: 0.9190 - val_loss: 0.8428 - val_accuracy: 0.6038\n",
      "Epoch 159/500\n",
      "21/21 [==============================] - 1s 25ms/step - loss: 0.2489 - accuracy: 0.9143 - val_loss: 0.8594 - val_accuracy: 0.6038\n",
      "Epoch 160/500\n",
      "21/21 [==============================] - 1s 24ms/step - loss: 0.2511 - accuracy: 0.9112 - val_loss: 0.8425 - val_accuracy: 0.5660\n",
      "Epoch 161/500\n",
      "21/21 [==============================] - 0s 24ms/step - loss: 0.2636 - accuracy: 0.8956 - val_loss: 0.8443 - val_accuracy: 0.5849\n",
      "Epoch 162/500\n",
      "21/21 [==============================] - 0s 23ms/step - loss: 0.2724 - accuracy: 0.9143 - val_loss: 0.8512 - val_accuracy: 0.5849\n",
      "Epoch 163/500\n",
      "21/21 [==============================] - 1s 24ms/step - loss: 0.2483 - accuracy: 0.9143 - val_loss: 0.8467 - val_accuracy: 0.5849\n",
      "Epoch 164/500\n",
      "21/21 [==============================] - 0s 22ms/step - loss: 0.2446 - accuracy: 0.9283 - val_loss: 0.8765 - val_accuracy: 0.6038\n",
      "Epoch 165/500\n",
      "21/21 [==============================] - 0s 23ms/step - loss: 0.2454 - accuracy: 0.9206 - val_loss: 0.8506 - val_accuracy: 0.5660\n",
      "Epoch 166/500\n",
      "21/21 [==============================] - 0s 23ms/step - loss: 0.2329 - accuracy: 0.9315 - val_loss: 0.8416 - val_accuracy: 0.6038\n",
      "Epoch 167/500\n",
      "21/21 [==============================] - 1s 26ms/step - loss: 0.2280 - accuracy: 0.9299 - val_loss: 0.8719 - val_accuracy: 0.6415\n",
      "Epoch 168/500\n",
      "21/21 [==============================] - 1s 24ms/step - loss: 0.2439 - accuracy: 0.9143 - val_loss: 0.8488 - val_accuracy: 0.5849\n",
      "Epoch 169/500\n",
      "21/21 [==============================] - 1s 26ms/step - loss: 0.2311 - accuracy: 0.9221 - val_loss: 0.8666 - val_accuracy: 0.5849\n",
      "Epoch 170/500\n",
      "21/21 [==============================] - 1s 26ms/step - loss: 0.2271 - accuracy: 0.9283 - val_loss: 0.8963 - val_accuracy: 0.6038\n",
      "Epoch 171/500\n",
      "21/21 [==============================] - 1s 24ms/step - loss: 0.2395 - accuracy: 0.9252 - val_loss: 0.8881 - val_accuracy: 0.5660\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 172/500\n",
      "21/21 [==============================] - 1s 24ms/step - loss: 0.2277 - accuracy: 0.9237 - val_loss: 0.8629 - val_accuracy: 0.6038\n",
      "Epoch 173/500\n",
      "21/21 [==============================] - 1s 24ms/step - loss: 0.2335 - accuracy: 0.9237 - val_loss: 0.9055 - val_accuracy: 0.5849\n",
      "Epoch 174/500\n",
      "21/21 [==============================] - 1s 25ms/step - loss: 0.2330 - accuracy: 0.9299 - val_loss: 0.9280 - val_accuracy: 0.6038\n",
      "Epoch 175/500\n",
      "21/21 [==============================] - 1s 26ms/step - loss: 0.2543 - accuracy: 0.9003 - val_loss: 0.9044 - val_accuracy: 0.5660\n",
      "Epoch 176/500\n",
      "21/21 [==============================] - 1s 26ms/step - loss: 0.2301 - accuracy: 0.9330 - val_loss: 0.9053 - val_accuracy: 0.5660\n",
      "Epoch 177/500\n",
      "21/21 [==============================] - 1s 27ms/step - loss: 0.2123 - accuracy: 0.9361 - val_loss: 0.9606 - val_accuracy: 0.5472\n",
      "Epoch 178/500\n",
      "21/21 [==============================] - 1s 25ms/step - loss: 0.2231 - accuracy: 0.9237 - val_loss: 0.8932 - val_accuracy: 0.5660\n",
      "Epoch 179/500\n",
      "21/21 [==============================] - 0s 24ms/step - loss: 0.2272 - accuracy: 0.9268 - val_loss: 0.9006 - val_accuracy: 0.5660\n",
      "Epoch 180/500\n",
      "21/21 [==============================] - 0s 24ms/step - loss: 0.2138 - accuracy: 0.9330 - val_loss: 0.8997 - val_accuracy: 0.5660\n",
      "Epoch 181/500\n",
      "21/21 [==============================] - 0s 22ms/step - loss: 0.2166 - accuracy: 0.9346 - val_loss: 0.9258 - val_accuracy: 0.6038\n",
      "Epoch 182/500\n",
      "21/21 [==============================] - 0s 22ms/step - loss: 0.2139 - accuracy: 0.9315 - val_loss: 0.9091 - val_accuracy: 0.6038\n",
      "Epoch 183/500\n",
      "21/21 [==============================] - 1s 25ms/step - loss: 0.2167 - accuracy: 0.9346 - val_loss: 0.9318 - val_accuracy: 0.5849\n",
      "Epoch 184/500\n",
      "21/21 [==============================] - 0s 22ms/step - loss: 0.2152 - accuracy: 0.9299 - val_loss: 0.9837 - val_accuracy: 0.5094\n",
      "Epoch 185/500\n",
      "21/21 [==============================] - 0s 22ms/step - loss: 0.2261 - accuracy: 0.9128 - val_loss: 0.9440 - val_accuracy: 0.5472\n",
      "Epoch 186/500\n",
      "21/21 [==============================] - 1s 24ms/step - loss: 0.2055 - accuracy: 0.9237 - val_loss: 0.9464 - val_accuracy: 0.5849\n",
      "Epoch 187/500\n",
      "21/21 [==============================] - 1s 25ms/step - loss: 0.2061 - accuracy: 0.9346 - val_loss: 0.9813 - val_accuracy: 0.5849\n",
      "Epoch 188/500\n",
      "21/21 [==============================] - 0s 23ms/step - loss: 0.2011 - accuracy: 0.9455 - val_loss: 0.9722 - val_accuracy: 0.6038\n",
      "Epoch 189/500\n",
      "21/21 [==============================] - 0s 22ms/step - loss: 0.1985 - accuracy: 0.9408 - val_loss: 0.9614 - val_accuracy: 0.6226\n",
      "Epoch 190/500\n",
      "21/21 [==============================] - 0s 23ms/step - loss: 0.2049 - accuracy: 0.9315 - val_loss: 0.9573 - val_accuracy: 0.6226\n",
      "Epoch 191/500\n",
      "21/21 [==============================] - 1s 26ms/step - loss: 0.1874 - accuracy: 0.9486 - val_loss: 0.9959 - val_accuracy: 0.5660\n",
      "Epoch 192/500\n",
      "21/21 [==============================] - 1s 25ms/step - loss: 0.1962 - accuracy: 0.9393 - val_loss: 0.9450 - val_accuracy: 0.5660\n",
      "Epoch 193/500\n",
      "21/21 [==============================] - 1s 25ms/step - loss: 0.1853 - accuracy: 0.9439 - val_loss: 0.9386 - val_accuracy: 0.5849\n",
      "Epoch 194/500\n",
      "21/21 [==============================] - 0s 24ms/step - loss: 0.1916 - accuracy: 0.9393 - val_loss: 0.9681 - val_accuracy: 0.6226\n",
      "Epoch 195/500\n",
      "21/21 [==============================] - 0s 21ms/step - loss: 0.1881 - accuracy: 0.9393 - val_loss: 0.9779 - val_accuracy: 0.6226\n",
      "Epoch 196/500\n",
      "21/21 [==============================] - 1s 24ms/step - loss: 0.1867 - accuracy: 0.9377 - val_loss: 0.9707 - val_accuracy: 0.6226\n",
      "Epoch 197/500\n",
      "21/21 [==============================] - 1s 26ms/step - loss: 0.2016 - accuracy: 0.9283 - val_loss: 0.9462 - val_accuracy: 0.6038\n",
      "Epoch 198/500\n",
      "21/21 [==============================] - 1s 27ms/step - loss: 0.1823 - accuracy: 0.9533 - val_loss: 0.9674 - val_accuracy: 0.6415\n",
      "Epoch 199/500\n",
      "21/21 [==============================] - 0s 23ms/step - loss: 0.1690 - accuracy: 0.9470 - val_loss: 0.9420 - val_accuracy: 0.6226\n",
      "Epoch 200/500\n",
      "21/21 [==============================] - 0s 22ms/step - loss: 0.1771 - accuracy: 0.9424 - val_loss: 0.9645 - val_accuracy: 0.6226\n",
      "Epoch 201/500\n",
      "21/21 [==============================] - 0s 22ms/step - loss: 0.1855 - accuracy: 0.9408 - val_loss: 0.9651 - val_accuracy: 0.6226\n",
      "Epoch 202/500\n",
      "21/21 [==============================] - 0s 23ms/step - loss: 0.1826 - accuracy: 0.9424 - val_loss: 0.9699 - val_accuracy: 0.6604\n",
      "Epoch 203/500\n",
      "21/21 [==============================] - 0s 22ms/step - loss: 0.1683 - accuracy: 0.9502 - val_loss: 0.9411 - val_accuracy: 0.6038\n",
      "Epoch 204/500\n",
      "21/21 [==============================] - 0s 23ms/step - loss: 0.1742 - accuracy: 0.9408 - val_loss: 0.9521 - val_accuracy: 0.5849\n",
      "Epoch 205/500\n",
      "21/21 [==============================] - 0s 24ms/step - loss: 0.1773 - accuracy: 0.9408 - val_loss: 0.9616 - val_accuracy: 0.6604\n",
      "Epoch 206/500\n",
      "21/21 [==============================] - 0s 23ms/step - loss: 0.1655 - accuracy: 0.9517 - val_loss: 0.9946 - val_accuracy: 0.6038\n",
      "Epoch 207/500\n",
      "21/21 [==============================] - 1s 24ms/step - loss: 0.1730 - accuracy: 0.9486 - val_loss: 0.9774 - val_accuracy: 0.6415\n",
      "Epoch 208/500\n",
      "21/21 [==============================] - 1s 26ms/step - loss: 0.1691 - accuracy: 0.9439 - val_loss: 0.9818 - val_accuracy: 0.6038\n",
      "Epoch 209/500\n",
      "21/21 [==============================] - 1s 24ms/step - loss: 0.1749 - accuracy: 0.9424 - val_loss: 0.9923 - val_accuracy: 0.6038\n",
      "Epoch 210/500\n",
      "21/21 [==============================] - 0s 23ms/step - loss: 0.1839 - accuracy: 0.9377 - val_loss: 1.0122 - val_accuracy: 0.5472\n",
      "Epoch 211/500\n",
      "21/21 [==============================] - 1s 25ms/step - loss: 0.1633 - accuracy: 0.9548 - val_loss: 0.9988 - val_accuracy: 0.6415\n",
      "Epoch 212/500\n",
      "21/21 [==============================] - 0s 24ms/step - loss: 0.1596 - accuracy: 0.9579 - val_loss: 1.0544 - val_accuracy: 0.6038\n",
      "Epoch 213/500\n",
      "21/21 [==============================] - 1s 25ms/step - loss: 0.1638 - accuracy: 0.9517 - val_loss: 1.0023 - val_accuracy: 0.5849\n",
      "Epoch 214/500\n",
      "21/21 [==============================] - 1s 26ms/step - loss: 0.1661 - accuracy: 0.9424 - val_loss: 1.0482 - val_accuracy: 0.6038\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f81707d09d0>"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_2_200.compile(optimizer=opt1, \n",
    "                loss='categorical_crossentropy', \n",
    "                metrics=['accuracy'])\n",
    "#Here we use cross-entropy as the criteria for loss.\n",
    "model_2_200.fit(X_train_over, y_train_over, \n",
    "            validation_data=(X_val, y_val), \n",
    "            epochs=500, verbose=True, \n",
    "            callbacks=[early_stopping_monitor])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "0b6112a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 4ms/step - loss: 0.6137 - accuracy: 0.6949\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.6139 - accuracy: 0.7358\n"
     ]
    }
   ],
   "source": [
    "m2_eval_test = model_2_200.evaluate(X_test, y_test)\n",
    "m2_eval_val = model_2_200.evaluate(X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "3f0886a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 3ms/step\n",
      "roc auc score:  0.7293233082706767\n",
      "average precision score:  0.728533038279001\n"
     ]
    }
   ],
   "source": [
    "pred = model_2_200.predict(X_test)\n",
    "roc_value = roc_auc_score(y_test, pred)\n",
    "ap_score = average_precision_score(y_test, pred)\n",
    "print('roc auc score: ', roc_value)\n",
    "print('average precision score: ', ap_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "35b53f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = pred\n",
    "y_c = (y_pred > 0.5).astype(\"int32\")\n",
    "y_test_np = y_test.to_numpy()\n",
    "y_test_np = y_test_np.astype('int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "470c1540",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 4ms/step - loss: 0.6137 - accuracy: 0.6949\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "cf_matrix = confusion_matrix(y_test_np.argmax(axis=1), y_c.argmax(axis=1)) \n",
    "#cf_matrix = confusion_matrix(y_test_np[:, 1], y_c[:, 1]) \n",
    "\n",
    "ax = sns.heatmap(cf_matrix, annot=True, cmap='Blues')\n",
    "\n",
    "ax.set_title('Seaborn Confusion Matrix with labels\\n\\n');\n",
    "ax.set_xlabel('\\nPredicted Values')\n",
    "ax.set_ylabel('Actual Values ');\n",
    "\n",
    "## Ticket labels - List must be in alphabetical order\n",
    "ax.xaxis.set_ticklabels(['Non-Progressor','Progressor'])\n",
    "ax.yaxis.set_ticklabels(['Non-Progressor','Progressor'])\n",
    "\n",
    "## Display the visualization of the Confusion Matrix.\n",
    "plt.savefig('M1_GRI_test.png')\n",
    "m2_eval_test = model_2_200.evaluate(X_test, y_test)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f30d6ab1",
   "metadata": {},
   "source": [
    "500:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "cf9c768f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_11\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv1d_11 (Conv1D)          (None, 766, 64)           256       \n",
      "                                                                 \n",
      " max_pooling1d_11 (MaxPoolin  (None, 255, 64)          0         \n",
      " g1D)                                                            \n",
      "                                                                 \n",
      " dropout_11 (Dropout)        (None, 255, 64)           0         \n",
      "                                                                 \n",
      " flatten_11 (Flatten)        (None, 16320)             0         \n",
      "                                                                 \n",
      " dense_33 (Dense)            (None, 64)                1044544   \n",
      "                                                                 \n",
      " dense_34 (Dense)            (None, 16)                1040      \n",
      "                                                                 \n",
      " dense_35 (Dense)            (None, 2)                 34        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,045,874\n",
      "Trainable params: 1,045,874\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#create model2_nocall\n",
    "model_2_500 = Sequential()\n",
    "\n",
    "#add layers\n",
    "model_2_500.add(Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(768,1)))\n",
    "model_2_500.add(MaxPooling1D(pool_size=3))\n",
    "# model_1.add(Conv1D(filters=32, kernel_size=3, activation='relu'))\n",
    "model_2_500.add(Dropout(0.5))\n",
    "# model_1.add(MaxPooling1D(pool_size=2))\n",
    "model_2_500.add(Flatten())\n",
    "model_2_500.add(Dense(64, activation='relu'))\n",
    "model_2_500.add(Dense(16, activation='relu'))\n",
    "model_2_500.add(Dense(2, activation='softmax'))\n",
    "model_2_500.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "cdae5df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "early_stopping_monitor = EarlyStopping(\n",
    "    monitor='val_accuracy',\n",
    "    min_delta=0,\n",
    "    patience=500,\n",
    "    verbose=0,\n",
    "    mode='auto',\n",
    "    baseline=None,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "opt1 = keras.optimizers.Adam(learning_rate = 1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "f2253fbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "21/21 [==============================] - 1s 31ms/step - loss: 0.6941 - accuracy: 0.5078 - val_loss: 0.6676 - val_accuracy: 0.6604\n",
      "Epoch 2/500\n",
      "21/21 [==============================] - 1s 27ms/step - loss: 0.6918 - accuracy: 0.5016 - val_loss: 0.6802 - val_accuracy: 0.6415\n",
      "Epoch 3/500\n",
      "21/21 [==============================] - 1s 26ms/step - loss: 0.6884 - accuracy: 0.5171 - val_loss: 0.6730 - val_accuracy: 0.6604\n",
      "Epoch 4/500\n",
      "21/21 [==============================] - 0s 24ms/step - loss: 0.6879 - accuracy: 0.5732 - val_loss: 0.6598 - val_accuracy: 0.6792\n",
      "Epoch 5/500\n",
      "21/21 [==============================] - 1s 26ms/step - loss: 0.6885 - accuracy: 0.5062 - val_loss: 0.6797 - val_accuracy: 0.6981\n",
      "Epoch 6/500\n",
      "21/21 [==============================] - 1s 26ms/step - loss: 0.6869 - accuracy: 0.5467 - val_loss: 0.6632 - val_accuracy: 0.7170\n",
      "Epoch 7/500\n",
      "21/21 [==============================] - 1s 27ms/step - loss: 0.6794 - accuracy: 0.5498 - val_loss: 0.6647 - val_accuracy: 0.6981\n",
      "Epoch 8/500\n",
      "21/21 [==============================] - 1s 26ms/step - loss: 0.6750 - accuracy: 0.6293 - val_loss: 0.6551 - val_accuracy: 0.7358\n",
      "Epoch 9/500\n",
      "21/21 [==============================] - 0s 24ms/step - loss: 0.6699 - accuracy: 0.6262 - val_loss: 0.6278 - val_accuracy: 0.6226\n",
      "Epoch 10/500\n",
      "21/21 [==============================] - 1s 26ms/step - loss: 0.6734 - accuracy: 0.5639 - val_loss: 0.6799 - val_accuracy: 0.5849\n",
      "Epoch 11/500\n",
      "21/21 [==============================] - 1s 24ms/step - loss: 0.6638 - accuracy: 0.6231 - val_loss: 0.6345 - val_accuracy: 0.7547\n",
      "Epoch 12/500\n",
      "21/21 [==============================] - 0s 24ms/step - loss: 0.6657 - accuracy: 0.6262 - val_loss: 0.6224 - val_accuracy: 0.7170\n",
      "Epoch 13/500\n",
      "21/21 [==============================] - 0s 24ms/step - loss: 0.6605 - accuracy: 0.6075 - val_loss: 0.6508 - val_accuracy: 0.6792\n",
      "Epoch 14/500\n",
      "21/21 [==============================] - 1s 26ms/step - loss: 0.6545 - accuracy: 0.6480 - val_loss: 0.6197 - val_accuracy: 0.7358\n",
      "Epoch 15/500\n",
      "21/21 [==============================] - 0s 24ms/step - loss: 0.6506 - accuracy: 0.6340 - val_loss: 0.6567 - val_accuracy: 0.6792\n",
      "Epoch 16/500\n",
      "21/21 [==============================] - 1s 25ms/step - loss: 0.6545 - accuracy: 0.6215 - val_loss: 0.6268 - val_accuracy: 0.7170\n",
      "Epoch 17/500\n",
      "21/21 [==============================] - 1s 26ms/step - loss: 0.6479 - accuracy: 0.6340 - val_loss: 0.6506 - val_accuracy: 0.6604\n",
      "Epoch 18/500\n",
      "21/21 [==============================] - 1s 26ms/step - loss: 0.6446 - accuracy: 0.6402 - val_loss: 0.6160 - val_accuracy: 0.7358\n",
      "Epoch 19/500\n",
      "21/21 [==============================] - 1s 25ms/step - loss: 0.6466 - accuracy: 0.6386 - val_loss: 0.6173 - val_accuracy: 0.6981\n",
      "Epoch 20/500\n",
      "21/21 [==============================] - 1s 25ms/step - loss: 0.6405 - accuracy: 0.6417 - val_loss: 0.6309 - val_accuracy: 0.6604\n",
      "Epoch 21/500\n",
      "21/21 [==============================] - 1s 26ms/step - loss: 0.6416 - accuracy: 0.6464 - val_loss: 0.6469 - val_accuracy: 0.6792\n",
      "Epoch 22/500\n",
      "21/21 [==============================] - 0s 23ms/step - loss: 0.6361 - accuracy: 0.6511 - val_loss: 0.6317 - val_accuracy: 0.6792\n",
      "Epoch 23/500\n",
      "21/21 [==============================] - 1s 28ms/step - loss: 0.6362 - accuracy: 0.6371 - val_loss: 0.6246 - val_accuracy: 0.6792\n",
      "Epoch 24/500\n",
      "21/21 [==============================] - 1s 25ms/step - loss: 0.6340 - accuracy: 0.6573 - val_loss: 0.6223 - val_accuracy: 0.6792\n",
      "Epoch 25/500\n",
      "21/21 [==============================] - 1s 27ms/step - loss: 0.6346 - accuracy: 0.6402 - val_loss: 0.6300 - val_accuracy: 0.6792\n",
      "Epoch 26/500\n",
      "21/21 [==============================] - 1s 26ms/step - loss: 0.6263 - accuracy: 0.6604 - val_loss: 0.6200 - val_accuracy: 0.6792\n",
      "Epoch 27/500\n",
      "21/21 [==============================] - 1s 24ms/step - loss: 0.6363 - accuracy: 0.6449 - val_loss: 0.6839 - val_accuracy: 0.5472\n",
      "Epoch 28/500\n",
      "21/21 [==============================] - 1s 26ms/step - loss: 0.6295 - accuracy: 0.6293 - val_loss: 0.6084 - val_accuracy: 0.6792\n",
      "Epoch 29/500\n",
      "21/21 [==============================] - 1s 26ms/step - loss: 0.6258 - accuracy: 0.6713 - val_loss: 0.6372 - val_accuracy: 0.6792\n",
      "Epoch 30/500\n",
      "21/21 [==============================] - 1s 26ms/step - loss: 0.6268 - accuracy: 0.6480 - val_loss: 0.6428 - val_accuracy: 0.6792\n",
      "Epoch 31/500\n",
      "21/21 [==============================] - 1s 27ms/step - loss: 0.6221 - accuracy: 0.6604 - val_loss: 0.6168 - val_accuracy: 0.6604\n",
      "Epoch 32/500\n",
      "21/21 [==============================] - 1s 25ms/step - loss: 0.6154 - accuracy: 0.6636 - val_loss: 0.6431 - val_accuracy: 0.6981\n",
      "Epoch 33/500\n",
      "21/21 [==============================] - 1s 27ms/step - loss: 0.6149 - accuracy: 0.6573 - val_loss: 0.6271 - val_accuracy: 0.6604\n",
      "Epoch 34/500\n",
      "21/21 [==============================] - 1s 25ms/step - loss: 0.6221 - accuracy: 0.6433 - val_loss: 0.6799 - val_accuracy: 0.5849\n",
      "Epoch 35/500\n",
      "21/21 [==============================] - 1s 26ms/step - loss: 0.6183 - accuracy: 0.6636 - val_loss: 0.6012 - val_accuracy: 0.6981\n",
      "Epoch 36/500\n",
      "21/21 [==============================] - 1s 25ms/step - loss: 0.6143 - accuracy: 0.6558 - val_loss: 0.6650 - val_accuracy: 0.6415\n",
      "Epoch 37/500\n",
      "21/21 [==============================] - 1s 25ms/step - loss: 0.6094 - accuracy: 0.6729 - val_loss: 0.6224 - val_accuracy: 0.6604\n",
      "Epoch 38/500\n",
      "21/21 [==============================] - 0s 23ms/step - loss: 0.6022 - accuracy: 0.6838 - val_loss: 0.6513 - val_accuracy: 0.6604\n",
      "Epoch 39/500\n",
      "21/21 [==============================] - 1s 25ms/step - loss: 0.6046 - accuracy: 0.6807 - val_loss: 0.6121 - val_accuracy: 0.6792\n",
      "Epoch 40/500\n",
      "21/21 [==============================] - 1s 24ms/step - loss: 0.5995 - accuracy: 0.6745 - val_loss: 0.6483 - val_accuracy: 0.6604\n",
      "Epoch 41/500\n",
      "21/21 [==============================] - 0s 24ms/step - loss: 0.5971 - accuracy: 0.6838 - val_loss: 0.6315 - val_accuracy: 0.6981\n",
      "Epoch 42/500\n",
      "21/21 [==============================] - 1s 24ms/step - loss: 0.6013 - accuracy: 0.6760 - val_loss: 0.6082 - val_accuracy: 0.6604\n",
      "Epoch 43/500\n",
      "21/21 [==============================] - 1s 24ms/step - loss: 0.5960 - accuracy: 0.6822 - val_loss: 0.6577 - val_accuracy: 0.6415\n",
      "Epoch 44/500\n",
      "21/21 [==============================] - 0s 23ms/step - loss: 0.5856 - accuracy: 0.6978 - val_loss: 0.6247 - val_accuracy: 0.6604\n",
      "Epoch 45/500\n",
      "21/21 [==============================] - 1s 24ms/step - loss: 0.5889 - accuracy: 0.6916 - val_loss: 0.6487 - val_accuracy: 0.6604\n",
      "Epoch 46/500\n",
      "21/21 [==============================] - 1s 25ms/step - loss: 0.5904 - accuracy: 0.6900 - val_loss: 0.6117 - val_accuracy: 0.6792\n",
      "Epoch 47/500\n",
      "21/21 [==============================] - 1s 27ms/step - loss: 0.5917 - accuracy: 0.6900 - val_loss: 0.6384 - val_accuracy: 0.6604\n",
      "Epoch 48/500\n",
      "21/21 [==============================] - 0s 23ms/step - loss: 0.5804 - accuracy: 0.7056 - val_loss: 0.6375 - val_accuracy: 0.6415\n",
      "Epoch 49/500\n",
      "21/21 [==============================] - 1s 26ms/step - loss: 0.5846 - accuracy: 0.6822 - val_loss: 0.6712 - val_accuracy: 0.6604\n",
      "Epoch 50/500\n",
      "21/21 [==============================] - 0s 24ms/step - loss: 0.5793 - accuracy: 0.7009 - val_loss: 0.6423 - val_accuracy: 0.6415\n",
      "Epoch 51/500\n",
      "21/21 [==============================] - 1s 24ms/step - loss: 0.5696 - accuracy: 0.7103 - val_loss: 0.6614 - val_accuracy: 0.6415\n",
      "Epoch 52/500\n",
      "21/21 [==============================] - 1s 25ms/step - loss: 0.5776 - accuracy: 0.6931 - val_loss: 0.6345 - val_accuracy: 0.6792\n",
      "Epoch 53/500\n",
      "21/21 [==============================] - 1s 25ms/step - loss: 0.5684 - accuracy: 0.7009 - val_loss: 0.6703 - val_accuracy: 0.6226\n",
      "Epoch 54/500\n",
      "21/21 [==============================] - 0s 20ms/step - loss: 0.5704 - accuracy: 0.6931 - val_loss: 0.6274 - val_accuracy: 0.6792\n",
      "Epoch 55/500\n",
      "21/21 [==============================] - 1s 24ms/step - loss: 0.5597 - accuracy: 0.7087 - val_loss: 0.6482 - val_accuracy: 0.6604\n",
      "Epoch 56/500\n",
      "21/21 [==============================] - 0s 24ms/step - loss: 0.5640 - accuracy: 0.7165 - val_loss: 0.6680 - val_accuracy: 0.6415\n",
      "Epoch 57/500\n",
      "21/21 [==============================] - 1s 25ms/step - loss: 0.5561 - accuracy: 0.7321 - val_loss: 0.6500 - val_accuracy: 0.6226\n",
      "Epoch 58/500\n",
      "21/21 [==============================] - 0s 24ms/step - loss: 0.5528 - accuracy: 0.7321 - val_loss: 0.6362 - val_accuracy: 0.6415\n",
      "Epoch 59/500\n",
      "21/21 [==============================] - 1s 25ms/step - loss: 0.5510 - accuracy: 0.7336 - val_loss: 0.6588 - val_accuracy: 0.6226\n",
      "Epoch 60/500\n",
      "21/21 [==============================] - 1s 25ms/step - loss: 0.5463 - accuracy: 0.7290 - val_loss: 0.6920 - val_accuracy: 0.6038\n",
      "Epoch 61/500\n",
      "21/21 [==============================] - 1s 24ms/step - loss: 0.5599 - accuracy: 0.7196 - val_loss: 0.6525 - val_accuracy: 0.6226\n",
      "Epoch 62/500\n",
      "21/21 [==============================] - 0s 23ms/step - loss: 0.5428 - accuracy: 0.7336 - val_loss: 0.6687 - val_accuracy: 0.6226\n",
      "Epoch 63/500\n",
      "21/21 [==============================] - 0s 23ms/step - loss: 0.5394 - accuracy: 0.7321 - val_loss: 0.6803 - val_accuracy: 0.5849\n",
      "Epoch 64/500\n",
      "21/21 [==============================] - 0s 23ms/step - loss: 0.5521 - accuracy: 0.7165 - val_loss: 0.6568 - val_accuracy: 0.6415\n",
      "Epoch 65/500\n",
      "21/21 [==============================] - 1s 26ms/step - loss: 0.5381 - accuracy: 0.7290 - val_loss: 0.6711 - val_accuracy: 0.6038\n",
      "Epoch 66/500\n",
      "21/21 [==============================] - 0s 22ms/step - loss: 0.5309 - accuracy: 0.7477 - val_loss: 0.6673 - val_accuracy: 0.6415\n",
      "Epoch 67/500\n",
      "21/21 [==============================] - 0s 24ms/step - loss: 0.5315 - accuracy: 0.7383 - val_loss: 0.6543 - val_accuracy: 0.6226\n",
      "Epoch 68/500\n",
      "21/21 [==============================] - 1s 26ms/step - loss: 0.5295 - accuracy: 0.7399 - val_loss: 0.6546 - val_accuracy: 0.6604\n",
      "Epoch 69/500\n",
      "21/21 [==============================] - 0s 24ms/step - loss: 0.5249 - accuracy: 0.7430 - val_loss: 0.6612 - val_accuracy: 0.6415\n",
      "Epoch 70/500\n",
      "21/21 [==============================] - 0s 24ms/step - loss: 0.5183 - accuracy: 0.7555 - val_loss: 0.6850 - val_accuracy: 0.5849\n",
      "Epoch 71/500\n",
      "21/21 [==============================] - 1s 25ms/step - loss: 0.5187 - accuracy: 0.7555 - val_loss: 0.6843 - val_accuracy: 0.6415\n",
      "Epoch 72/500\n",
      "21/21 [==============================] - 1s 26ms/step - loss: 0.5097 - accuracy: 0.7679 - val_loss: 0.6791 - val_accuracy: 0.6415\n",
      "Epoch 73/500\n",
      "21/21 [==============================] - 0s 24ms/step - loss: 0.5237 - accuracy: 0.7399 - val_loss: 0.6655 - val_accuracy: 0.6604\n",
      "Epoch 74/500\n",
      "21/21 [==============================] - 1s 25ms/step - loss: 0.5040 - accuracy: 0.7632 - val_loss: 0.6794 - val_accuracy: 0.6604\n",
      "Epoch 75/500\n",
      "21/21 [==============================] - 1s 25ms/step - loss: 0.5087 - accuracy: 0.7632 - val_loss: 0.6858 - val_accuracy: 0.6792\n",
      "Epoch 76/500\n",
      "21/21 [==============================] - 1s 25ms/step - loss: 0.5061 - accuracy: 0.7741 - val_loss: 0.7012 - val_accuracy: 0.6226\n",
      "Epoch 77/500\n",
      "21/21 [==============================] - 1s 25ms/step - loss: 0.5200 - accuracy: 0.7430 - val_loss: 0.6858 - val_accuracy: 0.6226\n",
      "Epoch 78/500\n",
      "21/21 [==============================] - 1s 25ms/step - loss: 0.4986 - accuracy: 0.7741 - val_loss: 0.6900 - val_accuracy: 0.6604\n",
      "Epoch 79/500\n",
      "21/21 [==============================] - 1s 25ms/step - loss: 0.5071 - accuracy: 0.7570 - val_loss: 0.6942 - val_accuracy: 0.6415\n",
      "Epoch 80/500\n",
      "21/21 [==============================] - 1s 24ms/step - loss: 0.4993 - accuracy: 0.7664 - val_loss: 0.6973 - val_accuracy: 0.6415\n",
      "Epoch 81/500\n",
      "21/21 [==============================] - 0s 22ms/step - loss: 0.4907 - accuracy: 0.7804 - val_loss: 0.7284 - val_accuracy: 0.5660\n",
      "Epoch 82/500\n",
      "21/21 [==============================] - 0s 24ms/step - loss: 0.4887 - accuracy: 0.7679 - val_loss: 0.6784 - val_accuracy: 0.6415\n",
      "Epoch 83/500\n",
      "21/21 [==============================] - 0s 24ms/step - loss: 0.4769 - accuracy: 0.7835 - val_loss: 0.7345 - val_accuracy: 0.5849\n",
      "Epoch 84/500\n",
      "21/21 [==============================] - 1s 24ms/step - loss: 0.4901 - accuracy: 0.7804 - val_loss: 0.7172 - val_accuracy: 0.6226\n",
      "Epoch 85/500\n",
      "21/21 [==============================] - 0s 24ms/step - loss: 0.4797 - accuracy: 0.7835 - val_loss: 0.6841 - val_accuracy: 0.6226\n",
      "Epoch 86/500\n",
      "21/21 [==============================] - 1s 25ms/step - loss: 0.4781 - accuracy: 0.7850 - val_loss: 0.6948 - val_accuracy: 0.6226\n",
      "Epoch 87/500\n",
      "21/21 [==============================] - 0s 24ms/step - loss: 0.4786 - accuracy: 0.7960 - val_loss: 0.7238 - val_accuracy: 0.6038\n",
      "Epoch 88/500\n",
      "21/21 [==============================] - 0s 23ms/step - loss: 0.4810 - accuracy: 0.7850 - val_loss: 0.7525 - val_accuracy: 0.6038\n",
      "Epoch 89/500\n",
      "21/21 [==============================] - 0s 23ms/step - loss: 0.4715 - accuracy: 0.7804 - val_loss: 0.7048 - val_accuracy: 0.6415\n",
      "Epoch 90/500\n",
      "21/21 [==============================] - 0s 23ms/step - loss: 0.4697 - accuracy: 0.7850 - val_loss: 0.7327 - val_accuracy: 0.6038\n",
      "Epoch 91/500\n",
      "21/21 [==============================] - 1s 24ms/step - loss: 0.4674 - accuracy: 0.7928 - val_loss: 0.7195 - val_accuracy: 0.6415\n",
      "Epoch 92/500\n",
      "21/21 [==============================] - 1s 26ms/step - loss: 0.4565 - accuracy: 0.7991 - val_loss: 0.7321 - val_accuracy: 0.6226\n",
      "Epoch 93/500\n",
      "21/21 [==============================] - 1s 26ms/step - loss: 0.4532 - accuracy: 0.8006 - val_loss: 0.6968 - val_accuracy: 0.6038\n",
      "Epoch 94/500\n",
      "21/21 [==============================] - 0s 23ms/step - loss: 0.4550 - accuracy: 0.7882 - val_loss: 0.7751 - val_accuracy: 0.5660\n",
      "Epoch 95/500\n",
      "21/21 [==============================] - 0s 22ms/step - loss: 0.4669 - accuracy: 0.7835 - val_loss: 0.7716 - val_accuracy: 0.6038\n",
      "Epoch 96/500\n",
      "21/21 [==============================] - 0s 22ms/step - loss: 0.4374 - accuracy: 0.8006 - val_loss: 0.7377 - val_accuracy: 0.6226\n",
      "Epoch 97/500\n",
      "21/21 [==============================] - 0s 23ms/step - loss: 0.4488 - accuracy: 0.8037 - val_loss: 0.7454 - val_accuracy: 0.6226\n",
      "Epoch 98/500\n",
      "21/21 [==============================] - 0s 24ms/step - loss: 0.4459 - accuracy: 0.8100 - val_loss: 0.7521 - val_accuracy: 0.6226\n",
      "Epoch 99/500\n",
      "21/21 [==============================] - 1s 24ms/step - loss: 0.4458 - accuracy: 0.8146 - val_loss: 0.7617 - val_accuracy: 0.6226\n",
      "Epoch 100/500\n",
      "21/21 [==============================] - 0s 23ms/step - loss: 0.4460 - accuracy: 0.7882 - val_loss: 0.7761 - val_accuracy: 0.6038\n",
      "Epoch 101/500\n",
      "21/21 [==============================] - 1s 24ms/step - loss: 0.4389 - accuracy: 0.8240 - val_loss: 0.7813 - val_accuracy: 0.6226\n",
      "Epoch 102/500\n",
      "21/21 [==============================] - 0s 23ms/step - loss: 0.4444 - accuracy: 0.7944 - val_loss: 0.7224 - val_accuracy: 0.5849\n",
      "Epoch 103/500\n",
      "21/21 [==============================] - 0s 23ms/step - loss: 0.4334 - accuracy: 0.8053 - val_loss: 0.7282 - val_accuracy: 0.5849\n",
      "Epoch 104/500\n",
      "21/21 [==============================] - 0s 22ms/step - loss: 0.4348 - accuracy: 0.7991 - val_loss: 0.7610 - val_accuracy: 0.6226\n",
      "Epoch 105/500\n",
      "21/21 [==============================] - 1s 25ms/step - loss: 0.4316 - accuracy: 0.8146 - val_loss: 0.7481 - val_accuracy: 0.6226\n",
      "Epoch 106/500\n",
      "21/21 [==============================] - 0s 22ms/step - loss: 0.4224 - accuracy: 0.8209 - val_loss: 0.7534 - val_accuracy: 0.6038\n",
      "Epoch 107/500\n",
      "21/21 [==============================] - 1s 24ms/step - loss: 0.4226 - accuracy: 0.8178 - val_loss: 0.7418 - val_accuracy: 0.6226\n",
      "Epoch 108/500\n",
      "21/21 [==============================] - 1s 25ms/step - loss: 0.4102 - accuracy: 0.8364 - val_loss: 0.7939 - val_accuracy: 0.6038\n",
      "Epoch 109/500\n",
      "21/21 [==============================] - 0s 22ms/step - loss: 0.4142 - accuracy: 0.8131 - val_loss: 0.7638 - val_accuracy: 0.5660\n",
      "Epoch 110/500\n",
      "21/21 [==============================] - 1s 25ms/step - loss: 0.4172 - accuracy: 0.8146 - val_loss: 0.7343 - val_accuracy: 0.5660\n",
      "Epoch 111/500\n",
      "21/21 [==============================] - 1s 26ms/step - loss: 0.4124 - accuracy: 0.8364 - val_loss: 0.7642 - val_accuracy: 0.6226\n",
      "Epoch 112/500\n",
      "21/21 [==============================] - 0s 23ms/step - loss: 0.4122 - accuracy: 0.8349 - val_loss: 0.7964 - val_accuracy: 0.5660\n",
      "Epoch 113/500\n",
      "21/21 [==============================] - 1s 24ms/step - loss: 0.4125 - accuracy: 0.8333 - val_loss: 0.7790 - val_accuracy: 0.6038\n",
      "Epoch 114/500\n",
      "21/21 [==============================] - 0s 19ms/step - loss: 0.4059 - accuracy: 0.8287 - val_loss: 0.7477 - val_accuracy: 0.5849\n",
      "Epoch 115/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21/21 [==============================] - 0s 23ms/step - loss: 0.4030 - accuracy: 0.8349 - val_loss: 0.7721 - val_accuracy: 0.6038\n",
      "Epoch 116/500\n",
      "21/21 [==============================] - 0s 22ms/step - loss: 0.4023 - accuracy: 0.8240 - val_loss: 0.7964 - val_accuracy: 0.5849\n",
      "Epoch 117/500\n",
      "21/21 [==============================] - 1s 26ms/step - loss: 0.3941 - accuracy: 0.8396 - val_loss: 0.8470 - val_accuracy: 0.5849\n",
      "Epoch 118/500\n",
      "21/21 [==============================] - 1s 24ms/step - loss: 0.3993 - accuracy: 0.8333 - val_loss: 0.8069 - val_accuracy: 0.5660\n",
      "Epoch 119/500\n",
      "21/21 [==============================] - 1s 25ms/step - loss: 0.3818 - accuracy: 0.8442 - val_loss: 0.7581 - val_accuracy: 0.5660\n",
      "Epoch 120/500\n",
      "21/21 [==============================] - 1s 24ms/step - loss: 0.3874 - accuracy: 0.8411 - val_loss: 0.7778 - val_accuracy: 0.5660\n",
      "Epoch 121/500\n",
      "21/21 [==============================] - 0s 21ms/step - loss: 0.3855 - accuracy: 0.8411 - val_loss: 0.7967 - val_accuracy: 0.5849\n",
      "Epoch 122/500\n",
      "21/21 [==============================] - 0s 24ms/step - loss: 0.3742 - accuracy: 0.8411 - val_loss: 0.7922 - val_accuracy: 0.5849\n",
      "Epoch 123/500\n",
      "21/21 [==============================] - 0s 23ms/step - loss: 0.3811 - accuracy: 0.8255 - val_loss: 0.7726 - val_accuracy: 0.5660\n",
      "Epoch 124/500\n",
      "21/21 [==============================] - 0s 23ms/step - loss: 0.3672 - accuracy: 0.8645 - val_loss: 0.8011 - val_accuracy: 0.5849\n",
      "Epoch 125/500\n",
      "21/21 [==============================] - 1s 25ms/step - loss: 0.3612 - accuracy: 0.8505 - val_loss: 0.7982 - val_accuracy: 0.5660\n",
      "Epoch 126/500\n",
      "21/21 [==============================] - 1s 25ms/step - loss: 0.3795 - accuracy: 0.8458 - val_loss: 0.7727 - val_accuracy: 0.5283\n",
      "Epoch 127/500\n",
      "21/21 [==============================] - 1s 27ms/step - loss: 0.3568 - accuracy: 0.8629 - val_loss: 0.8066 - val_accuracy: 0.6038\n",
      "Epoch 128/500\n",
      "21/21 [==============================] - 0s 22ms/step - loss: 0.3610 - accuracy: 0.8489 - val_loss: 0.7839 - val_accuracy: 0.5849\n",
      "Epoch 129/500\n",
      "21/21 [==============================] - 0s 23ms/step - loss: 0.3612 - accuracy: 0.8505 - val_loss: 0.8029 - val_accuracy: 0.5849\n",
      "Epoch 130/500\n",
      "21/21 [==============================] - 1s 25ms/step - loss: 0.3549 - accuracy: 0.8567 - val_loss: 0.8395 - val_accuracy: 0.6038\n",
      "Epoch 131/500\n",
      "21/21 [==============================] - 0s 23ms/step - loss: 0.3812 - accuracy: 0.8318 - val_loss: 0.7846 - val_accuracy: 0.5849\n",
      "Epoch 132/500\n",
      "21/21 [==============================] - 0s 23ms/step - loss: 0.3632 - accuracy: 0.8411 - val_loss: 0.7664 - val_accuracy: 0.5660\n",
      "Epoch 133/500\n",
      "21/21 [==============================] - 0s 23ms/step - loss: 0.3473 - accuracy: 0.8629 - val_loss: 0.7726 - val_accuracy: 0.5660\n",
      "Epoch 134/500\n",
      "21/21 [==============================] - 0s 23ms/step - loss: 0.3362 - accuracy: 0.8676 - val_loss: 0.7949 - val_accuracy: 0.5849\n",
      "Epoch 135/500\n",
      "21/21 [==============================] - 0s 23ms/step - loss: 0.3474 - accuracy: 0.8614 - val_loss: 0.7672 - val_accuracy: 0.5849\n",
      "Epoch 136/500\n",
      "21/21 [==============================] - 0s 23ms/step - loss: 0.3345 - accuracy: 0.8583 - val_loss: 0.8149 - val_accuracy: 0.5849\n",
      "Epoch 137/500\n",
      "21/21 [==============================] - 1s 25ms/step - loss: 0.3337 - accuracy: 0.8707 - val_loss: 0.7934 - val_accuracy: 0.5849\n",
      "Epoch 138/500\n",
      "21/21 [==============================] - 0s 23ms/step - loss: 0.3349 - accuracy: 0.8816 - val_loss: 0.7955 - val_accuracy: 0.5849\n",
      "Epoch 139/500\n",
      "21/21 [==============================] - 1s 24ms/step - loss: 0.3439 - accuracy: 0.8614 - val_loss: 0.7813 - val_accuracy: 0.5660\n",
      "Epoch 140/500\n",
      "21/21 [==============================] - 0s 23ms/step - loss: 0.3488 - accuracy: 0.8442 - val_loss: 0.7850 - val_accuracy: 0.5849\n",
      "Epoch 141/500\n",
      "21/21 [==============================] - 0s 23ms/step - loss: 0.3261 - accuracy: 0.8754 - val_loss: 0.8124 - val_accuracy: 0.6038\n",
      "Epoch 142/500\n",
      "21/21 [==============================] - 0s 23ms/step - loss: 0.3301 - accuracy: 0.8676 - val_loss: 0.7918 - val_accuracy: 0.6226\n",
      "Epoch 143/500\n",
      "21/21 [==============================] - 0s 23ms/step - loss: 0.3204 - accuracy: 0.8707 - val_loss: 0.7756 - val_accuracy: 0.6226\n",
      "Epoch 144/500\n",
      "21/21 [==============================] - 0s 22ms/step - loss: 0.3371 - accuracy: 0.8629 - val_loss: 0.7810 - val_accuracy: 0.6226\n",
      "Epoch 145/500\n",
      "21/21 [==============================] - 1s 26ms/step - loss: 0.3343 - accuracy: 0.8785 - val_loss: 0.8491 - val_accuracy: 0.6226\n",
      "Epoch 146/500\n",
      "21/21 [==============================] - 0s 23ms/step - loss: 0.3193 - accuracy: 0.8676 - val_loss: 0.8268 - val_accuracy: 0.6038\n",
      "Epoch 147/500\n",
      "21/21 [==============================] - 0s 23ms/step - loss: 0.3147 - accuracy: 0.8692 - val_loss: 0.8306 - val_accuracy: 0.5849\n",
      "Epoch 148/500\n",
      "21/21 [==============================] - 0s 23ms/step - loss: 0.3130 - accuracy: 0.8956 - val_loss: 0.8262 - val_accuracy: 0.5472\n",
      "Epoch 149/500\n",
      "21/21 [==============================] - 1s 25ms/step - loss: 0.3081 - accuracy: 0.8847 - val_loss: 0.8331 - val_accuracy: 0.5472\n",
      "Epoch 150/500\n",
      "21/21 [==============================] - 1s 26ms/step - loss: 0.3113 - accuracy: 0.8614 - val_loss: 0.9327 - val_accuracy: 0.5472\n",
      "Epoch 151/500\n",
      "21/21 [==============================] - 0s 22ms/step - loss: 0.3107 - accuracy: 0.8769 - val_loss: 0.8441 - val_accuracy: 0.5849\n",
      "Epoch 152/500\n",
      "21/21 [==============================] - 0s 23ms/step - loss: 0.2997 - accuracy: 0.8941 - val_loss: 0.8526 - val_accuracy: 0.6038\n",
      "Epoch 153/500\n",
      "21/21 [==============================] - 1s 25ms/step - loss: 0.3016 - accuracy: 0.8894 - val_loss: 0.8476 - val_accuracy: 0.5283\n",
      "Epoch 154/500\n",
      "21/21 [==============================] - 0s 23ms/step - loss: 0.2963 - accuracy: 0.8816 - val_loss: 0.8381 - val_accuracy: 0.5660\n",
      "Epoch 155/500\n",
      "21/21 [==============================] - 1s 24ms/step - loss: 0.2886 - accuracy: 0.8925 - val_loss: 0.8560 - val_accuracy: 0.5660\n",
      "Epoch 156/500\n",
      "21/21 [==============================] - 0s 23ms/step - loss: 0.2923 - accuracy: 0.8879 - val_loss: 0.8781 - val_accuracy: 0.6038\n",
      "Epoch 157/500\n",
      "21/21 [==============================] - 0s 22ms/step - loss: 0.2940 - accuracy: 0.8894 - val_loss: 0.8761 - val_accuracy: 0.5849\n",
      "Epoch 158/500\n",
      "21/21 [==============================] - 0s 23ms/step - loss: 0.2908 - accuracy: 0.9003 - val_loss: 0.8640 - val_accuracy: 0.6226\n",
      "Epoch 159/500\n",
      "21/21 [==============================] - 0s 23ms/step - loss: 0.2857 - accuracy: 0.8941 - val_loss: 0.8266 - val_accuracy: 0.6038\n",
      "Epoch 160/500\n",
      "21/21 [==============================] - 0s 22ms/step - loss: 0.3007 - accuracy: 0.8863 - val_loss: 0.8653 - val_accuracy: 0.6038\n",
      "Epoch 161/500\n",
      "21/21 [==============================] - 0s 22ms/step - loss: 0.2860 - accuracy: 0.8925 - val_loss: 0.9289 - val_accuracy: 0.5849\n",
      "Epoch 162/500\n",
      "21/21 [==============================] - 0s 21ms/step - loss: 0.2911 - accuracy: 0.8863 - val_loss: 0.9083 - val_accuracy: 0.5849\n",
      "Epoch 163/500\n",
      "21/21 [==============================] - 0s 22ms/step - loss: 0.2820 - accuracy: 0.8988 - val_loss: 0.8247 - val_accuracy: 0.5472\n",
      "Epoch 164/500\n",
      "21/21 [==============================] - 0s 23ms/step - loss: 0.2749 - accuracy: 0.8879 - val_loss: 0.8408 - val_accuracy: 0.6038\n",
      "Epoch 165/500\n",
      "21/21 [==============================] - 0s 22ms/step - loss: 0.2791 - accuracy: 0.8816 - val_loss: 0.8515 - val_accuracy: 0.5849\n",
      "Epoch 166/500\n",
      "21/21 [==============================] - 1s 24ms/step - loss: 0.2760 - accuracy: 0.9034 - val_loss: 0.8333 - val_accuracy: 0.5849\n",
      "Epoch 167/500\n",
      "21/21 [==============================] - 0s 22ms/step - loss: 0.2728 - accuracy: 0.8988 - val_loss: 0.8806 - val_accuracy: 0.6038\n",
      "Epoch 168/500\n",
      "21/21 [==============================] - 0s 22ms/step - loss: 0.2782 - accuracy: 0.8988 - val_loss: 0.8609 - val_accuracy: 0.6038\n",
      "Epoch 169/500\n",
      "21/21 [==============================] - 0s 23ms/step - loss: 0.2579 - accuracy: 0.9206 - val_loss: 0.8679 - val_accuracy: 0.6038\n",
      "Epoch 170/500\n",
      "21/21 [==============================] - 0s 22ms/step - loss: 0.2580 - accuracy: 0.9174 - val_loss: 0.8867 - val_accuracy: 0.6226\n",
      "Epoch 171/500\n",
      "21/21 [==============================] - 0s 23ms/step - loss: 0.2752 - accuracy: 0.8801 - val_loss: 0.8433 - val_accuracy: 0.6226\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 172/500\n",
      "21/21 [==============================] - 0s 24ms/step - loss: 0.2708 - accuracy: 0.9019 - val_loss: 0.8766 - val_accuracy: 0.6415\n",
      "Epoch 173/500\n",
      "21/21 [==============================] - 1s 25ms/step - loss: 0.2751 - accuracy: 0.9003 - val_loss: 0.8595 - val_accuracy: 0.6038\n",
      "Epoch 174/500\n",
      "21/21 [==============================] - 0s 24ms/step - loss: 0.2756 - accuracy: 0.8941 - val_loss: 0.8397 - val_accuracy: 0.5660\n",
      "Epoch 175/500\n",
      "21/21 [==============================] - 1s 27ms/step - loss: 0.2546 - accuracy: 0.9081 - val_loss: 0.8204 - val_accuracy: 0.5660\n",
      "Epoch 176/500\n",
      "21/21 [==============================] - 1s 26ms/step - loss: 0.2582 - accuracy: 0.9081 - val_loss: 0.8324 - val_accuracy: 0.5849\n",
      "Epoch 177/500\n",
      "21/21 [==============================] - 1s 24ms/step - loss: 0.2519 - accuracy: 0.9159 - val_loss: 0.8395 - val_accuracy: 0.5849\n",
      "Epoch 178/500\n",
      "21/21 [==============================] - 1s 26ms/step - loss: 0.2463 - accuracy: 0.9190 - val_loss: 0.8598 - val_accuracy: 0.5849\n",
      "Epoch 179/500\n",
      "21/21 [==============================] - 1s 24ms/step - loss: 0.2471 - accuracy: 0.9221 - val_loss: 0.8485 - val_accuracy: 0.5660\n",
      "Epoch 180/500\n",
      "21/21 [==============================] - 0s 22ms/step - loss: 0.2339 - accuracy: 0.9252 - val_loss: 0.8643 - val_accuracy: 0.6226\n",
      "Epoch 181/500\n",
      "21/21 [==============================] - 0s 24ms/step - loss: 0.2369 - accuracy: 0.9221 - val_loss: 0.8690 - val_accuracy: 0.6038\n",
      "Epoch 182/500\n",
      "21/21 [==============================] - 0s 24ms/step - loss: 0.2270 - accuracy: 0.9268 - val_loss: 0.8525 - val_accuracy: 0.5849\n",
      "Epoch 183/500\n",
      "21/21 [==============================] - 1s 25ms/step - loss: 0.2377 - accuracy: 0.9252 - val_loss: 0.8597 - val_accuracy: 0.5849\n",
      "Epoch 184/500\n",
      "21/21 [==============================] - 0s 24ms/step - loss: 0.2318 - accuracy: 0.9283 - val_loss: 0.8640 - val_accuracy: 0.5849\n",
      "Epoch 185/500\n",
      "21/21 [==============================] - 0s 22ms/step - loss: 0.2277 - accuracy: 0.9190 - val_loss: 0.8691 - val_accuracy: 0.5849\n",
      "Epoch 186/500\n",
      "21/21 [==============================] - 0s 21ms/step - loss: 0.2305 - accuracy: 0.9252 - val_loss: 0.9155 - val_accuracy: 0.6038\n",
      "Epoch 187/500\n",
      "21/21 [==============================] - 0s 22ms/step - loss: 0.2307 - accuracy: 0.9206 - val_loss: 0.8472 - val_accuracy: 0.5660\n",
      "Epoch 188/500\n",
      "21/21 [==============================] - 0s 22ms/step - loss: 0.2480 - accuracy: 0.9034 - val_loss: 0.8785 - val_accuracy: 0.6226\n",
      "Epoch 189/500\n",
      "21/21 [==============================] - 0s 23ms/step - loss: 0.2332 - accuracy: 0.9221 - val_loss: 0.9259 - val_accuracy: 0.6038\n",
      "Epoch 190/500\n",
      "21/21 [==============================] - 0s 23ms/step - loss: 0.2275 - accuracy: 0.9283 - val_loss: 0.8412 - val_accuracy: 0.6226\n",
      "Epoch 191/500\n",
      "21/21 [==============================] - 0s 23ms/step - loss: 0.2173 - accuracy: 0.9268 - val_loss: 0.8430 - val_accuracy: 0.5472\n",
      "Epoch 192/500\n",
      "21/21 [==============================] - 1s 25ms/step - loss: 0.2063 - accuracy: 0.9408 - val_loss: 0.8944 - val_accuracy: 0.5849\n",
      "Epoch 193/500\n",
      "21/21 [==============================] - 0s 24ms/step - loss: 0.2172 - accuracy: 0.9299 - val_loss: 0.9094 - val_accuracy: 0.6038\n",
      "Epoch 194/500\n",
      "21/21 [==============================] - 0s 23ms/step - loss: 0.2105 - accuracy: 0.9315 - val_loss: 0.9292 - val_accuracy: 0.6038\n",
      "Epoch 195/500\n",
      "21/21 [==============================] - 1s 26ms/step - loss: 0.2072 - accuracy: 0.9315 - val_loss: 0.8981 - val_accuracy: 0.6038\n",
      "Epoch 196/500\n",
      "21/21 [==============================] - 0s 23ms/step - loss: 0.2017 - accuracy: 0.9330 - val_loss: 0.9163 - val_accuracy: 0.6038\n",
      "Epoch 197/500\n",
      "21/21 [==============================] - 0s 21ms/step - loss: 0.1991 - accuracy: 0.9455 - val_loss: 0.9567 - val_accuracy: 0.6415\n",
      "Epoch 198/500\n",
      "21/21 [==============================] - 1s 25ms/step - loss: 0.2046 - accuracy: 0.9377 - val_loss: 0.9533 - val_accuracy: 0.6038\n",
      "Epoch 199/500\n",
      "21/21 [==============================] - 0s 24ms/step - loss: 0.1956 - accuracy: 0.9361 - val_loss: 0.9142 - val_accuracy: 0.6038\n",
      "Epoch 200/500\n",
      "21/21 [==============================] - 0s 21ms/step - loss: 0.2099 - accuracy: 0.9221 - val_loss: 0.9646 - val_accuracy: 0.5849\n",
      "Epoch 201/500\n",
      "21/21 [==============================] - 0s 22ms/step - loss: 0.1986 - accuracy: 0.9564 - val_loss: 0.9313 - val_accuracy: 0.6038\n",
      "Epoch 202/500\n",
      "21/21 [==============================] - 0s 22ms/step - loss: 0.2109 - accuracy: 0.9361 - val_loss: 0.9775 - val_accuracy: 0.5849\n",
      "Epoch 203/500\n",
      "21/21 [==============================] - 1s 24ms/step - loss: 0.2063 - accuracy: 0.9315 - val_loss: 0.9583 - val_accuracy: 0.5660\n",
      "Epoch 204/500\n",
      "21/21 [==============================] - 0s 24ms/step - loss: 0.2019 - accuracy: 0.9315 - val_loss: 0.9144 - val_accuracy: 0.5849\n",
      "Epoch 205/500\n",
      "21/21 [==============================] - 0s 23ms/step - loss: 0.1937 - accuracy: 0.9439 - val_loss: 0.9140 - val_accuracy: 0.5849\n",
      "Epoch 206/500\n",
      "21/21 [==============================] - 0s 22ms/step - loss: 0.1929 - accuracy: 0.9377 - val_loss: 0.9346 - val_accuracy: 0.5849\n",
      "Epoch 207/500\n",
      "21/21 [==============================] - 0s 23ms/step - loss: 0.1816 - accuracy: 0.9439 - val_loss: 0.9150 - val_accuracy: 0.5849\n",
      "Epoch 208/500\n",
      "21/21 [==============================] - 0s 23ms/step - loss: 0.2305 - accuracy: 0.9112 - val_loss: 0.8919 - val_accuracy: 0.6226\n",
      "Epoch 209/500\n",
      "21/21 [==============================] - 1s 24ms/step - loss: 0.1990 - accuracy: 0.9439 - val_loss: 0.9266 - val_accuracy: 0.5660\n",
      "Epoch 210/500\n",
      "21/21 [==============================] - 1s 25ms/step - loss: 0.1817 - accuracy: 0.9486 - val_loss: 0.9024 - val_accuracy: 0.6226\n",
      "Epoch 211/500\n",
      "21/21 [==============================] - 0s 24ms/step - loss: 0.1790 - accuracy: 0.9408 - val_loss: 0.9224 - val_accuracy: 0.5849\n",
      "Epoch 212/500\n",
      "21/21 [==============================] - 1s 25ms/step - loss: 0.1810 - accuracy: 0.9377 - val_loss: 0.9414 - val_accuracy: 0.6226\n",
      "Epoch 213/500\n",
      "21/21 [==============================] - 0s 23ms/step - loss: 0.1833 - accuracy: 0.9470 - val_loss: 0.9749 - val_accuracy: 0.6226\n",
      "Epoch 214/500\n",
      "21/21 [==============================] - 0s 23ms/step - loss: 0.1783 - accuracy: 0.9548 - val_loss: 1.0066 - val_accuracy: 0.5849\n",
      "Epoch 215/500\n",
      "21/21 [==============================] - 0s 24ms/step - loss: 0.1809 - accuracy: 0.9455 - val_loss: 0.9753 - val_accuracy: 0.5660\n",
      "Epoch 216/500\n",
      "21/21 [==============================] - 0s 24ms/step - loss: 0.1778 - accuracy: 0.9502 - val_loss: 0.9686 - val_accuracy: 0.5472\n",
      "Epoch 217/500\n",
      "21/21 [==============================] - 1s 25ms/step - loss: 0.1801 - accuracy: 0.9470 - val_loss: 0.9387 - val_accuracy: 0.5660\n",
      "Epoch 218/500\n",
      "21/21 [==============================] - 0s 24ms/step - loss: 0.1708 - accuracy: 0.9548 - val_loss: 0.9677 - val_accuracy: 0.5849\n",
      "Epoch 219/500\n",
      "21/21 [==============================] - 1s 26ms/step - loss: 0.1670 - accuracy: 0.9455 - val_loss: 0.9802 - val_accuracy: 0.5660\n",
      "Epoch 220/500\n",
      "21/21 [==============================] - 1s 26ms/step - loss: 0.1640 - accuracy: 0.9626 - val_loss: 0.9759 - val_accuracy: 0.5660\n",
      "Epoch 221/500\n",
      "21/21 [==============================] - 0s 23ms/step - loss: 0.1597 - accuracy: 0.9564 - val_loss: 0.9836 - val_accuracy: 0.5849\n",
      "Epoch 222/500\n",
      "21/21 [==============================] - 1s 25ms/step - loss: 0.1716 - accuracy: 0.9564 - val_loss: 1.0290 - val_accuracy: 0.5660\n",
      "Epoch 223/500\n",
      "21/21 [==============================] - 0s 24ms/step - loss: 0.1649 - accuracy: 0.9486 - val_loss: 0.9965 - val_accuracy: 0.5660\n",
      "Epoch 224/500\n",
      "21/21 [==============================] - 1s 25ms/step - loss: 0.1578 - accuracy: 0.9564 - val_loss: 0.9645 - val_accuracy: 0.5660\n",
      "Epoch 225/500\n",
      "21/21 [==============================] - 0s 23ms/step - loss: 0.1623 - accuracy: 0.9424 - val_loss: 1.0039 - val_accuracy: 0.5660\n",
      "Epoch 226/500\n",
      "21/21 [==============================] - 0s 22ms/step - loss: 0.1604 - accuracy: 0.9548 - val_loss: 0.9358 - val_accuracy: 0.5660\n",
      "Epoch 227/500\n",
      "21/21 [==============================] - 0s 22ms/step - loss: 0.1598 - accuracy: 0.9548 - val_loss: 0.9819 - val_accuracy: 0.5660\n",
      "Epoch 228/500\n",
      "21/21 [==============================] - 0s 23ms/step - loss: 0.1587 - accuracy: 0.9486 - val_loss: 1.0611 - val_accuracy: 0.5849\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 229/500\n",
      "21/21 [==============================] - 1s 25ms/step - loss: 0.1641 - accuracy: 0.9579 - val_loss: 1.0150 - val_accuracy: 0.5472\n",
      "Epoch 230/500\n",
      "21/21 [==============================] - 0s 23ms/step - loss: 0.1590 - accuracy: 0.9611 - val_loss: 1.0280 - val_accuracy: 0.5472\n",
      "Epoch 231/500\n",
      "21/21 [==============================] - 0s 22ms/step - loss: 0.1581 - accuracy: 0.9470 - val_loss: 1.0503 - val_accuracy: 0.5472\n",
      "Epoch 232/500\n",
      "21/21 [==============================] - 0s 21ms/step - loss: 0.1444 - accuracy: 0.9688 - val_loss: 1.0004 - val_accuracy: 0.5660\n",
      "Epoch 233/500\n",
      "21/21 [==============================] - 1s 26ms/step - loss: 0.1428 - accuracy: 0.9657 - val_loss: 1.0430 - val_accuracy: 0.5472\n",
      "Epoch 234/500\n",
      "21/21 [==============================] - 0s 22ms/step - loss: 0.1389 - accuracy: 0.9751 - val_loss: 1.0390 - val_accuracy: 0.5472\n",
      "Epoch 235/500\n",
      "21/21 [==============================] - 0s 22ms/step - loss: 0.1529 - accuracy: 0.9626 - val_loss: 1.0134 - val_accuracy: 0.5472\n",
      "Epoch 236/500\n",
      "21/21 [==============================] - 0s 24ms/step - loss: 0.1347 - accuracy: 0.9704 - val_loss: 1.0074 - val_accuracy: 0.5849\n",
      "Epoch 237/500\n",
      "21/21 [==============================] - 0s 24ms/step - loss: 0.1444 - accuracy: 0.9657 - val_loss: 1.0324 - val_accuracy: 0.5472\n",
      "Epoch 238/500\n",
      "21/21 [==============================] - 0s 24ms/step - loss: 0.1551 - accuracy: 0.9502 - val_loss: 1.0466 - val_accuracy: 0.5472\n",
      "Epoch 239/500\n",
      "21/21 [==============================] - 1s 25ms/step - loss: 0.1569 - accuracy: 0.9502 - val_loss: 1.0222 - val_accuracy: 0.5472\n",
      "Epoch 240/500\n",
      "21/21 [==============================] - 0s 23ms/step - loss: 0.1551 - accuracy: 0.9564 - val_loss: 1.0159 - val_accuracy: 0.5849\n",
      "Epoch 241/500\n",
      "21/21 [==============================] - 0s 22ms/step - loss: 0.1362 - accuracy: 0.9688 - val_loss: 1.0042 - val_accuracy: 0.5472\n",
      "Epoch 242/500\n",
      "21/21 [==============================] - 1s 25ms/step - loss: 0.1309 - accuracy: 0.9688 - val_loss: 1.0403 - val_accuracy: 0.5660\n",
      "Epoch 243/500\n",
      "21/21 [==============================] - 0s 23ms/step - loss: 0.1288 - accuracy: 0.9673 - val_loss: 1.0865 - val_accuracy: 0.5849\n",
      "Epoch 244/500\n",
      "21/21 [==============================] - 0s 23ms/step - loss: 0.1241 - accuracy: 0.9720 - val_loss: 1.0454 - val_accuracy: 0.5472\n",
      "Epoch 245/500\n",
      "21/21 [==============================] - 0s 23ms/step - loss: 0.1203 - accuracy: 0.9673 - val_loss: 1.0852 - val_accuracy: 0.5472\n",
      "Epoch 246/500\n",
      "21/21 [==============================] - 1s 26ms/step - loss: 0.1249 - accuracy: 0.9704 - val_loss: 1.0905 - val_accuracy: 0.5472\n",
      "Epoch 247/500\n",
      "21/21 [==============================] - 1s 26ms/step - loss: 0.1279 - accuracy: 0.9657 - val_loss: 1.0345 - val_accuracy: 0.5472\n",
      "Epoch 248/500\n",
      "21/21 [==============================] - 0s 22ms/step - loss: 0.1315 - accuracy: 0.9704 - val_loss: 1.0307 - val_accuracy: 0.6415\n",
      "Epoch 249/500\n",
      "21/21 [==============================] - 0s 24ms/step - loss: 0.1324 - accuracy: 0.9642 - val_loss: 1.0316 - val_accuracy: 0.5660\n",
      "Epoch 250/500\n",
      "21/21 [==============================] - 1s 24ms/step - loss: 0.1309 - accuracy: 0.9688 - val_loss: 1.0936 - val_accuracy: 0.5283\n",
      "Epoch 251/500\n",
      "21/21 [==============================] - 1s 26ms/step - loss: 0.1258 - accuracy: 0.9688 - val_loss: 1.0964 - val_accuracy: 0.5283\n",
      "Epoch 252/500\n",
      "21/21 [==============================] - 0s 23ms/step - loss: 0.1270 - accuracy: 0.9735 - val_loss: 1.0791 - val_accuracy: 0.5472\n",
      "Epoch 253/500\n",
      "21/21 [==============================] - 1s 25ms/step - loss: 0.1215 - accuracy: 0.9673 - val_loss: 1.0791 - val_accuracy: 0.5472\n",
      "Epoch 254/500\n",
      "21/21 [==============================] - 1s 24ms/step - loss: 0.1202 - accuracy: 0.9657 - val_loss: 1.0365 - val_accuracy: 0.6038\n",
      "Epoch 255/500\n",
      "21/21 [==============================] - 1s 24ms/step - loss: 0.1183 - accuracy: 0.9829 - val_loss: 1.0477 - val_accuracy: 0.5472\n",
      "Epoch 256/500\n",
      "21/21 [==============================] - 0s 23ms/step - loss: 0.1247 - accuracy: 0.9720 - val_loss: 1.0645 - val_accuracy: 0.5472\n",
      "Epoch 257/500\n",
      "21/21 [==============================] - 1s 24ms/step - loss: 0.1224 - accuracy: 0.9735 - val_loss: 1.0698 - val_accuracy: 0.5472\n",
      "Epoch 258/500\n",
      "21/21 [==============================] - 0s 23ms/step - loss: 0.1580 - accuracy: 0.9486 - val_loss: 1.0549 - val_accuracy: 0.5849\n",
      "Epoch 259/500\n",
      "21/21 [==============================] - 1s 24ms/step - loss: 0.1182 - accuracy: 0.9735 - val_loss: 1.1120 - val_accuracy: 0.5472\n",
      "Epoch 260/500\n",
      "21/21 [==============================] - 1s 25ms/step - loss: 0.1081 - accuracy: 0.9813 - val_loss: 1.1056 - val_accuracy: 0.5472\n",
      "Epoch 261/500\n",
      "21/21 [==============================] - 0s 24ms/step - loss: 0.1084 - accuracy: 0.9782 - val_loss: 1.1066 - val_accuracy: 0.5472\n",
      "Epoch 262/500\n",
      "21/21 [==============================] - 0s 23ms/step - loss: 0.1134 - accuracy: 0.9751 - val_loss: 1.1036 - val_accuracy: 0.5472\n",
      "Epoch 263/500\n",
      "21/21 [==============================] - 0s 23ms/step - loss: 0.1073 - accuracy: 0.9844 - val_loss: 1.0787 - val_accuracy: 0.5472\n",
      "Epoch 264/500\n",
      "21/21 [==============================] - 1s 24ms/step - loss: 0.1173 - accuracy: 0.9735 - val_loss: 1.1235 - val_accuracy: 0.5472\n",
      "Epoch 265/500\n",
      "21/21 [==============================] - 1s 25ms/step - loss: 0.1119 - accuracy: 0.9720 - val_loss: 1.0851 - val_accuracy: 0.5472\n",
      "Epoch 266/500\n",
      "21/21 [==============================] - 0s 23ms/step - loss: 0.1057 - accuracy: 0.9798 - val_loss: 1.0695 - val_accuracy: 0.5472\n",
      "Epoch 267/500\n",
      "21/21 [==============================] - 1s 24ms/step - loss: 0.1080 - accuracy: 0.9735 - val_loss: 1.0579 - val_accuracy: 0.6038\n",
      "Epoch 268/500\n",
      "21/21 [==============================] - 1s 25ms/step - loss: 0.1548 - accuracy: 0.9470 - val_loss: 1.0980 - val_accuracy: 0.5472\n",
      "Epoch 269/500\n",
      "21/21 [==============================] - 1s 25ms/step - loss: 0.1094 - accuracy: 0.9720 - val_loss: 1.0955 - val_accuracy: 0.5472\n",
      "Epoch 270/500\n",
      "21/21 [==============================] - 0s 22ms/step - loss: 0.1053 - accuracy: 0.9829 - val_loss: 1.1218 - val_accuracy: 0.5660\n",
      "Epoch 271/500\n",
      "21/21 [==============================] - 1s 26ms/step - loss: 0.1015 - accuracy: 0.9813 - val_loss: 1.1748 - val_accuracy: 0.5283\n",
      "Epoch 272/500\n",
      "21/21 [==============================] - 1s 27ms/step - loss: 0.1014 - accuracy: 0.9766 - val_loss: 1.1231 - val_accuracy: 0.5472\n",
      "Epoch 273/500\n",
      "21/21 [==============================] - 1s 25ms/step - loss: 0.0946 - accuracy: 0.9875 - val_loss: 1.1902 - val_accuracy: 0.4906\n",
      "Epoch 274/500\n",
      "21/21 [==============================] - 0s 23ms/step - loss: 0.0990 - accuracy: 0.9751 - val_loss: 1.2365 - val_accuracy: 0.5283\n",
      "Epoch 275/500\n",
      "21/21 [==============================] - 0s 24ms/step - loss: 0.0994 - accuracy: 0.9798 - val_loss: 1.2375 - val_accuracy: 0.5094\n",
      "Epoch 276/500\n",
      "21/21 [==============================] - 1s 25ms/step - loss: 0.0943 - accuracy: 0.9860 - val_loss: 1.1408 - val_accuracy: 0.5472\n",
      "Epoch 277/500\n",
      "21/21 [==============================] - 0s 22ms/step - loss: 0.0922 - accuracy: 0.9798 - val_loss: 1.1625 - val_accuracy: 0.5472\n",
      "Epoch 278/500\n",
      "21/21 [==============================] - 1s 25ms/step - loss: 0.1004 - accuracy: 0.9798 - val_loss: 1.1582 - val_accuracy: 0.5472\n",
      "Epoch 279/500\n",
      "21/21 [==============================] - 0s 22ms/step - loss: 0.0877 - accuracy: 0.9860 - val_loss: 1.2125 - val_accuracy: 0.5283\n",
      "Epoch 280/500\n",
      "21/21 [==============================] - 0s 21ms/step - loss: 0.0900 - accuracy: 0.9891 - val_loss: 1.1653 - val_accuracy: 0.5472\n",
      "Epoch 281/500\n",
      "21/21 [==============================] - 0s 24ms/step - loss: 0.0910 - accuracy: 0.9798 - val_loss: 1.1952 - val_accuracy: 0.5472\n",
      "Epoch 282/500\n",
      "21/21 [==============================] - 1s 25ms/step - loss: 0.0934 - accuracy: 0.9813 - val_loss: 1.2524 - val_accuracy: 0.5283\n",
      "Epoch 283/500\n",
      "21/21 [==============================] - 1s 24ms/step - loss: 0.1310 - accuracy: 0.9533 - val_loss: 1.2311 - val_accuracy: 0.5660\n",
      "Epoch 284/500\n",
      "21/21 [==============================] - 0s 24ms/step - loss: 0.0962 - accuracy: 0.9829 - val_loss: 1.1691 - val_accuracy: 0.5660\n",
      "Epoch 285/500\n",
      "21/21 [==============================] - 0s 23ms/step - loss: 0.0893 - accuracy: 0.9813 - val_loss: 1.1617 - val_accuracy: 0.5472\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 286/500\n",
      "21/21 [==============================] - 0s 23ms/step - loss: 0.0896 - accuracy: 0.9829 - val_loss: 1.1343 - val_accuracy: 0.5660\n",
      "Epoch 287/500\n",
      "21/21 [==============================] - 1s 25ms/step - loss: 0.0863 - accuracy: 0.9844 - val_loss: 1.1455 - val_accuracy: 0.5660\n",
      "Epoch 288/500\n",
      "21/21 [==============================] - 0s 23ms/step - loss: 0.1128 - accuracy: 0.9688 - val_loss: 1.2385 - val_accuracy: 0.5472\n",
      "Epoch 289/500\n",
      "21/21 [==============================] - 1s 25ms/step - loss: 0.0940 - accuracy: 0.9829 - val_loss: 1.2730 - val_accuracy: 0.5660\n",
      "Epoch 290/500\n",
      "21/21 [==============================] - 0s 23ms/step - loss: 0.1125 - accuracy: 0.9704 - val_loss: 1.1601 - val_accuracy: 0.5472\n",
      "Epoch 291/500\n",
      "21/21 [==============================] - 0s 23ms/step - loss: 0.1019 - accuracy: 0.9813 - val_loss: 1.1996 - val_accuracy: 0.5094\n",
      "Epoch 292/500\n",
      "21/21 [==============================] - 0s 23ms/step - loss: 0.0885 - accuracy: 0.9891 - val_loss: 1.2071 - val_accuracy: 0.5094\n",
      "Epoch 293/500\n",
      "21/21 [==============================] - 0s 23ms/step - loss: 0.0862 - accuracy: 0.9875 - val_loss: 1.1981 - val_accuracy: 0.5472\n",
      "Epoch 294/500\n",
      "21/21 [==============================] - 1s 24ms/step - loss: 0.0802 - accuracy: 0.9829 - val_loss: 1.2108 - val_accuracy: 0.5472\n",
      "Epoch 295/500\n",
      "21/21 [==============================] - 0s 22ms/step - loss: 0.0825 - accuracy: 0.9860 - val_loss: 1.1985 - val_accuracy: 0.5660\n",
      "Epoch 296/500\n",
      "21/21 [==============================] - 0s 22ms/step - loss: 0.0850 - accuracy: 0.9844 - val_loss: 1.2148 - val_accuracy: 0.5283\n",
      "Epoch 297/500\n",
      "21/21 [==============================] - 1s 24ms/step - loss: 0.0824 - accuracy: 0.9875 - val_loss: 1.1762 - val_accuracy: 0.5660\n",
      "Epoch 298/500\n",
      "21/21 [==============================] - 0s 24ms/step - loss: 0.0762 - accuracy: 0.9829 - val_loss: 1.2335 - val_accuracy: 0.5283\n",
      "Epoch 299/500\n",
      "21/21 [==============================] - 0s 23ms/step - loss: 0.0844 - accuracy: 0.9829 - val_loss: 1.1886 - val_accuracy: 0.5660\n",
      "Epoch 300/500\n",
      "21/21 [==============================] - 0s 21ms/step - loss: 0.0812 - accuracy: 0.9860 - val_loss: 1.1870 - val_accuracy: 0.5472\n",
      "Epoch 301/500\n",
      "21/21 [==============================] - 0s 21ms/step - loss: 0.0816 - accuracy: 0.9813 - val_loss: 1.1919 - val_accuracy: 0.5660\n",
      "Epoch 302/500\n",
      "21/21 [==============================] - 0s 24ms/step - loss: 0.0879 - accuracy: 0.9844 - val_loss: 1.2717 - val_accuracy: 0.5283\n",
      "Epoch 303/500\n",
      "21/21 [==============================] - 1s 25ms/step - loss: 0.0707 - accuracy: 0.9922 - val_loss: 1.2194 - val_accuracy: 0.5283\n",
      "Epoch 304/500\n",
      "21/21 [==============================] - 0s 23ms/step - loss: 0.0860 - accuracy: 0.9829 - val_loss: 1.2122 - val_accuracy: 0.5472\n",
      "Epoch 305/500\n",
      "21/21 [==============================] - 0s 22ms/step - loss: 0.0775 - accuracy: 0.9891 - val_loss: 1.1642 - val_accuracy: 0.5660\n",
      "Epoch 306/500\n",
      "21/21 [==============================] - 1s 24ms/step - loss: 0.0800 - accuracy: 0.9829 - val_loss: 1.2012 - val_accuracy: 0.5472\n",
      "Epoch 307/500\n",
      "21/21 [==============================] - 0s 23ms/step - loss: 0.0734 - accuracy: 0.9860 - val_loss: 1.2447 - val_accuracy: 0.5283\n",
      "Epoch 308/500\n",
      "21/21 [==============================] - 0s 24ms/step - loss: 0.0712 - accuracy: 0.9860 - val_loss: 1.2128 - val_accuracy: 0.5660\n",
      "Epoch 309/500\n",
      "21/21 [==============================] - 0s 23ms/step - loss: 0.0763 - accuracy: 0.9844 - val_loss: 1.2245 - val_accuracy: 0.5472\n",
      "Epoch 310/500\n",
      "21/21 [==============================] - 0s 22ms/step - loss: 0.0796 - accuracy: 0.9813 - val_loss: 1.1447 - val_accuracy: 0.5660\n",
      "Epoch 311/500\n",
      "21/21 [==============================] - 0s 23ms/step - loss: 0.0726 - accuracy: 0.9875 - val_loss: 1.2107 - val_accuracy: 0.5472\n",
      "Epoch 312/500\n",
      "21/21 [==============================] - 0s 23ms/step - loss: 0.0685 - accuracy: 0.9938 - val_loss: 1.2724 - val_accuracy: 0.5472\n",
      "Epoch 313/500\n",
      "21/21 [==============================] - 0s 24ms/step - loss: 0.0694 - accuracy: 0.9860 - val_loss: 1.2323 - val_accuracy: 0.5660\n",
      "Epoch 314/500\n",
      "21/21 [==============================] - 0s 24ms/step - loss: 0.0804 - accuracy: 0.9844 - val_loss: 1.2157 - val_accuracy: 0.5472\n",
      "Epoch 315/500\n",
      "21/21 [==============================] - 1s 25ms/step - loss: 0.0687 - accuracy: 0.9907 - val_loss: 1.1925 - val_accuracy: 0.5849\n",
      "Epoch 316/500\n",
      "21/21 [==============================] - 1s 24ms/step - loss: 0.0747 - accuracy: 0.9891 - val_loss: 1.2289 - val_accuracy: 0.5472\n",
      "Epoch 317/500\n",
      "21/21 [==============================] - 0s 24ms/step - loss: 0.0714 - accuracy: 0.9891 - val_loss: 1.2541 - val_accuracy: 0.5660\n",
      "Epoch 318/500\n",
      "21/21 [==============================] - 0s 24ms/step - loss: 0.0761 - accuracy: 0.9829 - val_loss: 1.2935 - val_accuracy: 0.5472\n",
      "Epoch 319/500\n",
      "21/21 [==============================] - 0s 23ms/step - loss: 0.0730 - accuracy: 0.9891 - val_loss: 1.2877 - val_accuracy: 0.5283\n",
      "Epoch 320/500\n",
      "21/21 [==============================] - 0s 22ms/step - loss: 0.0729 - accuracy: 0.9860 - val_loss: 1.2732 - val_accuracy: 0.5472\n",
      "Epoch 321/500\n",
      "21/21 [==============================] - 0s 23ms/step - loss: 0.0776 - accuracy: 0.9813 - val_loss: 1.2620 - val_accuracy: 0.5472\n",
      "Epoch 322/500\n",
      "21/21 [==============================] - 1s 24ms/step - loss: 0.0644 - accuracy: 0.9875 - val_loss: 1.3042 - val_accuracy: 0.5472\n",
      "Epoch 323/500\n",
      "21/21 [==============================] - 0s 23ms/step - loss: 0.0640 - accuracy: 0.9922 - val_loss: 1.3052 - val_accuracy: 0.5472\n",
      "Epoch 324/500\n",
      "21/21 [==============================] - 0s 23ms/step - loss: 0.0742 - accuracy: 0.9875 - val_loss: 1.2390 - val_accuracy: 0.5660\n",
      "Epoch 325/500\n",
      "21/21 [==============================] - 1s 25ms/step - loss: 0.0653 - accuracy: 0.9891 - val_loss: 1.3258 - val_accuracy: 0.5472\n",
      "Epoch 326/500\n",
      "21/21 [==============================] - 0s 22ms/step - loss: 0.0748 - accuracy: 0.9875 - val_loss: 1.3102 - val_accuracy: 0.5472\n",
      "Epoch 327/500\n",
      "21/21 [==============================] - 1s 24ms/step - loss: 0.0692 - accuracy: 0.9860 - val_loss: 1.3329 - val_accuracy: 0.5472\n",
      "Epoch 328/500\n",
      "21/21 [==============================] - 0s 23ms/step - loss: 0.0671 - accuracy: 0.9891 - val_loss: 1.2899 - val_accuracy: 0.5472\n",
      "Epoch 329/500\n",
      "21/21 [==============================] - 0s 24ms/step - loss: 0.0621 - accuracy: 0.9860 - val_loss: 1.2738 - val_accuracy: 0.5472\n",
      "Epoch 330/500\n",
      "21/21 [==============================] - 0s 21ms/step - loss: 0.0553 - accuracy: 0.9907 - val_loss: 1.2772 - val_accuracy: 0.5472\n",
      "Epoch 331/500\n",
      "21/21 [==============================] - 0s 23ms/step - loss: 0.0645 - accuracy: 0.9860 - val_loss: 1.4170 - val_accuracy: 0.5472\n",
      "Epoch 332/500\n",
      "21/21 [==============================] - 0s 24ms/step - loss: 0.0761 - accuracy: 0.9782 - val_loss: 1.3197 - val_accuracy: 0.5472\n",
      "Epoch 333/500\n",
      "21/21 [==============================] - 0s 22ms/step - loss: 0.0675 - accuracy: 0.9844 - val_loss: 1.3176 - val_accuracy: 0.5660\n",
      "Epoch 334/500\n",
      "21/21 [==============================] - 0s 24ms/step - loss: 0.0607 - accuracy: 0.9891 - val_loss: 1.3136 - val_accuracy: 0.5472\n",
      "Epoch 335/500\n",
      "21/21 [==============================] - 1s 25ms/step - loss: 0.0576 - accuracy: 0.9891 - val_loss: 1.3264 - val_accuracy: 0.5283\n",
      "Epoch 336/500\n",
      "21/21 [==============================] - 0s 22ms/step - loss: 0.0686 - accuracy: 0.9813 - val_loss: 1.3016 - val_accuracy: 0.5472\n",
      "Epoch 337/500\n",
      "21/21 [==============================] - 0s 22ms/step - loss: 0.0507 - accuracy: 0.9922 - val_loss: 1.3723 - val_accuracy: 0.5472\n",
      "Epoch 338/500\n",
      "21/21 [==============================] - 0s 22ms/step - loss: 0.0519 - accuracy: 0.9938 - val_loss: 1.3152 - val_accuracy: 0.5472\n",
      "Epoch 339/500\n",
      "21/21 [==============================] - 0s 23ms/step - loss: 0.0546 - accuracy: 0.9907 - val_loss: 1.3126 - val_accuracy: 0.5660\n",
      "Epoch 340/500\n",
      "21/21 [==============================] - 0s 22ms/step - loss: 0.0578 - accuracy: 0.9891 - val_loss: 1.2801 - val_accuracy: 0.5849\n",
      "Epoch 341/500\n",
      "21/21 [==============================] - 1s 25ms/step - loss: 0.0577 - accuracy: 0.9875 - val_loss: 1.3134 - val_accuracy: 0.5472\n",
      "Epoch 342/500\n",
      "21/21 [==============================] - 0s 22ms/step - loss: 0.0505 - accuracy: 0.9969 - val_loss: 1.3653 - val_accuracy: 0.5283\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 343/500\n",
      "21/21 [==============================] - 0s 21ms/step - loss: 0.0511 - accuracy: 0.9938 - val_loss: 1.3343 - val_accuracy: 0.5472\n",
      "Epoch 344/500\n",
      "21/21 [==============================] - 0s 23ms/step - loss: 0.0492 - accuracy: 0.9938 - val_loss: 1.2981 - val_accuracy: 0.5660\n",
      "Epoch 345/500\n",
      "21/21 [==============================] - 0s 23ms/step - loss: 0.0500 - accuracy: 0.9922 - val_loss: 1.3612 - val_accuracy: 0.5283\n",
      "Epoch 346/500\n",
      "21/21 [==============================] - 0s 23ms/step - loss: 0.0573 - accuracy: 0.9891 - val_loss: 1.4389 - val_accuracy: 0.5283\n",
      "Epoch 347/500\n",
      "21/21 [==============================] - 1s 24ms/step - loss: 0.0488 - accuracy: 0.9891 - val_loss: 1.3751 - val_accuracy: 0.5472\n",
      "Epoch 348/500\n",
      "21/21 [==============================] - 0s 23ms/step - loss: 0.0542 - accuracy: 0.9907 - val_loss: 1.3437 - val_accuracy: 0.5660\n",
      "Epoch 349/500\n",
      "21/21 [==============================] - 0s 23ms/step - loss: 0.0474 - accuracy: 0.9953 - val_loss: 1.3095 - val_accuracy: 0.5660\n",
      "Epoch 350/500\n",
      "21/21 [==============================] - 0s 24ms/step - loss: 0.0600 - accuracy: 0.9875 - val_loss: 1.3558 - val_accuracy: 0.5472\n",
      "Epoch 351/500\n",
      "21/21 [==============================] - 0s 23ms/step - loss: 0.0408 - accuracy: 0.9953 - val_loss: 1.3846 - val_accuracy: 0.5660\n",
      "Epoch 352/500\n",
      "21/21 [==============================] - 0s 22ms/step - loss: 0.0467 - accuracy: 0.9953 - val_loss: 1.4346 - val_accuracy: 0.5660\n",
      "Epoch 353/500\n",
      "21/21 [==============================] - 1s 24ms/step - loss: 0.0635 - accuracy: 0.9829 - val_loss: 1.3837 - val_accuracy: 0.5660\n",
      "Epoch 354/500\n",
      "21/21 [==============================] - 1s 24ms/step - loss: 0.0478 - accuracy: 0.9938 - val_loss: 1.3609 - val_accuracy: 0.5472\n",
      "Epoch 355/500\n",
      "21/21 [==============================] - 0s 23ms/step - loss: 0.0479 - accuracy: 0.9922 - val_loss: 1.3897 - val_accuracy: 0.5660\n",
      "Epoch 356/500\n",
      "21/21 [==============================] - 0s 22ms/step - loss: 0.0473 - accuracy: 0.9922 - val_loss: 1.3946 - val_accuracy: 0.5472\n",
      "Epoch 357/500\n",
      "21/21 [==============================] - 0s 22ms/step - loss: 0.0451 - accuracy: 0.9907 - val_loss: 1.3778 - val_accuracy: 0.5660\n",
      "Epoch 358/500\n",
      "21/21 [==============================] - 0s 23ms/step - loss: 0.0483 - accuracy: 0.9969 - val_loss: 1.3724 - val_accuracy: 0.5660\n",
      "Epoch 359/500\n",
      "21/21 [==============================] - 0s 24ms/step - loss: 0.0427 - accuracy: 0.9938 - val_loss: 1.3958 - val_accuracy: 0.5660\n",
      "Epoch 360/500\n",
      "21/21 [==============================] - 1s 24ms/step - loss: 0.0474 - accuracy: 0.9907 - val_loss: 1.4093 - val_accuracy: 0.5472\n",
      "Epoch 361/500\n",
      "21/21 [==============================] - 0s 24ms/step - loss: 0.0457 - accuracy: 0.9953 - val_loss: 1.4268 - val_accuracy: 0.5472\n",
      "Epoch 362/500\n",
      "21/21 [==============================] - 0s 22ms/step - loss: 0.0521 - accuracy: 0.9907 - val_loss: 1.4493 - val_accuracy: 0.5472\n",
      "Epoch 363/500\n",
      "21/21 [==============================] - 1s 24ms/step - loss: 0.0453 - accuracy: 0.9922 - val_loss: 1.3993 - val_accuracy: 0.5472\n",
      "Epoch 364/500\n",
      "21/21 [==============================] - 0s 23ms/step - loss: 0.0382 - accuracy: 0.9969 - val_loss: 1.5205 - val_accuracy: 0.5283\n",
      "Epoch 365/500\n",
      "21/21 [==============================] - 0s 22ms/step - loss: 0.0547 - accuracy: 0.9860 - val_loss: 1.4795 - val_accuracy: 0.5472\n",
      "Epoch 366/500\n",
      "21/21 [==============================] - 0s 22ms/step - loss: 0.0489 - accuracy: 0.9907 - val_loss: 1.5167 - val_accuracy: 0.5472\n",
      "Epoch 367/500\n",
      "21/21 [==============================] - 0s 23ms/step - loss: 0.0476 - accuracy: 0.9907 - val_loss: 1.4574 - val_accuracy: 0.5472\n",
      "Epoch 368/500\n",
      "21/21 [==============================] - 0s 23ms/step - loss: 0.0381 - accuracy: 0.9953 - val_loss: 1.4285 - val_accuracy: 0.5472\n",
      "Epoch 369/500\n",
      "21/21 [==============================] - 0s 24ms/step - loss: 0.0453 - accuracy: 0.9891 - val_loss: 1.4298 - val_accuracy: 0.5660\n",
      "Epoch 370/500\n",
      "21/21 [==============================] - 0s 23ms/step - loss: 0.0408 - accuracy: 0.9922 - val_loss: 1.3712 - val_accuracy: 0.5849\n",
      "Epoch 371/500\n",
      "21/21 [==============================] - 0s 24ms/step - loss: 0.0393 - accuracy: 0.9953 - val_loss: 1.4198 - val_accuracy: 0.5472\n",
      "Epoch 372/500\n",
      "21/21 [==============================] - 0s 23ms/step - loss: 0.0383 - accuracy: 0.9953 - val_loss: 1.4266 - val_accuracy: 0.5660\n",
      "Epoch 373/500\n",
      "21/21 [==============================] - 0s 23ms/step - loss: 0.0403 - accuracy: 0.9907 - val_loss: 1.4101 - val_accuracy: 0.5660\n",
      "Epoch 374/500\n",
      "21/21 [==============================] - 0s 24ms/step - loss: 0.0440 - accuracy: 0.9922 - val_loss: 1.4451 - val_accuracy: 0.5660\n",
      "Epoch 375/500\n",
      "21/21 [==============================] - 0s 23ms/step - loss: 0.0416 - accuracy: 0.9907 - val_loss: 1.4486 - val_accuracy: 0.5660\n",
      "Epoch 376/500\n",
      "21/21 [==============================] - 0s 24ms/step - loss: 0.0375 - accuracy: 0.9969 - val_loss: 1.4036 - val_accuracy: 0.5660\n",
      "Epoch 377/500\n",
      "21/21 [==============================] - 0s 21ms/step - loss: 0.0403 - accuracy: 0.9907 - val_loss: 1.4949 - val_accuracy: 0.5472\n",
      "Epoch 378/500\n",
      "21/21 [==============================] - 1s 25ms/step - loss: 0.0399 - accuracy: 0.9922 - val_loss: 1.5398 - val_accuracy: 0.5283\n",
      "Epoch 379/500\n",
      "21/21 [==============================] - 1s 24ms/step - loss: 0.0389 - accuracy: 0.9938 - val_loss: 1.4342 - val_accuracy: 0.5660\n",
      "Epoch 380/500\n",
      "21/21 [==============================] - 0s 23ms/step - loss: 0.0389 - accuracy: 0.9953 - val_loss: 1.4124 - val_accuracy: 0.5660\n",
      "Epoch 381/500\n",
      "21/21 [==============================] - 0s 22ms/step - loss: 0.0325 - accuracy: 0.9984 - val_loss: 1.4262 - val_accuracy: 0.5660\n",
      "Epoch 382/500\n",
      "21/21 [==============================] - 0s 21ms/step - loss: 0.0328 - accuracy: 0.9969 - val_loss: 1.4647 - val_accuracy: 0.5472\n",
      "Epoch 383/500\n",
      "21/21 [==============================] - 0s 23ms/step - loss: 0.0323 - accuracy: 0.9969 - val_loss: 1.4633 - val_accuracy: 0.5472\n",
      "Epoch 384/500\n",
      "21/21 [==============================] - 0s 22ms/step - loss: 0.0342 - accuracy: 0.9984 - val_loss: 1.4739 - val_accuracy: 0.5472\n",
      "Epoch 385/500\n",
      "21/21 [==============================] - 0s 22ms/step - loss: 0.0312 - accuracy: 0.9953 - val_loss: 1.4107 - val_accuracy: 0.5660\n",
      "Epoch 386/500\n",
      "21/21 [==============================] - 0s 22ms/step - loss: 0.0546 - accuracy: 0.9875 - val_loss: 1.4240 - val_accuracy: 0.5849\n",
      "Epoch 387/500\n",
      "21/21 [==============================] - 1s 25ms/step - loss: 0.0325 - accuracy: 0.9953 - val_loss: 1.6137 - val_accuracy: 0.5283\n",
      "Epoch 388/500\n",
      "21/21 [==============================] - 0s 22ms/step - loss: 0.0411 - accuracy: 0.9922 - val_loss: 1.5213 - val_accuracy: 0.5472\n",
      "Epoch 389/500\n",
      "21/21 [==============================] - 0s 22ms/step - loss: 0.0363 - accuracy: 0.9953 - val_loss: 1.4502 - val_accuracy: 0.5660\n",
      "Epoch 390/500\n",
      "21/21 [==============================] - 0s 23ms/step - loss: 0.0338 - accuracy: 0.9969 - val_loss: 1.4744 - val_accuracy: 0.5660\n",
      "Epoch 391/500\n",
      "21/21 [==============================] - 0s 22ms/step - loss: 0.0366 - accuracy: 0.9938 - val_loss: 1.4610 - val_accuracy: 0.5660\n",
      "Epoch 392/500\n",
      "21/21 [==============================] - 0s 21ms/step - loss: 0.0359 - accuracy: 0.9922 - val_loss: 1.4653 - val_accuracy: 0.5660\n",
      "Epoch 393/500\n",
      "21/21 [==============================] - 1s 24ms/step - loss: 0.0345 - accuracy: 0.9938 - val_loss: 1.5316 - val_accuracy: 0.5472\n",
      "Epoch 394/500\n",
      "21/21 [==============================] - 0s 23ms/step - loss: 0.0257 - accuracy: 0.9984 - val_loss: 1.5311 - val_accuracy: 0.5660\n",
      "Epoch 395/500\n",
      "21/21 [==============================] - 1s 25ms/step - loss: 0.0319 - accuracy: 0.9969 - val_loss: 1.5361 - val_accuracy: 0.5660\n",
      "Epoch 396/500\n",
      "21/21 [==============================] - 0s 24ms/step - loss: 0.0329 - accuracy: 0.9953 - val_loss: 1.5652 - val_accuracy: 0.5660\n",
      "Epoch 397/500\n",
      "21/21 [==============================] - 1s 25ms/step - loss: 0.0359 - accuracy: 0.9938 - val_loss: 1.5932 - val_accuracy: 0.5472\n",
      "Epoch 398/500\n",
      "21/21 [==============================] - 0s 23ms/step - loss: 0.0292 - accuracy: 0.9938 - val_loss: 1.5558 - val_accuracy: 0.5660\n",
      "Epoch 399/500\n",
      "21/21 [==============================] - 0s 23ms/step - loss: 0.0321 - accuracy: 0.9938 - val_loss: 1.4571 - val_accuracy: 0.6226\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 400/500\n",
      "21/21 [==============================] - 1s 24ms/step - loss: 0.0307 - accuracy: 0.9969 - val_loss: 1.4728 - val_accuracy: 0.5660\n",
      "Epoch 401/500\n",
      "21/21 [==============================] - 1s 24ms/step - loss: 0.0312 - accuracy: 0.9938 - val_loss: 1.5346 - val_accuracy: 0.5660\n",
      "Epoch 402/500\n",
      "21/21 [==============================] - 0s 23ms/step - loss: 0.0310 - accuracy: 0.9953 - val_loss: 1.5196 - val_accuracy: 0.5660\n",
      "Epoch 403/500\n",
      "21/21 [==============================] - 0s 23ms/step - loss: 0.0279 - accuracy: 0.9984 - val_loss: 1.4997 - val_accuracy: 0.5660\n",
      "Epoch 404/500\n",
      "21/21 [==============================] - 1s 25ms/step - loss: 0.0282 - accuracy: 0.9969 - val_loss: 1.5849 - val_accuracy: 0.5660\n",
      "Epoch 405/500\n",
      "21/21 [==============================] - 1s 24ms/step - loss: 0.0282 - accuracy: 0.9984 - val_loss: 1.5883 - val_accuracy: 0.5660\n",
      "Epoch 406/500\n",
      "21/21 [==============================] - 1s 24ms/step - loss: 0.0390 - accuracy: 0.9953 - val_loss: 1.5971 - val_accuracy: 0.5472\n",
      "Epoch 407/500\n",
      "21/21 [==============================] - 1s 24ms/step - loss: 0.0277 - accuracy: 0.9969 - val_loss: 1.5478 - val_accuracy: 0.5660\n",
      "Epoch 408/500\n",
      "21/21 [==============================] - 0s 22ms/step - loss: 0.0360 - accuracy: 0.9922 - val_loss: 1.5032 - val_accuracy: 0.5660\n",
      "Epoch 409/500\n",
      "21/21 [==============================] - 1s 24ms/step - loss: 0.0331 - accuracy: 0.9922 - val_loss: 1.5203 - val_accuracy: 0.5660\n",
      "Epoch 410/500\n",
      "21/21 [==============================] - 0s 24ms/step - loss: 0.0285 - accuracy: 0.9953 - val_loss: 1.5646 - val_accuracy: 0.5660\n",
      "Epoch 411/500\n",
      "21/21 [==============================] - 1s 25ms/step - loss: 0.0232 - accuracy: 0.9969 - val_loss: 1.5730 - val_accuracy: 0.5472\n",
      "Epoch 412/500\n",
      "21/21 [==============================] - 0s 23ms/step - loss: 0.0309 - accuracy: 0.9938 - val_loss: 1.5710 - val_accuracy: 0.5472\n",
      "Epoch 413/500\n",
      "21/21 [==============================] - 0s 24ms/step - loss: 0.0355 - accuracy: 0.9875 - val_loss: 1.5244 - val_accuracy: 0.5660\n",
      "Epoch 414/500\n",
      "21/21 [==============================] - 0s 22ms/step - loss: 0.0271 - accuracy: 0.9969 - val_loss: 1.5661 - val_accuracy: 0.5660\n",
      "Epoch 415/500\n",
      "21/21 [==============================] - 0s 24ms/step - loss: 0.0284 - accuracy: 0.9953 - val_loss: 1.6326 - val_accuracy: 0.5660\n",
      "Epoch 416/500\n",
      "21/21 [==============================] - 0s 23ms/step - loss: 0.0264 - accuracy: 0.9984 - val_loss: 1.6271 - val_accuracy: 0.5660\n",
      "Epoch 417/500\n",
      "21/21 [==============================] - 0s 23ms/step - loss: 0.0249 - accuracy: 0.9953 - val_loss: 1.5448 - val_accuracy: 0.5660\n",
      "Epoch 418/500\n",
      "21/21 [==============================] - 0s 23ms/step - loss: 0.0258 - accuracy: 0.9969 - val_loss: 1.6028 - val_accuracy: 0.5660\n",
      "Epoch 419/500\n",
      "21/21 [==============================] - 1s 26ms/step - loss: 0.0247 - accuracy: 0.9984 - val_loss: 1.5831 - val_accuracy: 0.5660\n",
      "Epoch 420/500\n",
      "21/21 [==============================] - 1s 26ms/step - loss: 0.0257 - accuracy: 0.9969 - val_loss: 1.5557 - val_accuracy: 0.5660\n",
      "Epoch 421/500\n",
      "21/21 [==============================] - 1s 24ms/step - loss: 0.0308 - accuracy: 0.9969 - val_loss: 1.6158 - val_accuracy: 0.5660\n",
      "Epoch 422/500\n",
      "21/21 [==============================] - 0s 24ms/step - loss: 0.0222 - accuracy: 0.9984 - val_loss: 1.5765 - val_accuracy: 0.5660\n",
      "Epoch 423/500\n",
      "21/21 [==============================] - 0s 23ms/step - loss: 0.0241 - accuracy: 0.9969 - val_loss: 1.6157 - val_accuracy: 0.5660\n",
      "Epoch 424/500\n",
      "21/21 [==============================] - 1s 25ms/step - loss: 0.0225 - accuracy: 0.9984 - val_loss: 1.5888 - val_accuracy: 0.5660\n",
      "Epoch 425/500\n",
      "21/21 [==============================] - 0s 23ms/step - loss: 0.0244 - accuracy: 1.0000 - val_loss: 1.6870 - val_accuracy: 0.5283\n",
      "Epoch 426/500\n",
      "21/21 [==============================] - 0s 23ms/step - loss: 0.0282 - accuracy: 0.9922 - val_loss: 1.5958 - val_accuracy: 0.5660\n",
      "Epoch 427/500\n",
      "21/21 [==============================] - 0s 24ms/step - loss: 0.0206 - accuracy: 0.9984 - val_loss: 1.6528 - val_accuracy: 0.5660\n",
      "Epoch 428/500\n",
      "21/21 [==============================] - 1s 25ms/step - loss: 0.0253 - accuracy: 0.9969 - val_loss: 1.6172 - val_accuracy: 0.5660\n",
      "Epoch 429/500\n",
      "21/21 [==============================] - 1s 25ms/step - loss: 0.0256 - accuracy: 0.9969 - val_loss: 1.6452 - val_accuracy: 0.5660\n",
      "Epoch 430/500\n",
      "21/21 [==============================] - 0s 22ms/step - loss: 0.0202 - accuracy: 1.0000 - val_loss: 1.6597 - val_accuracy: 0.5660\n",
      "Epoch 431/500\n",
      "21/21 [==============================] - 0s 22ms/step - loss: 0.0290 - accuracy: 0.9953 - val_loss: 1.6435 - val_accuracy: 0.5472\n",
      "Epoch 432/500\n",
      "21/21 [==============================] - 0s 22ms/step - loss: 0.0256 - accuracy: 0.9953 - val_loss: 1.6353 - val_accuracy: 0.5660\n",
      "Epoch 433/500\n",
      "21/21 [==============================] - 0s 23ms/step - loss: 0.0208 - accuracy: 0.9984 - val_loss: 1.6519 - val_accuracy: 0.5472\n",
      "Epoch 434/500\n",
      "21/21 [==============================] - 0s 22ms/step - loss: 0.0224 - accuracy: 0.9984 - val_loss: 1.5920 - val_accuracy: 0.5849\n",
      "Epoch 435/500\n",
      "21/21 [==============================] - 0s 23ms/step - loss: 0.0188 - accuracy: 0.9984 - val_loss: 1.6626 - val_accuracy: 0.5660\n",
      "Epoch 436/500\n",
      "21/21 [==============================] - 0s 23ms/step - loss: 0.0180 - accuracy: 1.0000 - val_loss: 1.6368 - val_accuracy: 0.5660\n",
      "Epoch 437/500\n",
      "21/21 [==============================] - 0s 22ms/step - loss: 0.0210 - accuracy: 0.9969 - val_loss: 1.6524 - val_accuracy: 0.5660\n",
      "Epoch 438/500\n",
      "21/21 [==============================] - 0s 21ms/step - loss: 0.0298 - accuracy: 0.9953 - val_loss: 1.6239 - val_accuracy: 0.5849\n",
      "Epoch 439/500\n",
      "21/21 [==============================] - 0s 22ms/step - loss: 0.0283 - accuracy: 0.9907 - val_loss: 1.6435 - val_accuracy: 0.5660\n",
      "Epoch 440/500\n",
      "21/21 [==============================] - 0s 22ms/step - loss: 0.0271 - accuracy: 0.9938 - val_loss: 1.6371 - val_accuracy: 0.5849\n",
      "Epoch 441/500\n",
      "21/21 [==============================] - 0s 23ms/step - loss: 0.0284 - accuracy: 0.9922 - val_loss: 1.6750 - val_accuracy: 0.5472\n",
      "Epoch 442/500\n",
      "21/21 [==============================] - 0s 23ms/step - loss: 0.0253 - accuracy: 0.9953 - val_loss: 1.6852 - val_accuracy: 0.5660\n",
      "Epoch 443/500\n",
      "21/21 [==============================] - 0s 22ms/step - loss: 0.0236 - accuracy: 0.9984 - val_loss: 1.6653 - val_accuracy: 0.5660\n",
      "Epoch 444/500\n",
      "21/21 [==============================] - 1s 24ms/step - loss: 0.0179 - accuracy: 1.0000 - val_loss: 1.6684 - val_accuracy: 0.5660\n",
      "Epoch 445/500\n",
      "21/21 [==============================] - 1s 24ms/step - loss: 0.0185 - accuracy: 1.0000 - val_loss: 1.7356 - val_accuracy: 0.5660\n",
      "Epoch 446/500\n",
      "21/21 [==============================] - 0s 23ms/step - loss: 0.0256 - accuracy: 0.9938 - val_loss: 1.7406 - val_accuracy: 0.5660\n",
      "Epoch 447/500\n",
      "21/21 [==============================] - 0s 23ms/step - loss: 0.0241 - accuracy: 0.9938 - val_loss: 1.7335 - val_accuracy: 0.5660\n",
      "Epoch 448/500\n",
      "21/21 [==============================] - 0s 22ms/step - loss: 0.0200 - accuracy: 0.9984 - val_loss: 1.7121 - val_accuracy: 0.5660\n",
      "Epoch 449/500\n",
      "21/21 [==============================] - 0s 24ms/step - loss: 0.0205 - accuracy: 0.9984 - val_loss: 1.7622 - val_accuracy: 0.5660\n",
      "Epoch 450/500\n",
      "21/21 [==============================] - 0s 23ms/step - loss: 0.0239 - accuracy: 0.9938 - val_loss: 1.7435 - val_accuracy: 0.5472\n",
      "Epoch 451/500\n",
      "21/21 [==============================] - 0s 23ms/step - loss: 0.0190 - accuracy: 0.9969 - val_loss: 1.7244 - val_accuracy: 0.5660\n",
      "Epoch 452/500\n",
      "21/21 [==============================] - 0s 22ms/step - loss: 0.0206 - accuracy: 0.9969 - val_loss: 1.7036 - val_accuracy: 0.5660\n",
      "Epoch 453/500\n",
      "21/21 [==============================] - 0s 21ms/step - loss: 0.0194 - accuracy: 0.9984 - val_loss: 1.7778 - val_accuracy: 0.5472\n",
      "Epoch 454/500\n",
      "21/21 [==============================] - 0s 23ms/step - loss: 0.0152 - accuracy: 1.0000 - val_loss: 1.6937 - val_accuracy: 0.5660\n",
      "Epoch 455/500\n",
      "21/21 [==============================] - 1s 25ms/step - loss: 0.0198 - accuracy: 0.9969 - val_loss: 1.7643 - val_accuracy: 0.5472\n",
      "Epoch 456/500\n",
      "21/21 [==============================] - 0s 23ms/step - loss: 0.0196 - accuracy: 0.9969 - val_loss: 1.7078 - val_accuracy: 0.5472\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 457/500\n",
      "21/21 [==============================] - 0s 22ms/step - loss: 0.0174 - accuracy: 0.9969 - val_loss: 1.7093 - val_accuracy: 0.5472\n",
      "Epoch 458/500\n",
      "21/21 [==============================] - 0s 23ms/step - loss: 0.0137 - accuracy: 1.0000 - val_loss: 1.7333 - val_accuracy: 0.5660\n",
      "Epoch 459/500\n",
      "21/21 [==============================] - 0s 24ms/step - loss: 0.0154 - accuracy: 1.0000 - val_loss: 1.7429 - val_accuracy: 0.5660\n",
      "Epoch 460/500\n",
      "21/21 [==============================] - 0s 22ms/step - loss: 0.0163 - accuracy: 0.9984 - val_loss: 1.7100 - val_accuracy: 0.5660\n",
      "Epoch 461/500\n",
      "21/21 [==============================] - 0s 23ms/step - loss: 0.0175 - accuracy: 0.9984 - val_loss: 1.6881 - val_accuracy: 0.5660\n",
      "Epoch 462/500\n",
      "21/21 [==============================] - 1s 25ms/step - loss: 0.0320 - accuracy: 0.9938 - val_loss: 1.7101 - val_accuracy: 0.5660\n",
      "Epoch 463/500\n",
      "21/21 [==============================] - 0s 24ms/step - loss: 0.0158 - accuracy: 1.0000 - val_loss: 1.7431 - val_accuracy: 0.5660\n",
      "Epoch 464/500\n",
      "21/21 [==============================] - 0s 24ms/step - loss: 0.0147 - accuracy: 1.0000 - val_loss: 1.7487 - val_accuracy: 0.5660\n",
      "Epoch 465/500\n",
      "21/21 [==============================] - 0s 24ms/step - loss: 0.0138 - accuracy: 1.0000 - val_loss: 1.7177 - val_accuracy: 0.5660\n",
      "Epoch 466/500\n",
      "21/21 [==============================] - 1s 25ms/step - loss: 0.0141 - accuracy: 1.0000 - val_loss: 1.6819 - val_accuracy: 0.5660\n",
      "Epoch 467/500\n",
      "21/21 [==============================] - 1s 25ms/step - loss: 0.0170 - accuracy: 1.0000 - val_loss: 1.7359 - val_accuracy: 0.5660\n",
      "Epoch 468/500\n",
      "21/21 [==============================] - 1s 24ms/step - loss: 0.0184 - accuracy: 0.9969 - val_loss: 1.7466 - val_accuracy: 0.5660\n",
      "Epoch 469/500\n",
      "21/21 [==============================] - 1s 25ms/step - loss: 0.0241 - accuracy: 0.9953 - val_loss: 1.9088 - val_accuracy: 0.5094\n",
      "Epoch 470/500\n",
      "21/21 [==============================] - 1s 26ms/step - loss: 0.0174 - accuracy: 0.9984 - val_loss: 1.8526 - val_accuracy: 0.5472\n",
      "Epoch 471/500\n",
      "21/21 [==============================] - 1s 25ms/step - loss: 0.0235 - accuracy: 0.9953 - val_loss: 1.7815 - val_accuracy: 0.5660\n",
      "Epoch 472/500\n",
      "21/21 [==============================] - 1s 24ms/step - loss: 0.0149 - accuracy: 1.0000 - val_loss: 1.7901 - val_accuracy: 0.5472\n",
      "Epoch 473/500\n",
      "21/21 [==============================] - 0s 24ms/step - loss: 0.0157 - accuracy: 1.0000 - val_loss: 1.7293 - val_accuracy: 0.5660\n",
      "Epoch 474/500\n",
      "21/21 [==============================] - 1s 25ms/step - loss: 0.0133 - accuracy: 1.0000 - val_loss: 1.7539 - val_accuracy: 0.5849\n",
      "Epoch 475/500\n",
      "21/21 [==============================] - 1s 24ms/step - loss: 0.0164 - accuracy: 0.9984 - val_loss: 1.7656 - val_accuracy: 0.5660\n",
      "Epoch 476/500\n",
      "21/21 [==============================] - 0s 23ms/step - loss: 0.0163 - accuracy: 1.0000 - val_loss: 1.7798 - val_accuracy: 0.5660\n",
      "Epoch 477/500\n",
      "21/21 [==============================] - 1s 24ms/step - loss: 0.0278 - accuracy: 0.9953 - val_loss: 1.7255 - val_accuracy: 0.6038\n",
      "Epoch 478/500\n",
      "21/21 [==============================] - 0s 23ms/step - loss: 0.0186 - accuracy: 0.9984 - val_loss: 1.7499 - val_accuracy: 0.5660\n",
      "Epoch 479/500\n",
      "21/21 [==============================] - 0s 21ms/step - loss: 0.0142 - accuracy: 1.0000 - val_loss: 1.7708 - val_accuracy: 0.5849\n",
      "Epoch 480/500\n",
      "21/21 [==============================] - 0s 23ms/step - loss: 0.0116 - accuracy: 1.0000 - val_loss: 1.7715 - val_accuracy: 0.5849\n",
      "Epoch 481/500\n",
      "21/21 [==============================] - 0s 22ms/step - loss: 0.0293 - accuracy: 0.9938 - val_loss: 1.8428 - val_accuracy: 0.5660\n",
      "Epoch 482/500\n",
      "21/21 [==============================] - 0s 22ms/step - loss: 0.0169 - accuracy: 0.9984 - val_loss: 1.8723 - val_accuracy: 0.5472\n",
      "Epoch 483/500\n",
      "21/21 [==============================] - 0s 22ms/step - loss: 0.0203 - accuracy: 0.9953 - val_loss: 1.8564 - val_accuracy: 0.5472\n",
      "Epoch 484/500\n",
      "21/21 [==============================] - 0s 23ms/step - loss: 0.0165 - accuracy: 1.0000 - val_loss: 1.7249 - val_accuracy: 0.6038\n",
      "Epoch 485/500\n",
      "21/21 [==============================] - 1s 25ms/step - loss: 0.0156 - accuracy: 1.0000 - val_loss: 1.7167 - val_accuracy: 0.5849\n",
      "Epoch 486/500\n",
      "21/21 [==============================] - 0s 23ms/step - loss: 0.0131 - accuracy: 0.9984 - val_loss: 1.7645 - val_accuracy: 0.5660\n",
      "Epoch 487/500\n",
      "21/21 [==============================] - 0s 22ms/step - loss: 0.0152 - accuracy: 0.9984 - val_loss: 1.7357 - val_accuracy: 0.5660\n",
      "Epoch 488/500\n",
      "21/21 [==============================] - 1s 25ms/step - loss: 0.0121 - accuracy: 1.0000 - val_loss: 1.7493 - val_accuracy: 0.6038\n",
      "Epoch 489/500\n",
      "21/21 [==============================] - 0s 22ms/step - loss: 0.0154 - accuracy: 0.9969 - val_loss: 1.7705 - val_accuracy: 0.5660\n",
      "Epoch 490/500\n",
      "21/21 [==============================] - 0s 23ms/step - loss: 0.0170 - accuracy: 0.9953 - val_loss: 1.7516 - val_accuracy: 0.5660\n",
      "Epoch 491/500\n",
      "21/21 [==============================] - 0s 23ms/step - loss: 0.0161 - accuracy: 0.9969 - val_loss: 1.8162 - val_accuracy: 0.5660\n",
      "Epoch 492/500\n",
      "21/21 [==============================] - 0s 22ms/step - loss: 0.0120 - accuracy: 1.0000 - val_loss: 1.8894 - val_accuracy: 0.5660\n",
      "Epoch 493/500\n",
      "21/21 [==============================] - 0s 22ms/step - loss: 0.0108 - accuracy: 1.0000 - val_loss: 1.8330 - val_accuracy: 0.5660\n",
      "Epoch 494/500\n",
      "21/21 [==============================] - 0s 22ms/step - loss: 0.0145 - accuracy: 0.9984 - val_loss: 1.8626 - val_accuracy: 0.5660\n",
      "Epoch 495/500\n",
      "21/21 [==============================] - 0s 24ms/step - loss: 0.0095 - accuracy: 1.0000 - val_loss: 1.9153 - val_accuracy: 0.5660\n",
      "Epoch 496/500\n",
      "21/21 [==============================] - 1s 24ms/step - loss: 0.0099 - accuracy: 1.0000 - val_loss: 1.8220 - val_accuracy: 0.5660\n",
      "Epoch 497/500\n",
      "21/21 [==============================] - 0s 21ms/step - loss: 0.0097 - accuracy: 1.0000 - val_loss: 1.8887 - val_accuracy: 0.5472\n",
      "Epoch 498/500\n",
      "21/21 [==============================] - 1s 25ms/step - loss: 0.0124 - accuracy: 0.9984 - val_loss: 1.8003 - val_accuracy: 0.5660\n",
      "Epoch 499/500\n",
      "21/21 [==============================] - 0s 22ms/step - loss: 0.0147 - accuracy: 0.9969 - val_loss: 1.8564 - val_accuracy: 0.5283\n",
      "Epoch 500/500\n",
      "21/21 [==============================] - 1s 24ms/step - loss: 0.0124 - accuracy: 0.9984 - val_loss: 1.8312 - val_accuracy: 0.6038\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f81000171f0>"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_2_500.compile(optimizer=opt1, \n",
    "                loss='categorical_crossentropy', \n",
    "                metrics=['accuracy'])\n",
    "#Here we use cross-entropy as the criteria for loss.\n",
    "model_2_500.fit(X_train_over, y_train_over, \n",
    "            validation_data=(X_val, y_val), \n",
    "            epochs=500, verbose=True, \n",
    "            callbacks=[early_stopping_monitor])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "d9f7eb4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 4ms/step - loss: 1.9019 - accuracy: 0.6441\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 1.8312 - accuracy: 0.6038\n"
     ]
    }
   ],
   "source": [
    "m2_eval_test = model_2_500.evaluate(X_test, y_test)\n",
    "m2_eval_val = model_2_500.evaluate(X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "c04e5058",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 6ms/step\n",
      "roc auc score:  0.5726817042606517\n",
      "average precision score:  0.6156977792986061\n"
     ]
    }
   ],
   "source": [
    "pred = model_2_500.predict(X_test)\n",
    "roc_value = roc_auc_score(y_test, pred)\n",
    "ap_score = average_precision_score(y_test, pred)\n",
    "print('roc auc score: ', roc_value)\n",
    "print('average precision score: ', ap_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "ce0466d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = pred\n",
    "y_c = (y_pred > 0.5).astype(\"int32\")\n",
    "y_test_np = y_test.to_numpy()\n",
    "y_test_np = y_test_np.astype('int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "e62c43df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 6ms/step - loss: 1.9019 - accuracy: 0.6441\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "cf_matrix = confusion_matrix(y_test_np.argmax(axis=1), y_c.argmax(axis=1)) \n",
    "#cf_matrix = confusion_matrix(y_test_np[:, 1], y_c[:, 1]) \n",
    "\n",
    "ax = sns.heatmap(cf_matrix, annot=True, cmap='Blues')\n",
    "\n",
    "ax.set_title('Seaborn Confusion Matrix with labels\\n\\n');\n",
    "ax.set_xlabel('\\nPredicted Values')\n",
    "ax.set_ylabel('Actual Values ');\n",
    "\n",
    "## Ticket labels - List must be in alphabetical order\n",
    "ax.xaxis.set_ticklabels(['Non-Progressor','Progressor'])\n",
    "ax.yaxis.set_ticklabels(['Non-Progressor','Progressor'])\n",
    "\n",
    "## Display the visualization of the Confusion Matrix.\n",
    "plt.savefig('M1_GRI_test.png')\n",
    "m2_eval_test = model_2_500.evaluate(X_test, y_test)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "419d7a62",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c41fe260",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd9e236",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "24145515",
   "metadata": {},
   "source": [
    "### Idea 3: Using 'RNFLT 1 to 768' as the predictors, 'Y_combined' as the dependent variable, no resampling, k-fold validation method "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "eebcc90d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "early_stopping_monitor = EarlyStopping(\n",
    "    monitor='val_accuracy',\n",
    "    min_delta=0,\n",
    "    patience=100,\n",
    "    verbose=0,\n",
    "    mode='auto',\n",
    "    baseline=None,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "opt1 = keras.optimizers.Adam(learning_rate = 1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "9e7a6e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define a function for getting new CNN models.\n",
    "def get_model():\n",
    "    #create model\n",
    "    model = Sequential()\n",
    "\n",
    "    #add layers\n",
    "    model.add(Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(768,1)))\n",
    "    model.add(MaxPooling1D(pool_size=3))\n",
    "    # model.add(Conv1D(filters=32, kernel_size=3, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    # model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dense(16, activation='relu'))\n",
    "    model.add(Dense(2, activation='softmax'))\n",
    "    model.compile(optimizer=opt1, \n",
    "                loss='categorical_crossentropy', \n",
    "                metrics=['accuracy'])\n",
    "    #Here we use cross-entropy as the criteria for loss.\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "2576dfbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#k-fold\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "kfold = StratifiedKFold(n_splits=5,shuffle=True,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "3ad5fb76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(516, 768, 1)"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Join the training set and validation set together.\n",
    "X_trainval = np.concatenate((X_train, X_val), axis=0)\n",
    "X_trainval.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "9b027c53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(516, 2)"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_trainval = np.concatenate((y_train, y_val), axis = 0)\n",
    "y_trainval.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "ea77c79b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0],\n",
       "       [0, 1],\n",
       "       [1, 0],\n",
       "       ...,\n",
       "       [1, 0],\n",
       "       [1, 0],\n",
       "       [0, 1]], dtype=uint8)"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_trainval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "70e9dfb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.449438202247191"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(y_trainval[:, 1])/sum(y_trainval[:, 0])\n",
    "#since this result is less than 1, we can know that the first col is non-progressor and the second col is progressor. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "f66aebf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_trainval = y_trainval[:, 1] #Only keep the progressor column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "01c3d1c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "13/13 [==============================] - 1s 38ms/step - loss: 0.6252 - accuracy: 0.6869 - val_loss: 0.6019 - val_accuracy: 0.6923\n",
      "Epoch 2/500\n",
      "13/13 [==============================] - 0s 26ms/step - loss: 0.6107 - accuracy: 0.6893 - val_loss: 0.6007 - val_accuracy: 0.6923\n",
      "Epoch 3/500\n",
      "13/13 [==============================] - 0s 24ms/step - loss: 0.6089 - accuracy: 0.6893 - val_loss: 0.6005 - val_accuracy: 0.6923\n",
      "Epoch 4/500\n",
      "13/13 [==============================] - 0s 26ms/step - loss: 0.6052 - accuracy: 0.6893 - val_loss: 0.6009 - val_accuracy: 0.6923\n",
      "Epoch 5/500\n",
      "13/13 [==============================] - 0s 29ms/step - loss: 0.6055 - accuracy: 0.6893 - val_loss: 0.6008 - val_accuracy: 0.6923\n",
      "Epoch 6/500\n",
      "13/13 [==============================] - 0s 29ms/step - loss: 0.6039 - accuracy: 0.6893 - val_loss: 0.6002 - val_accuracy: 0.6923\n",
      "Epoch 7/500\n",
      "13/13 [==============================] - 0s 31ms/step - loss: 0.6002 - accuracy: 0.6893 - val_loss: 0.6000 - val_accuracy: 0.6923\n",
      "Epoch 8/500\n",
      "13/13 [==============================] - 0s 31ms/step - loss: 0.6021 - accuracy: 0.6893 - val_loss: 0.6003 - val_accuracy: 0.6923\n",
      "Epoch 9/500\n",
      "13/13 [==============================] - 0s 30ms/step - loss: 0.6007 - accuracy: 0.6893 - val_loss: 0.6009 - val_accuracy: 0.6923\n",
      "Epoch 10/500\n",
      "13/13 [==============================] - 0s 25ms/step - loss: 0.5966 - accuracy: 0.6893 - val_loss: 0.6020 - val_accuracy: 0.6923\n",
      "Epoch 11/500\n",
      "13/13 [==============================] - 0s 27ms/step - loss: 0.5967 - accuracy: 0.6893 - val_loss: 0.6007 - val_accuracy: 0.6923\n",
      "Epoch 12/500\n",
      "13/13 [==============================] - 0s 30ms/step - loss: 0.5941 - accuracy: 0.6893 - val_loss: 0.6007 - val_accuracy: 0.6923\n",
      "Epoch 13/500\n",
      "13/13 [==============================] - 0s 30ms/step - loss: 0.5905 - accuracy: 0.6893 - val_loss: 0.6020 - val_accuracy: 0.6923\n",
      "Epoch 14/500\n",
      "13/13 [==============================] - 0s 24ms/step - loss: 0.5919 - accuracy: 0.6893 - val_loss: 0.6013 - val_accuracy: 0.6923\n",
      "Epoch 15/500\n",
      "13/13 [==============================] - 0s 22ms/step - loss: 0.5886 - accuracy: 0.6893 - val_loss: 0.6014 - val_accuracy: 0.6923\n",
      "Epoch 16/500\n",
      "13/13 [==============================] - 0s 30ms/step - loss: 0.5864 - accuracy: 0.6893 - val_loss: 0.6023 - val_accuracy: 0.6923\n",
      "Epoch 17/500\n",
      "13/13 [==============================] - 0s 26ms/step - loss: 0.5858 - accuracy: 0.6893 - val_loss: 0.6028 - val_accuracy: 0.6923\n",
      "Epoch 18/500\n",
      "13/13 [==============================] - 0s 26ms/step - loss: 0.5874 - accuracy: 0.6869 - val_loss: 0.6039 - val_accuracy: 0.6923\n",
      "Epoch 19/500\n",
      "13/13 [==============================] - 0s 28ms/step - loss: 0.5830 - accuracy: 0.6893 - val_loss: 0.6048 - val_accuracy: 0.6923\n",
      "Epoch 20/500\n",
      "13/13 [==============================] - 0s 28ms/step - loss: 0.5857 - accuracy: 0.6917 - val_loss: 0.6042 - val_accuracy: 0.6923\n",
      "Epoch 21/500\n",
      "13/13 [==============================] - 0s 29ms/step - loss: 0.5806 - accuracy: 0.6869 - val_loss: 0.6053 - val_accuracy: 0.6923\n",
      "Epoch 22/500\n",
      "13/13 [==============================] - 0s 31ms/step - loss: 0.5788 - accuracy: 0.6917 - val_loss: 0.6073 - val_accuracy: 0.6923\n",
      "Epoch 23/500\n",
      "13/13 [==============================] - 0s 27ms/step - loss: 0.5758 - accuracy: 0.6869 - val_loss: 0.6077 - val_accuracy: 0.6923\n",
      "Epoch 24/500\n",
      "13/13 [==============================] - 0s 24ms/step - loss: 0.5737 - accuracy: 0.6869 - val_loss: 0.6083 - val_accuracy: 0.6923\n",
      "Epoch 25/500\n",
      "13/13 [==============================] - 0s 22ms/step - loss: 0.5726 - accuracy: 0.6893 - val_loss: 0.6100 - val_accuracy: 0.6923\n",
      "Epoch 26/500\n",
      "13/13 [==============================] - 0s 25ms/step - loss: 0.5742 - accuracy: 0.6990 - val_loss: 0.6106 - val_accuracy: 0.6923\n",
      "Epoch 27/500\n",
      "13/13 [==============================] - 0s 29ms/step - loss: 0.5680 - accuracy: 0.6893 - val_loss: 0.6118 - val_accuracy: 0.6923\n",
      "Epoch 28/500\n",
      "13/13 [==============================] - 0s 24ms/step - loss: 0.5685 - accuracy: 0.6942 - val_loss: 0.6139 - val_accuracy: 0.6635\n",
      "Epoch 29/500\n",
      "13/13 [==============================] - 0s 32ms/step - loss: 0.5680 - accuracy: 0.7087 - val_loss: 0.6143 - val_accuracy: 0.6923\n",
      "Epoch 30/500\n",
      "13/13 [==============================] - 0s 24ms/step - loss: 0.5677 - accuracy: 0.7039 - val_loss: 0.6157 - val_accuracy: 0.6827\n",
      "Epoch 31/500\n",
      "13/13 [==============================] - 0s 28ms/step - loss: 0.5624 - accuracy: 0.7015 - val_loss: 0.6175 - val_accuracy: 0.6731\n",
      "Epoch 32/500\n",
      "13/13 [==============================] - 0s 26ms/step - loss: 0.5623 - accuracy: 0.7063 - val_loss: 0.6180 - val_accuracy: 0.6538\n",
      "Epoch 33/500\n",
      "13/13 [==============================] - 0s 25ms/step - loss: 0.5584 - accuracy: 0.7087 - val_loss: 0.6194 - val_accuracy: 0.6635\n",
      "Epoch 34/500\n",
      "13/13 [==============================] - 0s 25ms/step - loss: 0.5561 - accuracy: 0.7160 - val_loss: 0.6210 - val_accuracy: 0.6538\n",
      "Epoch 35/500\n",
      "13/13 [==============================] - 0s 29ms/step - loss: 0.5596 - accuracy: 0.6990 - val_loss: 0.6222 - val_accuracy: 0.6538\n",
      "Epoch 36/500\n",
      "13/13 [==============================] - 0s 25ms/step - loss: 0.5562 - accuracy: 0.7112 - val_loss: 0.6231 - val_accuracy: 0.6538\n",
      "Epoch 37/500\n",
      "13/13 [==============================] - 0s 27ms/step - loss: 0.5578 - accuracy: 0.7087 - val_loss: 0.6249 - val_accuracy: 0.6538\n",
      "Epoch 38/500\n",
      "13/13 [==============================] - 0s 33ms/step - loss: 0.5570 - accuracy: 0.7063 - val_loss: 0.6269 - val_accuracy: 0.6731\n",
      "Epoch 39/500\n",
      "13/13 [==============================] - 0s 31ms/step - loss: 0.5530 - accuracy: 0.7233 - val_loss: 0.6262 - val_accuracy: 0.6442\n",
      "Epoch 40/500\n",
      "13/13 [==============================] - 0s 28ms/step - loss: 0.5505 - accuracy: 0.7136 - val_loss: 0.6304 - val_accuracy: 0.6635\n",
      "Epoch 41/500\n",
      "13/13 [==============================] - 0s 25ms/step - loss: 0.5465 - accuracy: 0.7112 - val_loss: 0.6313 - val_accuracy: 0.6538\n",
      "Epoch 42/500\n",
      "13/13 [==============================] - 0s 24ms/step - loss: 0.5527 - accuracy: 0.7160 - val_loss: 0.6318 - val_accuracy: 0.6538\n",
      "Epoch 43/500\n",
      "13/13 [==============================] - 0s 26ms/step - loss: 0.5501 - accuracy: 0.7112 - val_loss: 0.6342 - val_accuracy: 0.6538\n",
      "Epoch 44/500\n",
      "13/13 [==============================] - 0s 28ms/step - loss: 0.5502 - accuracy: 0.7354 - val_loss: 0.6332 - val_accuracy: 0.6346\n",
      "Epoch 45/500\n",
      "13/13 [==============================] - 0s 31ms/step - loss: 0.5443 - accuracy: 0.7354 - val_loss: 0.6367 - val_accuracy: 0.6635\n",
      "Epoch 46/500\n",
      "13/13 [==============================] - 0s 31ms/step - loss: 0.5459 - accuracy: 0.7160 - val_loss: 0.6351 - val_accuracy: 0.6346\n",
      "Epoch 47/500\n",
      "13/13 [==============================] - 0s 28ms/step - loss: 0.5410 - accuracy: 0.7330 - val_loss: 0.6397 - val_accuracy: 0.6538\n",
      "Epoch 48/500\n",
      "13/13 [==============================] - 0s 30ms/step - loss: 0.5385 - accuracy: 0.7306 - val_loss: 0.6393 - val_accuracy: 0.6442\n",
      "Epoch 49/500\n",
      "13/13 [==============================] - 0s 24ms/step - loss: 0.5337 - accuracy: 0.7257 - val_loss: 0.6458 - val_accuracy: 0.6635\n",
      "Epoch 50/500\n",
      "13/13 [==============================] - 0s 21ms/step - loss: 0.5398 - accuracy: 0.7233 - val_loss: 0.6412 - val_accuracy: 0.6346\n",
      "Epoch 51/500\n",
      "13/13 [==============================] - 0s 27ms/step - loss: 0.5343 - accuracy: 0.7379 - val_loss: 0.6461 - val_accuracy: 0.6635\n",
      "Epoch 52/500\n",
      "13/13 [==============================] - 0s 24ms/step - loss: 0.5314 - accuracy: 0.7476 - val_loss: 0.6439 - val_accuracy: 0.6346\n",
      "Epoch 53/500\n",
      "13/13 [==============================] - 0s 24ms/step - loss: 0.5359 - accuracy: 0.7330 - val_loss: 0.6464 - val_accuracy: 0.6442\n",
      "Epoch 54/500\n",
      "13/13 [==============================] - 0s 24ms/step - loss: 0.5322 - accuracy: 0.7524 - val_loss: 0.6463 - val_accuracy: 0.6442\n",
      "Epoch 55/500\n",
      "13/13 [==============================] - 0s 23ms/step - loss: 0.5261 - accuracy: 0.7379 - val_loss: 0.6501 - val_accuracy: 0.6442\n",
      "Epoch 56/500\n",
      "13/13 [==============================] - 0s 26ms/step - loss: 0.5239 - accuracy: 0.7354 - val_loss: 0.6508 - val_accuracy: 0.6442\n",
      "Epoch 57/500\n",
      "13/13 [==============================] - 0s 25ms/step - loss: 0.5262 - accuracy: 0.7597 - val_loss: 0.6492 - val_accuracy: 0.6250\n",
      "Epoch 58/500\n",
      "13/13 [==============================] - 0s 24ms/step - loss: 0.5221 - accuracy: 0.7500 - val_loss: 0.6552 - val_accuracy: 0.6442\n",
      "Epoch 59/500\n",
      "13/13 [==============================] - 0s 27ms/step - loss: 0.5241 - accuracy: 0.7451 - val_loss: 0.6555 - val_accuracy: 0.6346\n",
      "Epoch 60/500\n",
      "13/13 [==============================] - 0s 27ms/step - loss: 0.5247 - accuracy: 0.7573 - val_loss: 0.6571 - val_accuracy: 0.6442\n",
      "Epoch 61/500\n",
      "13/13 [==============================] - 0s 25ms/step - loss: 0.5176 - accuracy: 0.7427 - val_loss: 0.6598 - val_accuracy: 0.6442\n",
      "Epoch 62/500\n",
      "13/13 [==============================] - 0s 24ms/step - loss: 0.5168 - accuracy: 0.7451 - val_loss: 0.6597 - val_accuracy: 0.6442\n",
      "Epoch 63/500\n",
      "13/13 [==============================] - 0s 24ms/step - loss: 0.5146 - accuracy: 0.7451 - val_loss: 0.6607 - val_accuracy: 0.6154\n",
      "Epoch 64/500\n",
      "13/13 [==============================] - 0s 23ms/step - loss: 0.5192 - accuracy: 0.7524 - val_loss: 0.6634 - val_accuracy: 0.6442\n",
      "Epoch 65/500\n",
      "13/13 [==============================] - 0s 24ms/step - loss: 0.5104 - accuracy: 0.7621 - val_loss: 0.6627 - val_accuracy: 0.6154\n",
      "Epoch 66/500\n",
      "13/13 [==============================] - 0s 24ms/step - loss: 0.5144 - accuracy: 0.7597 - val_loss: 0.6637 - val_accuracy: 0.6250\n",
      "Epoch 67/500\n",
      "13/13 [==============================] - 0s 24ms/step - loss: 0.5100 - accuracy: 0.7621 - val_loss: 0.6674 - val_accuracy: 0.6346\n",
      "Epoch 68/500\n",
      "13/13 [==============================] - 0s 26ms/step - loss: 0.5111 - accuracy: 0.7621 - val_loss: 0.6657 - val_accuracy: 0.6250\n",
      "Epoch 69/500\n",
      "13/13 [==============================] - 0s 24ms/step - loss: 0.5051 - accuracy: 0.7767 - val_loss: 0.6694 - val_accuracy: 0.6346\n",
      "Epoch 70/500\n",
      "13/13 [==============================] - 0s 26ms/step - loss: 0.5082 - accuracy: 0.7646 - val_loss: 0.6705 - val_accuracy: 0.6250\n",
      "Epoch 71/500\n",
      "13/13 [==============================] - 0s 26ms/step - loss: 0.5036 - accuracy: 0.7767 - val_loss: 0.6719 - val_accuracy: 0.6346\n",
      "Epoch 72/500\n",
      "13/13 [==============================] - 0s 24ms/step - loss: 0.4986 - accuracy: 0.7816 - val_loss: 0.6749 - val_accuracy: 0.6250\n",
      "Epoch 73/500\n",
      "13/13 [==============================] - 0s 30ms/step - loss: 0.4987 - accuracy: 0.7743 - val_loss: 0.6755 - val_accuracy: 0.6346\n",
      "Epoch 74/500\n",
      "13/13 [==============================] - 0s 32ms/step - loss: 0.4994 - accuracy: 0.7767 - val_loss: 0.6787 - val_accuracy: 0.6154\n",
      "Epoch 75/500\n",
      "13/13 [==============================] - 0s 28ms/step - loss: 0.4972 - accuracy: 0.7694 - val_loss: 0.6783 - val_accuracy: 0.6250\n",
      "Epoch 76/500\n",
      "13/13 [==============================] - 0s 26ms/step - loss: 0.4906 - accuracy: 0.7791 - val_loss: 0.6783 - val_accuracy: 0.6058\n",
      "Epoch 77/500\n",
      "13/13 [==============================] - 0s 25ms/step - loss: 0.4920 - accuracy: 0.7791 - val_loss: 0.6935 - val_accuracy: 0.6442\n",
      "Epoch 78/500\n",
      "13/13 [==============================] - 0s 25ms/step - loss: 0.4895 - accuracy: 0.7791 - val_loss: 0.6795 - val_accuracy: 0.6058\n",
      "Epoch 79/500\n",
      "13/13 [==============================] - 0s 23ms/step - loss: 0.4821 - accuracy: 0.7961 - val_loss: 0.6867 - val_accuracy: 0.5962\n",
      "Epoch 80/500\n",
      "13/13 [==============================] - 0s 23ms/step - loss: 0.4907 - accuracy: 0.7840 - val_loss: 0.6861 - val_accuracy: 0.6058\n",
      "Epoch 81/500\n",
      "13/13 [==============================] - 0s 23ms/step - loss: 0.4737 - accuracy: 0.8010 - val_loss: 0.6905 - val_accuracy: 0.5962\n",
      "Epoch 82/500\n",
      "13/13 [==============================] - 0s 22ms/step - loss: 0.4771 - accuracy: 0.7937 - val_loss: 0.6885 - val_accuracy: 0.6058\n",
      "Epoch 83/500\n",
      "13/13 [==============================] - 0s 24ms/step - loss: 0.4881 - accuracy: 0.7840 - val_loss: 0.6952 - val_accuracy: 0.5962\n",
      "Epoch 84/500\n",
      "13/13 [==============================] - 0s 25ms/step - loss: 0.4872 - accuracy: 0.7913 - val_loss: 0.6928 - val_accuracy: 0.6058\n",
      "Epoch 85/500\n",
      "13/13 [==============================] - 0s 25ms/step - loss: 0.4853 - accuracy: 0.7816 - val_loss: 0.6944 - val_accuracy: 0.5962\n",
      "Epoch 86/500\n",
      "13/13 [==============================] - 0s 28ms/step - loss: 0.4850 - accuracy: 0.7913 - val_loss: 0.6937 - val_accuracy: 0.6058\n",
      "Epoch 87/500\n",
      "13/13 [==============================] - 0s 27ms/step - loss: 0.4769 - accuracy: 0.7888 - val_loss: 0.6982 - val_accuracy: 0.5962\n",
      "Epoch 88/500\n",
      "13/13 [==============================] - 0s 25ms/step - loss: 0.4750 - accuracy: 0.8010 - val_loss: 0.6961 - val_accuracy: 0.6058\n",
      "Epoch 89/500\n",
      "13/13 [==============================] - 0s 24ms/step - loss: 0.4695 - accuracy: 0.8107 - val_loss: 0.7028 - val_accuracy: 0.5962\n",
      "Epoch 90/500\n",
      "13/13 [==============================] - 0s 26ms/step - loss: 0.4681 - accuracy: 0.7985 - val_loss: 0.7035 - val_accuracy: 0.6058\n",
      "Epoch 91/500\n",
      "13/13 [==============================] - 0s 28ms/step - loss: 0.4652 - accuracy: 0.7888 - val_loss: 0.7055 - val_accuracy: 0.6058\n",
      "Epoch 92/500\n",
      "13/13 [==============================] - 0s 24ms/step - loss: 0.4585 - accuracy: 0.7937 - val_loss: 0.7088 - val_accuracy: 0.5962\n",
      "Epoch 93/500\n",
      "13/13 [==============================] - 0s 25ms/step - loss: 0.4633 - accuracy: 0.8010 - val_loss: 0.7086 - val_accuracy: 0.6058\n",
      "Epoch 94/500\n",
      "13/13 [==============================] - 0s 25ms/step - loss: 0.4678 - accuracy: 0.7961 - val_loss: 0.7125 - val_accuracy: 0.6154\n",
      "Epoch 95/500\n",
      "13/13 [==============================] - 0s 25ms/step - loss: 0.4573 - accuracy: 0.7937 - val_loss: 0.7123 - val_accuracy: 0.5865\n",
      "Epoch 96/500\n",
      "13/13 [==============================] - 0s 26ms/step - loss: 0.4563 - accuracy: 0.8010 - val_loss: 0.7120 - val_accuracy: 0.6058\n",
      "Epoch 97/500\n",
      "13/13 [==============================] - 0s 25ms/step - loss: 0.4614 - accuracy: 0.7937 - val_loss: 0.7120 - val_accuracy: 0.5865\n",
      "Epoch 98/500\n",
      "13/13 [==============================] - 0s 27ms/step - loss: 0.4569 - accuracy: 0.7961 - val_loss: 0.7131 - val_accuracy: 0.5962\n",
      "Epoch 99/500\n",
      "13/13 [==============================] - 0s 28ms/step - loss: 0.4515 - accuracy: 0.8107 - val_loss: 0.7132 - val_accuracy: 0.5769\n",
      "Epoch 100/500\n",
      "13/13 [==============================] - 0s 26ms/step - loss: 0.4551 - accuracy: 0.7985 - val_loss: 0.7192 - val_accuracy: 0.6058\n",
      "Epoch 101/500\n",
      "13/13 [==============================] - 0s 26ms/step - loss: 0.4494 - accuracy: 0.8155 - val_loss: 0.7157 - val_accuracy: 0.5865\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1/500\n",
      "13/13 [==============================] - 1s 35ms/step - loss: 0.6263 - accuracy: 0.6731 - val_loss: 0.6277 - val_accuracy: 0.6893\n",
      "Epoch 2/500\n",
      "13/13 [==============================] - 0s 24ms/step - loss: 0.6142 - accuracy: 0.6901 - val_loss: 0.6385 - val_accuracy: 0.6893\n",
      "Epoch 3/500\n",
      "13/13 [==============================] - 0s 27ms/step - loss: 0.6027 - accuracy: 0.6901 - val_loss: 0.6217 - val_accuracy: 0.6893\n",
      "Epoch 4/500\n",
      "13/13 [==============================] - 0s 27ms/step - loss: 0.5966 - accuracy: 0.6901 - val_loss: 0.6236 - val_accuracy: 0.6893\n",
      "Epoch 5/500\n",
      "13/13 [==============================] - 0s 27ms/step - loss: 0.5959 - accuracy: 0.6901 - val_loss: 0.6233 - val_accuracy: 0.6893\n",
      "Epoch 6/500\n",
      "13/13 [==============================] - 0s 25ms/step - loss: 0.5880 - accuracy: 0.6901 - val_loss: 0.6247 - val_accuracy: 0.6893\n",
      "Epoch 7/500\n",
      "13/13 [==============================] - 0s 28ms/step - loss: 0.5834 - accuracy: 0.6901 - val_loss: 0.6258 - val_accuracy: 0.6893\n",
      "Epoch 8/500\n",
      "13/13 [==============================] - 0s 27ms/step - loss: 0.5826 - accuracy: 0.6901 - val_loss: 0.6249 - val_accuracy: 0.6796\n",
      "Epoch 9/500\n",
      "13/13 [==============================] - 0s 26ms/step - loss: 0.5816 - accuracy: 0.6949 - val_loss: 0.6291 - val_accuracy: 0.6796\n",
      "Epoch 10/500\n",
      "13/13 [==============================] - 0s 24ms/step - loss: 0.5803 - accuracy: 0.7022 - val_loss: 0.6280 - val_accuracy: 0.6408\n",
      "Epoch 11/500\n",
      "13/13 [==============================] - 0s 26ms/step - loss: 0.5729 - accuracy: 0.7046 - val_loss: 0.6341 - val_accuracy: 0.6796\n",
      "Epoch 12/500\n",
      "13/13 [==============================] - 0s 23ms/step - loss: 0.5735 - accuracy: 0.6973 - val_loss: 0.6317 - val_accuracy: 0.6311\n",
      "Epoch 13/500\n",
      "13/13 [==============================] - 0s 25ms/step - loss: 0.5740 - accuracy: 0.7022 - val_loss: 0.6332 - val_accuracy: 0.6408\n",
      "Epoch 14/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13/13 [==============================] - 0s 25ms/step - loss: 0.5741 - accuracy: 0.7070 - val_loss: 0.6339 - val_accuracy: 0.6214\n",
      "Epoch 15/500\n",
      "13/13 [==============================] - 0s 26ms/step - loss: 0.5701 - accuracy: 0.7119 - val_loss: 0.6348 - val_accuracy: 0.6311\n",
      "Epoch 16/500\n",
      "13/13 [==============================] - 0s 25ms/step - loss: 0.5642 - accuracy: 0.7215 - val_loss: 0.6384 - val_accuracy: 0.6311\n",
      "Epoch 17/500\n",
      "13/13 [==============================] - 0s 29ms/step - loss: 0.5670 - accuracy: 0.6925 - val_loss: 0.6366 - val_accuracy: 0.6311\n",
      "Epoch 18/500\n",
      "13/13 [==============================] - 0s 25ms/step - loss: 0.5619 - accuracy: 0.7143 - val_loss: 0.6400 - val_accuracy: 0.6311\n",
      "Epoch 19/500\n",
      "13/13 [==============================] - 0s 28ms/step - loss: 0.5608 - accuracy: 0.7264 - val_loss: 0.6382 - val_accuracy: 0.6214\n",
      "Epoch 20/500\n",
      "13/13 [==============================] - 0s 30ms/step - loss: 0.5666 - accuracy: 0.7191 - val_loss: 0.6393 - val_accuracy: 0.6311\n",
      "Epoch 21/500\n",
      "13/13 [==============================] - 0s 24ms/step - loss: 0.5545 - accuracy: 0.7167 - val_loss: 0.6390 - val_accuracy: 0.6214\n",
      "Epoch 22/500\n",
      "13/13 [==============================] - 0s 29ms/step - loss: 0.5527 - accuracy: 0.7215 - val_loss: 0.6404 - val_accuracy: 0.6214\n",
      "Epoch 23/500\n",
      "13/13 [==============================] - 0s 26ms/step - loss: 0.5561 - accuracy: 0.7312 - val_loss: 0.6409 - val_accuracy: 0.6214\n",
      "Epoch 24/500\n",
      "13/13 [==============================] - 0s 25ms/step - loss: 0.5500 - accuracy: 0.7385 - val_loss: 0.6434 - val_accuracy: 0.6117\n",
      "Epoch 25/500\n",
      "13/13 [==============================] - 0s 25ms/step - loss: 0.5537 - accuracy: 0.7337 - val_loss: 0.6451 - val_accuracy: 0.6311\n",
      "Epoch 26/500\n",
      "13/13 [==============================] - 0s 27ms/step - loss: 0.5501 - accuracy: 0.7409 - val_loss: 0.6443 - val_accuracy: 0.6214\n",
      "Epoch 27/500\n",
      "13/13 [==============================] - 0s 26ms/step - loss: 0.5457 - accuracy: 0.7337 - val_loss: 0.6484 - val_accuracy: 0.6214\n",
      "Epoch 28/500\n",
      "13/13 [==============================] - 0s 27ms/step - loss: 0.5448 - accuracy: 0.7312 - val_loss: 0.6495 - val_accuracy: 0.6214\n",
      "Epoch 29/500\n",
      "13/13 [==============================] - 0s 25ms/step - loss: 0.5473 - accuracy: 0.7337 - val_loss: 0.6548 - val_accuracy: 0.6311\n",
      "Epoch 30/500\n",
      "13/13 [==============================] - 0s 25ms/step - loss: 0.5389 - accuracy: 0.7361 - val_loss: 0.6593 - val_accuracy: 0.6117\n",
      "Epoch 31/500\n",
      "13/13 [==============================] - 0s 28ms/step - loss: 0.5377 - accuracy: 0.7385 - val_loss: 0.6558 - val_accuracy: 0.6117\n",
      "Epoch 32/500\n",
      "13/13 [==============================] - 0s 25ms/step - loss: 0.5412 - accuracy: 0.7288 - val_loss: 0.6620 - val_accuracy: 0.6117\n",
      "Epoch 33/500\n",
      "13/13 [==============================] - 0s 29ms/step - loss: 0.5430 - accuracy: 0.7506 - val_loss: 0.6572 - val_accuracy: 0.6117\n",
      "Epoch 34/500\n",
      "13/13 [==============================] - 0s 25ms/step - loss: 0.5358 - accuracy: 0.7482 - val_loss: 0.6607 - val_accuracy: 0.6214\n",
      "Epoch 35/500\n",
      "13/13 [==============================] - 0s 27ms/step - loss: 0.5348 - accuracy: 0.7337 - val_loss: 0.6675 - val_accuracy: 0.6117\n",
      "Epoch 36/500\n",
      "13/13 [==============================] - 0s 26ms/step - loss: 0.5299 - accuracy: 0.7554 - val_loss: 0.6615 - val_accuracy: 0.6117\n",
      "Epoch 37/500\n",
      "13/13 [==============================] - 0s 26ms/step - loss: 0.5361 - accuracy: 0.7385 - val_loss: 0.6641 - val_accuracy: 0.6214\n",
      "Epoch 38/500\n",
      "13/13 [==============================] - 0s 25ms/step - loss: 0.5280 - accuracy: 0.7385 - val_loss: 0.6696 - val_accuracy: 0.6214\n",
      "Epoch 39/500\n",
      "13/13 [==============================] - 0s 24ms/step - loss: 0.5215 - accuracy: 0.7506 - val_loss: 0.6652 - val_accuracy: 0.6311\n",
      "Epoch 40/500\n",
      "13/13 [==============================] - 0s 26ms/step - loss: 0.5279 - accuracy: 0.7579 - val_loss: 0.6704 - val_accuracy: 0.6311\n",
      "Epoch 41/500\n",
      "13/13 [==============================] - 0s 24ms/step - loss: 0.5233 - accuracy: 0.7627 - val_loss: 0.6730 - val_accuracy: 0.6311\n",
      "Epoch 42/500\n",
      "13/13 [==============================] - 0s 23ms/step - loss: 0.5201 - accuracy: 0.7700 - val_loss: 0.6679 - val_accuracy: 0.6311\n",
      "Epoch 43/500\n",
      "13/13 [==============================] - 0s 27ms/step - loss: 0.5192 - accuracy: 0.7579 - val_loss: 0.6706 - val_accuracy: 0.6505\n",
      "Epoch 44/500\n",
      "13/13 [==============================] - 0s 24ms/step - loss: 0.5195 - accuracy: 0.7627 - val_loss: 0.6737 - val_accuracy: 0.6408\n",
      "Epoch 45/500\n",
      "13/13 [==============================] - 0s 25ms/step - loss: 0.5170 - accuracy: 0.7651 - val_loss: 0.6735 - val_accuracy: 0.6408\n",
      "Epoch 46/500\n",
      "13/13 [==============================] - 0s 25ms/step - loss: 0.5140 - accuracy: 0.7506 - val_loss: 0.6816 - val_accuracy: 0.6311\n",
      "Epoch 47/500\n",
      "13/13 [==============================] - 0s 23ms/step - loss: 0.5158 - accuracy: 0.7772 - val_loss: 0.6807 - val_accuracy: 0.6019\n",
      "Epoch 48/500\n",
      "13/13 [==============================] - 0s 24ms/step - loss: 0.5172 - accuracy: 0.7627 - val_loss: 0.6774 - val_accuracy: 0.6505\n",
      "Epoch 49/500\n",
      "13/13 [==============================] - 0s 24ms/step - loss: 0.5095 - accuracy: 0.7579 - val_loss: 0.6881 - val_accuracy: 0.6019\n",
      "Epoch 50/500\n",
      "13/13 [==============================] - 0s 27ms/step - loss: 0.5097 - accuracy: 0.7579 - val_loss: 0.6760 - val_accuracy: 0.6602\n",
      "Epoch 51/500\n",
      "13/13 [==============================] - 0s 23ms/step - loss: 0.5082 - accuracy: 0.7651 - val_loss: 0.6901 - val_accuracy: 0.6214\n",
      "Epoch 52/500\n",
      "13/13 [==============================] - 0s 25ms/step - loss: 0.5039 - accuracy: 0.7676 - val_loss: 0.6809 - val_accuracy: 0.6602\n",
      "Epoch 53/500\n",
      "13/13 [==============================] - 0s 26ms/step - loss: 0.5019 - accuracy: 0.7869 - val_loss: 0.6856 - val_accuracy: 0.6214\n",
      "Epoch 54/500\n",
      "13/13 [==============================] - 0s 26ms/step - loss: 0.5082 - accuracy: 0.7579 - val_loss: 0.6850 - val_accuracy: 0.6311\n",
      "Epoch 55/500\n",
      "13/13 [==============================] - 0s 22ms/step - loss: 0.5038 - accuracy: 0.7627 - val_loss: 0.6817 - val_accuracy: 0.6311\n",
      "Epoch 56/500\n",
      "13/13 [==============================] - 0s 22ms/step - loss: 0.5041 - accuracy: 0.7797 - val_loss: 0.6909 - val_accuracy: 0.6505\n",
      "Epoch 57/500\n",
      "13/13 [==============================] - 0s 22ms/step - loss: 0.4936 - accuracy: 0.7845 - val_loss: 0.6897 - val_accuracy: 0.6311\n",
      "Epoch 58/500\n",
      "13/13 [==============================] - 0s 20ms/step - loss: 0.4934 - accuracy: 0.7845 - val_loss: 0.6937 - val_accuracy: 0.6214\n",
      "Epoch 59/500\n",
      "13/13 [==============================] - 0s 24ms/step - loss: 0.4922 - accuracy: 0.7966 - val_loss: 0.6927 - val_accuracy: 0.6408\n",
      "Epoch 60/500\n",
      "13/13 [==============================] - 0s 24ms/step - loss: 0.4942 - accuracy: 0.7821 - val_loss: 0.6996 - val_accuracy: 0.6214\n",
      "Epoch 61/500\n",
      "13/13 [==============================] - 0s 23ms/step - loss: 0.4897 - accuracy: 0.7845 - val_loss: 0.7003 - val_accuracy: 0.6214\n",
      "Epoch 62/500\n",
      "13/13 [==============================] - 0s 24ms/step - loss: 0.4865 - accuracy: 0.8063 - val_loss: 0.7008 - val_accuracy: 0.6408\n",
      "Epoch 63/500\n",
      "13/13 [==============================] - 0s 24ms/step - loss: 0.4896 - accuracy: 0.7797 - val_loss: 0.7040 - val_accuracy: 0.6214\n",
      "Epoch 64/500\n",
      "13/13 [==============================] - 0s 24ms/step - loss: 0.4870 - accuracy: 0.7869 - val_loss: 0.7026 - val_accuracy: 0.6214\n",
      "Epoch 65/500\n",
      "13/13 [==============================] - 0s 24ms/step - loss: 0.4872 - accuracy: 0.7966 - val_loss: 0.7040 - val_accuracy: 0.6311\n",
      "Epoch 66/500\n",
      "13/13 [==============================] - 0s 23ms/step - loss: 0.4805 - accuracy: 0.7869 - val_loss: 0.7141 - val_accuracy: 0.6117\n",
      "Epoch 67/500\n",
      "13/13 [==============================] - 0s 26ms/step - loss: 0.4811 - accuracy: 0.7990 - val_loss: 0.7138 - val_accuracy: 0.6117\n",
      "Epoch 68/500\n",
      "13/13 [==============================] - 0s 25ms/step - loss: 0.4741 - accuracy: 0.8039 - val_loss: 0.7104 - val_accuracy: 0.6311\n",
      "Epoch 69/500\n",
      "13/13 [==============================] - 0s 23ms/step - loss: 0.4744 - accuracy: 0.7918 - val_loss: 0.7141 - val_accuracy: 0.6117\n",
      "Epoch 70/500\n",
      "13/13 [==============================] - 0s 22ms/step - loss: 0.4745 - accuracy: 0.8063 - val_loss: 0.7151 - val_accuracy: 0.6214\n",
      "Epoch 71/500\n",
      "13/13 [==============================] - 0s 22ms/step - loss: 0.4754 - accuracy: 0.8136 - val_loss: 0.7169 - val_accuracy: 0.6214\n",
      "Epoch 72/500\n",
      "13/13 [==============================] - 0s 23ms/step - loss: 0.4711 - accuracy: 0.7966 - val_loss: 0.7253 - val_accuracy: 0.6019\n",
      "Epoch 73/500\n",
      "13/13 [==============================] - 0s 27ms/step - loss: 0.4657 - accuracy: 0.8039 - val_loss: 0.7303 - val_accuracy: 0.5922\n",
      "Epoch 74/500\n",
      "13/13 [==============================] - 0s 24ms/step - loss: 0.4696 - accuracy: 0.8039 - val_loss: 0.7233 - val_accuracy: 0.6214\n",
      "Epoch 75/500\n",
      "13/13 [==============================] - 0s 25ms/step - loss: 0.4608 - accuracy: 0.8063 - val_loss: 0.7346 - val_accuracy: 0.5728\n",
      "Epoch 76/500\n",
      "13/13 [==============================] - 0s 24ms/step - loss: 0.4691 - accuracy: 0.8015 - val_loss: 0.7254 - val_accuracy: 0.6214\n",
      "Epoch 77/500\n",
      "13/13 [==============================] - 0s 21ms/step - loss: 0.4547 - accuracy: 0.8136 - val_loss: 0.7370 - val_accuracy: 0.6214\n",
      "Epoch 78/500\n",
      "13/13 [==============================] - 0s 25ms/step - loss: 0.4626 - accuracy: 0.8136 - val_loss: 0.7325 - val_accuracy: 0.6311\n",
      "Epoch 79/500\n",
      "13/13 [==============================] - 0s 26ms/step - loss: 0.4634 - accuracy: 0.8015 - val_loss: 0.7434 - val_accuracy: 0.5825\n",
      "Epoch 80/500\n",
      "13/13 [==============================] - 0s 28ms/step - loss: 0.4621 - accuracy: 0.8087 - val_loss: 0.7353 - val_accuracy: 0.6117\n",
      "Epoch 81/500\n",
      "13/13 [==============================] - 0s 25ms/step - loss: 0.4544 - accuracy: 0.8111 - val_loss: 0.7438 - val_accuracy: 0.6019\n",
      "Epoch 82/500\n",
      "13/13 [==============================] - 0s 24ms/step - loss: 0.4639 - accuracy: 0.8015 - val_loss: 0.7428 - val_accuracy: 0.6214\n",
      "Epoch 83/500\n",
      "13/13 [==============================] - 0s 22ms/step - loss: 0.4491 - accuracy: 0.8208 - val_loss: 0.7449 - val_accuracy: 0.5922\n",
      "Epoch 84/500\n",
      "13/13 [==============================] - 0s 21ms/step - loss: 0.4566 - accuracy: 0.8087 - val_loss: 0.7485 - val_accuracy: 0.6117\n",
      "Epoch 85/500\n",
      "13/13 [==============================] - 0s 21ms/step - loss: 0.4548 - accuracy: 0.8087 - val_loss: 0.7543 - val_accuracy: 0.6214\n",
      "Epoch 86/500\n",
      "13/13 [==============================] - 0s 23ms/step - loss: 0.4556 - accuracy: 0.8063 - val_loss: 0.7487 - val_accuracy: 0.5825\n",
      "Epoch 87/500\n",
      "13/13 [==============================] - 0s 25ms/step - loss: 0.4494 - accuracy: 0.8136 - val_loss: 0.7516 - val_accuracy: 0.5728\n",
      "Epoch 88/500\n",
      "13/13 [==============================] - 0s 26ms/step - loss: 0.4479 - accuracy: 0.8087 - val_loss: 0.7631 - val_accuracy: 0.5825\n",
      "Epoch 89/500\n",
      "13/13 [==============================] - 0s 24ms/step - loss: 0.4401 - accuracy: 0.8281 - val_loss: 0.7540 - val_accuracy: 0.5728\n",
      "Epoch 90/500\n",
      "13/13 [==============================] - 0s 24ms/step - loss: 0.4427 - accuracy: 0.8184 - val_loss: 0.7657 - val_accuracy: 0.5534\n",
      "Epoch 91/500\n",
      "13/13 [==============================] - 0s 26ms/step - loss: 0.4466 - accuracy: 0.8208 - val_loss: 0.7596 - val_accuracy: 0.5922\n",
      "Epoch 92/500\n",
      "13/13 [==============================] - 0s 27ms/step - loss: 0.4352 - accuracy: 0.8281 - val_loss: 0.7735 - val_accuracy: 0.5534\n",
      "Epoch 93/500\n",
      "13/13 [==============================] - 0s 24ms/step - loss: 0.4383 - accuracy: 0.8208 - val_loss: 0.7607 - val_accuracy: 0.6019\n",
      "Epoch 94/500\n",
      "13/13 [==============================] - 0s 24ms/step - loss: 0.4394 - accuracy: 0.8281 - val_loss: 0.7699 - val_accuracy: 0.5631\n",
      "Epoch 95/500\n",
      "13/13 [==============================] - 0s 27ms/step - loss: 0.4290 - accuracy: 0.8232 - val_loss: 0.7656 - val_accuracy: 0.6214\n",
      "Epoch 96/500\n",
      "13/13 [==============================] - 0s 25ms/step - loss: 0.4311 - accuracy: 0.8208 - val_loss: 0.7831 - val_accuracy: 0.5922\n",
      "Epoch 97/500\n",
      "13/13 [==============================] - 0s 25ms/step - loss: 0.4315 - accuracy: 0.8111 - val_loss: 0.7712 - val_accuracy: 0.5437\n",
      "Epoch 98/500\n",
      "13/13 [==============================] - 0s 24ms/step - loss: 0.4201 - accuracy: 0.8281 - val_loss: 0.7831 - val_accuracy: 0.5922\n",
      "Epoch 99/500\n",
      "13/13 [==============================] - 0s 25ms/step - loss: 0.4181 - accuracy: 0.8281 - val_loss: 0.7895 - val_accuracy: 0.5534\n",
      "Epoch 100/500\n",
      "13/13 [==============================] - 0s 26ms/step - loss: 0.4284 - accuracy: 0.8184 - val_loss: 0.7840 - val_accuracy: 0.5728\n",
      "Epoch 101/500\n",
      "13/13 [==============================] - 0s 24ms/step - loss: 0.4223 - accuracy: 0.8160 - val_loss: 0.7811 - val_accuracy: 0.5437\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "Epoch 1/500\n",
      "13/13 [==============================] - 1s 33ms/step - loss: 0.6311 - accuracy: 0.6901 - val_loss: 0.6167 - val_accuracy: 0.6893\n",
      "Epoch 2/500\n",
      "13/13 [==============================] - 0s 24ms/step - loss: 0.6175 - accuracy: 0.6901 - val_loss: 0.6113 - val_accuracy: 0.6893\n",
      "Epoch 3/500\n",
      "13/13 [==============================] - 0s 23ms/step - loss: 0.6028 - accuracy: 0.6901 - val_loss: 0.6060 - val_accuracy: 0.6893\n",
      "Epoch 4/500\n",
      "13/13 [==============================] - 0s 25ms/step - loss: 0.6016 - accuracy: 0.6901 - val_loss: 0.6052 - val_accuracy: 0.6893\n",
      "Epoch 5/500\n",
      "13/13 [==============================] - 0s 26ms/step - loss: 0.5989 - accuracy: 0.6901 - val_loss: 0.6012 - val_accuracy: 0.6893\n",
      "Epoch 6/500\n",
      "13/13 [==============================] - 0s 23ms/step - loss: 0.5977 - accuracy: 0.6901 - val_loss: 0.6002 - val_accuracy: 0.6893\n",
      "Epoch 7/500\n",
      "13/13 [==============================] - 0s 26ms/step - loss: 0.5909 - accuracy: 0.6901 - val_loss: 0.5984 - val_accuracy: 0.6893\n",
      "Epoch 8/500\n",
      "13/13 [==============================] - 0s 23ms/step - loss: 0.5880 - accuracy: 0.6901 - val_loss: 0.5972 - val_accuracy: 0.6893\n",
      "Epoch 9/500\n",
      "13/13 [==============================] - 0s 24ms/step - loss: 0.5891 - accuracy: 0.6901 - val_loss: 0.5962 - val_accuracy: 0.6893\n",
      "Epoch 10/500\n",
      "13/13 [==============================] - 0s 24ms/step - loss: 0.5857 - accuracy: 0.6901 - val_loss: 0.5935 - val_accuracy: 0.6893\n",
      "Epoch 11/500\n",
      "13/13 [==============================] - 0s 23ms/step - loss: 0.5831 - accuracy: 0.6901 - val_loss: 0.5919 - val_accuracy: 0.6893\n",
      "Epoch 12/500\n",
      "13/13 [==============================] - 0s 23ms/step - loss: 0.5894 - accuracy: 0.6901 - val_loss: 0.5906 - val_accuracy: 0.6893\n",
      "Epoch 13/500\n",
      "13/13 [==============================] - 0s 22ms/step - loss: 0.5798 - accuracy: 0.6901 - val_loss: 0.5889 - val_accuracy: 0.6893\n",
      "Epoch 14/500\n",
      "13/13 [==============================] - 0s 23ms/step - loss: 0.5823 - accuracy: 0.6901 - val_loss: 0.5843 - val_accuracy: 0.6893\n",
      "Epoch 15/500\n",
      "13/13 [==============================] - 0s 25ms/step - loss: 0.5720 - accuracy: 0.6901 - val_loss: 0.5831 - val_accuracy: 0.6893\n",
      "Epoch 16/500\n",
      "13/13 [==============================] - 0s 25ms/step - loss: 0.5693 - accuracy: 0.6901 - val_loss: 0.5835 - val_accuracy: 0.6893\n",
      "Epoch 17/500\n",
      "13/13 [==============================] - 0s 27ms/step - loss: 0.5722 - accuracy: 0.6949 - val_loss: 0.5829 - val_accuracy: 0.6893\n",
      "Epoch 18/500\n",
      "13/13 [==============================] - 0s 23ms/step - loss: 0.5724 - accuracy: 0.6925 - val_loss: 0.5824 - val_accuracy: 0.6893\n",
      "Epoch 19/500\n",
      "13/13 [==============================] - 0s 22ms/step - loss: 0.5722 - accuracy: 0.6925 - val_loss: 0.5808 - val_accuracy: 0.6893\n",
      "Epoch 20/500\n",
      "13/13 [==============================] - 0s 25ms/step - loss: 0.5709 - accuracy: 0.6925 - val_loss: 0.5804 - val_accuracy: 0.6893\n",
      "Epoch 21/500\n",
      "13/13 [==============================] - 0s 24ms/step - loss: 0.5660 - accuracy: 0.6949 - val_loss: 0.5791 - val_accuracy: 0.6893\n",
      "Epoch 22/500\n",
      "13/13 [==============================] - 0s 25ms/step - loss: 0.5671 - accuracy: 0.6998 - val_loss: 0.5810 - val_accuracy: 0.6699\n",
      "Epoch 23/500\n",
      "13/13 [==============================] - 0s 25ms/step - loss: 0.5632 - accuracy: 0.6973 - val_loss: 0.5800 - val_accuracy: 0.6796\n",
      "Epoch 24/500\n",
      "13/13 [==============================] - 0s 26ms/step - loss: 0.5646 - accuracy: 0.7022 - val_loss: 0.5784 - val_accuracy: 0.6602\n",
      "Epoch 25/500\n",
      "13/13 [==============================] - 0s 26ms/step - loss: 0.5618 - accuracy: 0.7167 - val_loss: 0.5787 - val_accuracy: 0.6699\n",
      "Epoch 26/500\n",
      "13/13 [==============================] - 0s 25ms/step - loss: 0.5603 - accuracy: 0.7070 - val_loss: 0.5775 - val_accuracy: 0.6699\n",
      "Epoch 27/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13/13 [==============================] - 0s 25ms/step - loss: 0.5641 - accuracy: 0.7240 - val_loss: 0.5775 - val_accuracy: 0.6796\n",
      "Epoch 28/500\n",
      "13/13 [==============================] - 0s 25ms/step - loss: 0.5621 - accuracy: 0.7046 - val_loss: 0.5779 - val_accuracy: 0.6602\n",
      "Epoch 29/500\n",
      "13/13 [==============================] - 0s 24ms/step - loss: 0.5589 - accuracy: 0.7167 - val_loss: 0.5764 - val_accuracy: 0.6699\n",
      "Epoch 30/500\n",
      "13/13 [==============================] - 0s 25ms/step - loss: 0.5572 - accuracy: 0.7215 - val_loss: 0.5786 - val_accuracy: 0.6893\n",
      "Epoch 31/500\n",
      "13/13 [==============================] - 0s 27ms/step - loss: 0.5575 - accuracy: 0.7046 - val_loss: 0.5772 - val_accuracy: 0.6699\n",
      "Epoch 32/500\n",
      "13/13 [==============================] - 0s 24ms/step - loss: 0.5513 - accuracy: 0.7143 - val_loss: 0.5774 - val_accuracy: 0.6699\n",
      "Epoch 33/500\n",
      "13/13 [==============================] - 0s 25ms/step - loss: 0.5504 - accuracy: 0.7143 - val_loss: 0.5759 - val_accuracy: 0.6893\n",
      "Epoch 34/500\n",
      "13/13 [==============================] - 0s 26ms/step - loss: 0.5466 - accuracy: 0.7070 - val_loss: 0.5766 - val_accuracy: 0.6505\n",
      "Epoch 35/500\n",
      "13/13 [==============================] - 0s 27ms/step - loss: 0.5547 - accuracy: 0.7409 - val_loss: 0.5767 - val_accuracy: 0.6796\n",
      "Epoch 36/500\n",
      "13/13 [==============================] - 0s 28ms/step - loss: 0.5530 - accuracy: 0.7240 - val_loss: 0.5759 - val_accuracy: 0.6796\n",
      "Epoch 37/500\n",
      "13/13 [==============================] - 0s 27ms/step - loss: 0.5421 - accuracy: 0.7337 - val_loss: 0.5762 - val_accuracy: 0.6699\n",
      "Epoch 38/500\n",
      "13/13 [==============================] - 0s 28ms/step - loss: 0.5434 - accuracy: 0.7264 - val_loss: 0.5748 - val_accuracy: 0.6893\n",
      "Epoch 39/500\n",
      "13/13 [==============================] - 0s 25ms/step - loss: 0.5449 - accuracy: 0.7337 - val_loss: 0.5766 - val_accuracy: 0.6699\n",
      "Epoch 40/500\n",
      "13/13 [==============================] - 0s 24ms/step - loss: 0.5426 - accuracy: 0.7337 - val_loss: 0.5755 - val_accuracy: 0.6796\n",
      "Epoch 41/500\n",
      "13/13 [==============================] - 0s 23ms/step - loss: 0.5395 - accuracy: 0.7361 - val_loss: 0.5749 - val_accuracy: 0.6796\n",
      "Epoch 42/500\n",
      "13/13 [==============================] - 0s 25ms/step - loss: 0.5364 - accuracy: 0.7337 - val_loss: 0.5770 - val_accuracy: 0.6505\n",
      "Epoch 43/500\n",
      "13/13 [==============================] - 0s 25ms/step - loss: 0.5345 - accuracy: 0.7312 - val_loss: 0.5788 - val_accuracy: 0.6699\n",
      "Epoch 44/500\n",
      "13/13 [==============================] - 0s 24ms/step - loss: 0.5332 - accuracy: 0.7070 - val_loss: 0.5765 - val_accuracy: 0.6602\n",
      "Epoch 45/500\n",
      "13/13 [==============================] - 0s 24ms/step - loss: 0.5345 - accuracy: 0.7530 - val_loss: 0.5770 - val_accuracy: 0.6699\n",
      "Epoch 46/500\n",
      "13/13 [==============================] - 0s 24ms/step - loss: 0.5392 - accuracy: 0.7482 - val_loss: 0.5788 - val_accuracy: 0.6505\n",
      "Epoch 47/500\n",
      "13/13 [==============================] - 0s 22ms/step - loss: 0.5300 - accuracy: 0.7312 - val_loss: 0.5806 - val_accuracy: 0.6602\n",
      "Epoch 48/500\n",
      "13/13 [==============================] - 0s 24ms/step - loss: 0.5270 - accuracy: 0.7506 - val_loss: 0.5770 - val_accuracy: 0.6602\n",
      "Epoch 49/500\n",
      "13/13 [==============================] - 0s 24ms/step - loss: 0.5255 - accuracy: 0.7554 - val_loss: 0.5792 - val_accuracy: 0.6699\n",
      "Epoch 50/500\n",
      "13/13 [==============================] - 0s 26ms/step - loss: 0.5284 - accuracy: 0.7409 - val_loss: 0.5818 - val_accuracy: 0.6602\n",
      "Epoch 51/500\n",
      "13/13 [==============================] - 0s 24ms/step - loss: 0.5281 - accuracy: 0.7506 - val_loss: 0.5814 - val_accuracy: 0.6602\n",
      "Epoch 52/500\n",
      "13/13 [==============================] - 0s 25ms/step - loss: 0.5247 - accuracy: 0.7482 - val_loss: 0.5804 - val_accuracy: 0.6699\n",
      "Epoch 53/500\n",
      "13/13 [==============================] - 0s 26ms/step - loss: 0.5214 - accuracy: 0.7554 - val_loss: 0.5795 - val_accuracy: 0.6602\n",
      "Epoch 54/500\n",
      "13/13 [==============================] - 0s 26ms/step - loss: 0.5240 - accuracy: 0.7651 - val_loss: 0.5822 - val_accuracy: 0.6796\n",
      "Epoch 55/500\n",
      "13/13 [==============================] - 0s 24ms/step - loss: 0.5165 - accuracy: 0.7554 - val_loss: 0.5814 - val_accuracy: 0.6796\n",
      "Epoch 56/500\n",
      "13/13 [==============================] - 0s 26ms/step - loss: 0.5177 - accuracy: 0.7554 - val_loss: 0.5829 - val_accuracy: 0.6699\n",
      "Epoch 57/500\n",
      "13/13 [==============================] - 0s 24ms/step - loss: 0.5193 - accuracy: 0.7579 - val_loss: 0.5853 - val_accuracy: 0.6505\n",
      "Epoch 58/500\n",
      "13/13 [==============================] - 0s 24ms/step - loss: 0.5226 - accuracy: 0.7724 - val_loss: 0.5835 - val_accuracy: 0.6699\n",
      "Epoch 59/500\n",
      "13/13 [==============================] - 0s 25ms/step - loss: 0.5185 - accuracy: 0.7530 - val_loss: 0.5842 - val_accuracy: 0.6408\n",
      "Epoch 60/500\n",
      "13/13 [==============================] - 0s 23ms/step - loss: 0.5124 - accuracy: 0.7627 - val_loss: 0.5866 - val_accuracy: 0.6311\n",
      "Epoch 61/500\n",
      "13/13 [==============================] - 0s 24ms/step - loss: 0.5152 - accuracy: 0.7700 - val_loss: 0.5834 - val_accuracy: 0.6505\n",
      "Epoch 62/500\n",
      "13/13 [==============================] - 0s 24ms/step - loss: 0.5121 - accuracy: 0.7579 - val_loss: 0.5852 - val_accuracy: 0.6408\n",
      "Epoch 63/500\n",
      "13/13 [==============================] - 0s 23ms/step - loss: 0.5160 - accuracy: 0.7676 - val_loss: 0.5881 - val_accuracy: 0.6602\n",
      "Epoch 64/500\n",
      "13/13 [==============================] - 0s 23ms/step - loss: 0.5111 - accuracy: 0.7530 - val_loss: 0.5890 - val_accuracy: 0.6408\n",
      "Epoch 65/500\n",
      "13/13 [==============================] - 0s 24ms/step - loss: 0.5051 - accuracy: 0.7893 - val_loss: 0.5873 - val_accuracy: 0.6602\n",
      "Epoch 66/500\n",
      "13/13 [==============================] - 0s 23ms/step - loss: 0.5130 - accuracy: 0.7700 - val_loss: 0.5890 - val_accuracy: 0.6505\n",
      "Epoch 67/500\n",
      "13/13 [==============================] - 0s 25ms/step - loss: 0.5011 - accuracy: 0.7676 - val_loss: 0.5911 - val_accuracy: 0.6408\n",
      "Epoch 68/500\n",
      "13/13 [==============================] - 0s 25ms/step - loss: 0.5041 - accuracy: 0.7772 - val_loss: 0.5895 - val_accuracy: 0.6408\n",
      "Epoch 69/500\n",
      "13/13 [==============================] - 0s 25ms/step - loss: 0.5043 - accuracy: 0.7748 - val_loss: 0.5905 - val_accuracy: 0.6602\n",
      "Epoch 70/500\n",
      "13/13 [==============================] - 0s 25ms/step - loss: 0.5028 - accuracy: 0.7651 - val_loss: 0.5898 - val_accuracy: 0.6602\n",
      "Epoch 71/500\n",
      "13/13 [==============================] - 0s 22ms/step - loss: 0.4922 - accuracy: 0.7869 - val_loss: 0.5927 - val_accuracy: 0.6408\n",
      "Epoch 72/500\n",
      "13/13 [==============================] - 0s 24ms/step - loss: 0.5032 - accuracy: 0.7482 - val_loss: 0.5931 - val_accuracy: 0.6505\n",
      "Epoch 73/500\n",
      "13/13 [==============================] - 0s 24ms/step - loss: 0.4958 - accuracy: 0.7821 - val_loss: 0.5928 - val_accuracy: 0.6602\n",
      "Epoch 74/500\n",
      "13/13 [==============================] - 0s 25ms/step - loss: 0.5009 - accuracy: 0.7676 - val_loss: 0.5943 - val_accuracy: 0.6602\n",
      "Epoch 75/500\n",
      "13/13 [==============================] - 0s 25ms/step - loss: 0.4962 - accuracy: 0.7772 - val_loss: 0.5942 - val_accuracy: 0.6602\n",
      "Epoch 76/500\n",
      "13/13 [==============================] - 0s 24ms/step - loss: 0.4941 - accuracy: 0.7772 - val_loss: 0.5958 - val_accuracy: 0.6602\n",
      "Epoch 77/500\n",
      "13/13 [==============================] - 0s 24ms/step - loss: 0.4871 - accuracy: 0.7748 - val_loss: 0.5962 - val_accuracy: 0.6602\n",
      "Epoch 78/500\n",
      "13/13 [==============================] - 0s 24ms/step - loss: 0.4960 - accuracy: 0.7603 - val_loss: 0.5982 - val_accuracy: 0.6602\n",
      "Epoch 79/500\n",
      "13/13 [==============================] - 0s 26ms/step - loss: 0.4875 - accuracy: 0.7700 - val_loss: 0.6008 - val_accuracy: 0.6602\n",
      "Epoch 80/500\n",
      "13/13 [==============================] - 0s 24ms/step - loss: 0.4873 - accuracy: 0.7845 - val_loss: 0.6016 - val_accuracy: 0.6602\n",
      "Epoch 81/500\n",
      "13/13 [==============================] - 0s 21ms/step - loss: 0.4774 - accuracy: 0.7748 - val_loss: 0.6016 - val_accuracy: 0.6602\n",
      "Epoch 82/500\n",
      "13/13 [==============================] - 0s 25ms/step - loss: 0.4811 - accuracy: 0.7821 - val_loss: 0.5994 - val_accuracy: 0.6505\n",
      "Epoch 83/500\n",
      "13/13 [==============================] - 0s 25ms/step - loss: 0.4831 - accuracy: 0.7918 - val_loss: 0.6032 - val_accuracy: 0.6505\n",
      "Epoch 84/500\n",
      "13/13 [==============================] - 0s 24ms/step - loss: 0.4776 - accuracy: 0.7651 - val_loss: 0.6043 - val_accuracy: 0.6505\n",
      "Epoch 85/500\n",
      "13/13 [==============================] - 0s 24ms/step - loss: 0.4867 - accuracy: 0.7918 - val_loss: 0.6043 - val_accuracy: 0.6505\n",
      "Epoch 86/500\n",
      "13/13 [==============================] - 0s 27ms/step - loss: 0.4774 - accuracy: 0.7869 - val_loss: 0.6086 - val_accuracy: 0.6505\n",
      "Epoch 87/500\n",
      "13/13 [==============================] - 0s 28ms/step - loss: 0.4712 - accuracy: 0.7700 - val_loss: 0.6064 - val_accuracy: 0.6408\n",
      "Epoch 88/500\n",
      "13/13 [==============================] - 0s 27ms/step - loss: 0.4806 - accuracy: 0.7990 - val_loss: 0.6105 - val_accuracy: 0.6408\n",
      "Epoch 89/500\n",
      "13/13 [==============================] - 0s 26ms/step - loss: 0.4792 - accuracy: 0.7748 - val_loss: 0.6078 - val_accuracy: 0.6602\n",
      "Epoch 90/500\n",
      "13/13 [==============================] - 0s 24ms/step - loss: 0.4719 - accuracy: 0.7966 - val_loss: 0.6107 - val_accuracy: 0.6408\n",
      "Epoch 91/500\n",
      "13/13 [==============================] - 0s 25ms/step - loss: 0.4688 - accuracy: 0.7845 - val_loss: 0.6129 - val_accuracy: 0.6602\n",
      "Epoch 92/500\n",
      "13/13 [==============================] - 0s 28ms/step - loss: 0.4713 - accuracy: 0.7918 - val_loss: 0.6117 - val_accuracy: 0.6505\n",
      "Epoch 93/500\n",
      "13/13 [==============================] - 0s 24ms/step - loss: 0.4635 - accuracy: 0.7966 - val_loss: 0.6135 - val_accuracy: 0.6505\n",
      "Epoch 94/500\n",
      "13/13 [==============================] - 0s 24ms/step - loss: 0.4595 - accuracy: 0.8015 - val_loss: 0.6154 - val_accuracy: 0.6602\n",
      "Epoch 95/500\n",
      "13/13 [==============================] - 0s 22ms/step - loss: 0.4708 - accuracy: 0.7942 - val_loss: 0.6254 - val_accuracy: 0.6408\n",
      "Epoch 96/500\n",
      "13/13 [==============================] - 0s 25ms/step - loss: 0.4656 - accuracy: 0.8087 - val_loss: 0.6146 - val_accuracy: 0.6699\n",
      "Epoch 97/500\n",
      "13/13 [==============================] - 0s 23ms/step - loss: 0.4667 - accuracy: 0.8015 - val_loss: 0.6214 - val_accuracy: 0.6505\n",
      "Epoch 98/500\n",
      "13/13 [==============================] - 0s 24ms/step - loss: 0.4613 - accuracy: 0.7918 - val_loss: 0.6188 - val_accuracy: 0.6602\n",
      "Epoch 99/500\n",
      "13/13 [==============================] - 0s 26ms/step - loss: 0.4600 - accuracy: 0.8039 - val_loss: 0.6258 - val_accuracy: 0.6505\n",
      "Epoch 100/500\n",
      "13/13 [==============================] - 0s 23ms/step - loss: 0.4607 - accuracy: 0.7966 - val_loss: 0.6245 - val_accuracy: 0.6408\n",
      "Epoch 101/500\n",
      "13/13 [==============================] - 0s 24ms/step - loss: 0.4580 - accuracy: 0.8087 - val_loss: 0.6284 - val_accuracy: 0.6505\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "Epoch 1/500\n",
      "13/13 [==============================] - 1s 35ms/step - loss: 0.6083 - accuracy: 0.6828 - val_loss: 0.6566 - val_accuracy: 0.6893\n",
      "Epoch 2/500\n",
      "13/13 [==============================] - 0s 23ms/step - loss: 0.6209 - accuracy: 0.6901 - val_loss: 0.6105 - val_accuracy: 0.6893\n",
      "Epoch 3/500\n",
      "13/13 [==============================] - 0s 26ms/step - loss: 0.5999 - accuracy: 0.6901 - val_loss: 0.6084 - val_accuracy: 0.6893\n",
      "Epoch 4/500\n",
      "13/13 [==============================] - 0s 24ms/step - loss: 0.5947 - accuracy: 0.6901 - val_loss: 0.6074 - val_accuracy: 0.6893\n",
      "Epoch 5/500\n",
      "13/13 [==============================] - 0s 27ms/step - loss: 0.5935 - accuracy: 0.6901 - val_loss: 0.6063 - val_accuracy: 0.6893\n",
      "Epoch 6/500\n",
      "13/13 [==============================] - 0s 24ms/step - loss: 0.5956 - accuracy: 0.6901 - val_loss: 0.6054 - val_accuracy: 0.6893\n",
      "Epoch 7/500\n",
      "13/13 [==============================] - 0s 25ms/step - loss: 0.5887 - accuracy: 0.6901 - val_loss: 0.6081 - val_accuracy: 0.6893\n",
      "Epoch 8/500\n",
      "13/13 [==============================] - 0s 25ms/step - loss: 0.5852 - accuracy: 0.6901 - val_loss: 0.6037 - val_accuracy: 0.6893\n",
      "Epoch 9/500\n",
      "13/13 [==============================] - 0s 26ms/step - loss: 0.5816 - accuracy: 0.6901 - val_loss: 0.6073 - val_accuracy: 0.6893\n",
      "Epoch 10/500\n",
      "13/13 [==============================] - 0s 25ms/step - loss: 0.5778 - accuracy: 0.6877 - val_loss: 0.6093 - val_accuracy: 0.6893\n",
      "Epoch 11/500\n",
      "13/13 [==============================] - 0s 24ms/step - loss: 0.5747 - accuracy: 0.6901 - val_loss: 0.6026 - val_accuracy: 0.6990\n",
      "Epoch 12/500\n",
      "13/13 [==============================] - 0s 26ms/step - loss: 0.5703 - accuracy: 0.6925 - val_loss: 0.6042 - val_accuracy: 0.6990\n",
      "Epoch 13/500\n",
      "13/13 [==============================] - 0s 25ms/step - loss: 0.5687 - accuracy: 0.6998 - val_loss: 0.6075 - val_accuracy: 0.6990\n",
      "Epoch 14/500\n",
      "13/13 [==============================] - 0s 25ms/step - loss: 0.5716 - accuracy: 0.7022 - val_loss: 0.6071 - val_accuracy: 0.6990\n",
      "Epoch 15/500\n",
      "13/13 [==============================] - 0s 25ms/step - loss: 0.5596 - accuracy: 0.6949 - val_loss: 0.6074 - val_accuracy: 0.6990\n",
      "Epoch 16/500\n",
      "13/13 [==============================] - 0s 26ms/step - loss: 0.5636 - accuracy: 0.7046 - val_loss: 0.6052 - val_accuracy: 0.6990\n",
      "Epoch 17/500\n",
      "13/13 [==============================] - 0s 22ms/step - loss: 0.5666 - accuracy: 0.6973 - val_loss: 0.6074 - val_accuracy: 0.6990\n",
      "Epoch 18/500\n",
      "13/13 [==============================] - 0s 23ms/step - loss: 0.5535 - accuracy: 0.7046 - val_loss: 0.6059 - val_accuracy: 0.6893\n",
      "Epoch 19/500\n",
      "13/13 [==============================] - 0s 25ms/step - loss: 0.5529 - accuracy: 0.7191 - val_loss: 0.6103 - val_accuracy: 0.6893\n",
      "Epoch 20/500\n",
      "13/13 [==============================] - 0s 24ms/step - loss: 0.5523 - accuracy: 0.7094 - val_loss: 0.6106 - val_accuracy: 0.6990\n",
      "Epoch 21/500\n",
      "13/13 [==============================] - 0s 23ms/step - loss: 0.5488 - accuracy: 0.7070 - val_loss: 0.6123 - val_accuracy: 0.6893\n",
      "Epoch 22/500\n",
      "13/13 [==============================] - 0s 22ms/step - loss: 0.5499 - accuracy: 0.7167 - val_loss: 0.6097 - val_accuracy: 0.6893\n",
      "Epoch 23/500\n",
      "13/13 [==============================] - 0s 24ms/step - loss: 0.5441 - accuracy: 0.7337 - val_loss: 0.6070 - val_accuracy: 0.6990\n",
      "Epoch 24/500\n",
      "13/13 [==============================] - 0s 25ms/step - loss: 0.5396 - accuracy: 0.7143 - val_loss: 0.6114 - val_accuracy: 0.6990\n",
      "Epoch 25/500\n",
      "13/13 [==============================] - 0s 25ms/step - loss: 0.5401 - accuracy: 0.7312 - val_loss: 0.6195 - val_accuracy: 0.6893\n",
      "Epoch 26/500\n",
      "13/13 [==============================] - 0s 23ms/step - loss: 0.5367 - accuracy: 0.7288 - val_loss: 0.6117 - val_accuracy: 0.7282\n",
      "Epoch 27/500\n",
      "13/13 [==============================] - 0s 23ms/step - loss: 0.5348 - accuracy: 0.7337 - val_loss: 0.6186 - val_accuracy: 0.6990\n",
      "Epoch 28/500\n",
      "13/13 [==============================] - 0s 25ms/step - loss: 0.5366 - accuracy: 0.7264 - val_loss: 0.6170 - val_accuracy: 0.6990\n",
      "Epoch 29/500\n",
      "13/13 [==============================] - 0s 27ms/step - loss: 0.5347 - accuracy: 0.7482 - val_loss: 0.6167 - val_accuracy: 0.7087\n",
      "Epoch 30/500\n",
      "13/13 [==============================] - 0s 26ms/step - loss: 0.5284 - accuracy: 0.7337 - val_loss: 0.6180 - val_accuracy: 0.7087\n",
      "Epoch 31/500\n",
      "13/13 [==============================] - 0s 27ms/step - loss: 0.5261 - accuracy: 0.7385 - val_loss: 0.6176 - val_accuracy: 0.7087\n",
      "Epoch 32/500\n",
      "13/13 [==============================] - 0s 26ms/step - loss: 0.5200 - accuracy: 0.7554 - val_loss: 0.6234 - val_accuracy: 0.7087\n",
      "Epoch 33/500\n",
      "13/13 [==============================] - 0s 27ms/step - loss: 0.5194 - accuracy: 0.7312 - val_loss: 0.6219 - val_accuracy: 0.7184\n",
      "Epoch 34/500\n",
      "13/13 [==============================] - 0s 26ms/step - loss: 0.5159 - accuracy: 0.7506 - val_loss: 0.6270 - val_accuracy: 0.6990\n",
      "Epoch 35/500\n",
      "13/13 [==============================] - 0s 27ms/step - loss: 0.5171 - accuracy: 0.7506 - val_loss: 0.6246 - val_accuracy: 0.7184\n",
      "Epoch 36/500\n",
      "13/13 [==============================] - 0s 26ms/step - loss: 0.5053 - accuracy: 0.7627 - val_loss: 0.6291 - val_accuracy: 0.6990\n",
      "Epoch 37/500\n",
      "13/13 [==============================] - 0s 25ms/step - loss: 0.5128 - accuracy: 0.7458 - val_loss: 0.6251 - val_accuracy: 0.7087\n",
      "Epoch 38/500\n",
      "13/13 [==============================] - 0s 25ms/step - loss: 0.5164 - accuracy: 0.7603 - val_loss: 0.6332 - val_accuracy: 0.6990\n",
      "Epoch 39/500\n",
      "13/13 [==============================] - 0s 25ms/step - loss: 0.5050 - accuracy: 0.7627 - val_loss: 0.6271 - val_accuracy: 0.7087\n",
      "Epoch 40/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13/13 [==============================] - 0s 25ms/step - loss: 0.4973 - accuracy: 0.7724 - val_loss: 0.6350 - val_accuracy: 0.7087\n",
      "Epoch 41/500\n",
      "13/13 [==============================] - 0s 26ms/step - loss: 0.4880 - accuracy: 0.7724 - val_loss: 0.6423 - val_accuracy: 0.6990\n",
      "Epoch 42/500\n",
      "13/13 [==============================] - 0s 25ms/step - loss: 0.4961 - accuracy: 0.7676 - val_loss: 0.6419 - val_accuracy: 0.7087\n",
      "Epoch 43/500\n",
      "13/13 [==============================] - 0s 26ms/step - loss: 0.4933 - accuracy: 0.7627 - val_loss: 0.6524 - val_accuracy: 0.6990\n",
      "Epoch 44/500\n",
      "13/13 [==============================] - 0s 25ms/step - loss: 0.4955 - accuracy: 0.7627 - val_loss: 0.6412 - val_accuracy: 0.6990\n",
      "Epoch 45/500\n",
      "13/13 [==============================] - 0s 27ms/step - loss: 0.4865 - accuracy: 0.7724 - val_loss: 0.6419 - val_accuracy: 0.7087\n",
      "Epoch 46/500\n",
      "13/13 [==============================] - 0s 28ms/step - loss: 0.4875 - accuracy: 0.7748 - val_loss: 0.6552 - val_accuracy: 0.6990\n",
      "Epoch 47/500\n",
      "13/13 [==============================] - 0s 25ms/step - loss: 0.4898 - accuracy: 0.7506 - val_loss: 0.6449 - val_accuracy: 0.7087\n",
      "Epoch 48/500\n",
      "13/13 [==============================] - 0s 28ms/step - loss: 0.4803 - accuracy: 0.7748 - val_loss: 0.6561 - val_accuracy: 0.7087\n",
      "Epoch 49/500\n",
      "13/13 [==============================] - 0s 25ms/step - loss: 0.4737 - accuracy: 0.7772 - val_loss: 0.6611 - val_accuracy: 0.6893\n",
      "Epoch 50/500\n",
      "13/13 [==============================] - 0s 26ms/step - loss: 0.4741 - accuracy: 0.7893 - val_loss: 0.6536 - val_accuracy: 0.6893\n",
      "Epoch 51/500\n",
      "13/13 [==============================] - 0s 27ms/step - loss: 0.4754 - accuracy: 0.7869 - val_loss: 0.6643 - val_accuracy: 0.6893\n",
      "Epoch 52/500\n",
      "13/13 [==============================] - 0s 24ms/step - loss: 0.4745 - accuracy: 0.7821 - val_loss: 0.6559 - val_accuracy: 0.7087\n",
      "Epoch 53/500\n",
      "13/13 [==============================] - 0s 25ms/step - loss: 0.4659 - accuracy: 0.7821 - val_loss: 0.6658 - val_accuracy: 0.6893\n",
      "Epoch 54/500\n",
      "13/13 [==============================] - 0s 26ms/step - loss: 0.4735 - accuracy: 0.7966 - val_loss: 0.6628 - val_accuracy: 0.6990\n",
      "Epoch 55/500\n",
      "13/13 [==============================] - 0s 26ms/step - loss: 0.4715 - accuracy: 0.7893 - val_loss: 0.6655 - val_accuracy: 0.7087\n",
      "Epoch 56/500\n",
      "13/13 [==============================] - 0s 27ms/step - loss: 0.4682 - accuracy: 0.7869 - val_loss: 0.6700 - val_accuracy: 0.6893\n",
      "Epoch 57/500\n",
      "13/13 [==============================] - 0s 25ms/step - loss: 0.4638 - accuracy: 0.7893 - val_loss: 0.6712 - val_accuracy: 0.6893\n",
      "Epoch 58/500\n",
      "13/13 [==============================] - 0s 25ms/step - loss: 0.4671 - accuracy: 0.8015 - val_loss: 0.6715 - val_accuracy: 0.7087\n",
      "Epoch 59/500\n",
      "13/13 [==============================] - 0s 26ms/step - loss: 0.4563 - accuracy: 0.8232 - val_loss: 0.6908 - val_accuracy: 0.6893\n",
      "Epoch 60/500\n",
      "13/13 [==============================] - 0s 22ms/step - loss: 0.4540 - accuracy: 0.7918 - val_loss: 0.6741 - val_accuracy: 0.7087\n",
      "Epoch 61/500\n",
      "13/13 [==============================] - 0s 23ms/step - loss: 0.4511 - accuracy: 0.7990 - val_loss: 0.6806 - val_accuracy: 0.6990\n",
      "Epoch 62/500\n",
      "13/13 [==============================] - 0s 26ms/step - loss: 0.4547 - accuracy: 0.7966 - val_loss: 0.6916 - val_accuracy: 0.6893\n",
      "Epoch 63/500\n",
      "13/13 [==============================] - 0s 27ms/step - loss: 0.4392 - accuracy: 0.8087 - val_loss: 0.6942 - val_accuracy: 0.6796\n",
      "Epoch 64/500\n",
      "13/13 [==============================] - 0s 26ms/step - loss: 0.4460 - accuracy: 0.8015 - val_loss: 0.6920 - val_accuracy: 0.6990\n",
      "Epoch 65/500\n",
      "13/13 [==============================] - 0s 24ms/step - loss: 0.4388 - accuracy: 0.8329 - val_loss: 0.7060 - val_accuracy: 0.6796\n",
      "Epoch 66/500\n",
      "13/13 [==============================] - 0s 24ms/step - loss: 0.4403 - accuracy: 0.7966 - val_loss: 0.6934 - val_accuracy: 0.7087\n",
      "Epoch 67/500\n",
      "13/13 [==============================] - 0s 24ms/step - loss: 0.4421 - accuracy: 0.8039 - val_loss: 0.7116 - val_accuracy: 0.6796\n",
      "Epoch 68/500\n",
      "13/13 [==============================] - 0s 24ms/step - loss: 0.4328 - accuracy: 0.8111 - val_loss: 0.6975 - val_accuracy: 0.7087\n",
      "Epoch 69/500\n",
      "13/13 [==============================] - 0s 25ms/step - loss: 0.4304 - accuracy: 0.8329 - val_loss: 0.7156 - val_accuracy: 0.6893\n",
      "Epoch 70/500\n",
      "13/13 [==============================] - 0s 24ms/step - loss: 0.4379 - accuracy: 0.8111 - val_loss: 0.7025 - val_accuracy: 0.7087\n",
      "Epoch 71/500\n",
      "13/13 [==============================] - 0s 25ms/step - loss: 0.4331 - accuracy: 0.8111 - val_loss: 0.7157 - val_accuracy: 0.6990\n",
      "Epoch 72/500\n",
      "13/13 [==============================] - 0s 27ms/step - loss: 0.4336 - accuracy: 0.8160 - val_loss: 0.7063 - val_accuracy: 0.7087\n",
      "Epoch 73/500\n",
      "13/13 [==============================] - 0s 23ms/step - loss: 0.4235 - accuracy: 0.8208 - val_loss: 0.7204 - val_accuracy: 0.6796\n",
      "Epoch 74/500\n",
      "13/13 [==============================] - 0s 24ms/step - loss: 0.4208 - accuracy: 0.8208 - val_loss: 0.7100 - val_accuracy: 0.7087\n",
      "Epoch 75/500\n",
      "13/13 [==============================] - 0s 28ms/step - loss: 0.4235 - accuracy: 0.8450 - val_loss: 0.7343 - val_accuracy: 0.6796\n",
      "Epoch 76/500\n",
      "13/13 [==============================] - 0s 25ms/step - loss: 0.4394 - accuracy: 0.8039 - val_loss: 0.7231 - val_accuracy: 0.7087\n",
      "Epoch 77/500\n",
      "13/13 [==============================] - 0s 26ms/step - loss: 0.4174 - accuracy: 0.8257 - val_loss: 0.7176 - val_accuracy: 0.7087\n",
      "Epoch 78/500\n",
      "13/13 [==============================] - 0s 26ms/step - loss: 0.4172 - accuracy: 0.8305 - val_loss: 0.7332 - val_accuracy: 0.6796\n",
      "Epoch 79/500\n",
      "13/13 [==============================] - 0s 26ms/step - loss: 0.4111 - accuracy: 0.8232 - val_loss: 0.7352 - val_accuracy: 0.6893\n",
      "Epoch 80/500\n",
      "13/13 [==============================] - 0s 26ms/step - loss: 0.4275 - accuracy: 0.8039 - val_loss: 0.7293 - val_accuracy: 0.6893\n",
      "Epoch 81/500\n",
      "13/13 [==============================] - 0s 24ms/step - loss: 0.4139 - accuracy: 0.8184 - val_loss: 0.7507 - val_accuracy: 0.6893\n",
      "Epoch 82/500\n",
      "13/13 [==============================] - 0s 26ms/step - loss: 0.4140 - accuracy: 0.8208 - val_loss: 0.7273 - val_accuracy: 0.7184\n",
      "Epoch 83/500\n",
      "13/13 [==============================] - 0s 25ms/step - loss: 0.4214 - accuracy: 0.8160 - val_loss: 0.7543 - val_accuracy: 0.6796\n",
      "Epoch 84/500\n",
      "13/13 [==============================] - 0s 25ms/step - loss: 0.4101 - accuracy: 0.8232 - val_loss: 0.7348 - val_accuracy: 0.6990\n",
      "Epoch 85/500\n",
      "13/13 [==============================] - 0s 24ms/step - loss: 0.4033 - accuracy: 0.8450 - val_loss: 0.7572 - val_accuracy: 0.6796\n",
      "Epoch 86/500\n",
      "13/13 [==============================] - 0s 24ms/step - loss: 0.4069 - accuracy: 0.8160 - val_loss: 0.7375 - val_accuracy: 0.7087\n",
      "Epoch 87/500\n",
      "13/13 [==============================] - 0s 24ms/step - loss: 0.4074 - accuracy: 0.8402 - val_loss: 0.7790 - val_accuracy: 0.6893\n",
      "Epoch 88/500\n",
      "13/13 [==============================] - 0s 28ms/step - loss: 0.3984 - accuracy: 0.8305 - val_loss: 0.7441 - val_accuracy: 0.6990\n",
      "Epoch 89/500\n",
      "13/13 [==============================] - 0s 24ms/step - loss: 0.3974 - accuracy: 0.8547 - val_loss: 0.7565 - val_accuracy: 0.6893\n",
      "Epoch 90/500\n",
      "13/13 [==============================] - 0s 27ms/step - loss: 0.3908 - accuracy: 0.8475 - val_loss: 0.7593 - val_accuracy: 0.6990\n",
      "Epoch 91/500\n",
      "13/13 [==============================] - 0s 27ms/step - loss: 0.3995 - accuracy: 0.8402 - val_loss: 0.7543 - val_accuracy: 0.6893\n",
      "Epoch 92/500\n",
      "13/13 [==============================] - 0s 25ms/step - loss: 0.3912 - accuracy: 0.8378 - val_loss: 0.7650 - val_accuracy: 0.6893\n",
      "Epoch 93/500\n",
      "13/13 [==============================] - 0s 27ms/step - loss: 0.3877 - accuracy: 0.8571 - val_loss: 0.7666 - val_accuracy: 0.6796\n",
      "Epoch 94/500\n",
      "13/13 [==============================] - 0s 27ms/step - loss: 0.3853 - accuracy: 0.8475 - val_loss: 0.7723 - val_accuracy: 0.6990\n",
      "Epoch 95/500\n",
      "13/13 [==============================] - 0s 28ms/step - loss: 0.3888 - accuracy: 0.8426 - val_loss: 0.7649 - val_accuracy: 0.6990\n",
      "Epoch 96/500\n",
      "13/13 [==============================] - 0s 27ms/step - loss: 0.3856 - accuracy: 0.8450 - val_loss: 0.7837 - val_accuracy: 0.6796\n",
      "Epoch 97/500\n",
      "13/13 [==============================] - 0s 27ms/step - loss: 0.3823 - accuracy: 0.8499 - val_loss: 0.7784 - val_accuracy: 0.6893\n",
      "Epoch 98/500\n",
      "13/13 [==============================] - 0s 27ms/step - loss: 0.3810 - accuracy: 0.8547 - val_loss: 0.7736 - val_accuracy: 0.6990\n",
      "Epoch 99/500\n",
      "13/13 [==============================] - 0s 26ms/step - loss: 0.3825 - accuracy: 0.8523 - val_loss: 0.7787 - val_accuracy: 0.6990\n",
      "Epoch 100/500\n",
      "13/13 [==============================] - 0s 26ms/step - loss: 0.3753 - accuracy: 0.8571 - val_loss: 0.7873 - val_accuracy: 0.6893\n",
      "Epoch 101/500\n",
      "13/13 [==============================] - 0s 27ms/step - loss: 0.3810 - accuracy: 0.8692 - val_loss: 0.7868 - val_accuracy: 0.6990\n",
      "Epoch 102/500\n",
      "13/13 [==============================] - 0s 25ms/step - loss: 0.3731 - accuracy: 0.8571 - val_loss: 0.7963 - val_accuracy: 0.6990\n",
      "Epoch 103/500\n",
      "13/13 [==============================] - 0s 25ms/step - loss: 0.3760 - accuracy: 0.8668 - val_loss: 0.7813 - val_accuracy: 0.6699\n",
      "Epoch 104/500\n",
      "13/13 [==============================] - 0s 23ms/step - loss: 0.3727 - accuracy: 0.8475 - val_loss: 0.7870 - val_accuracy: 0.6893\n",
      "Epoch 105/500\n",
      "13/13 [==============================] - 0s 23ms/step - loss: 0.3686 - accuracy: 0.8644 - val_loss: 0.8086 - val_accuracy: 0.6893\n",
      "Epoch 106/500\n",
      "13/13 [==============================] - 0s 25ms/step - loss: 0.3695 - accuracy: 0.8644 - val_loss: 0.7819 - val_accuracy: 0.6311\n",
      "Epoch 107/500\n",
      "13/13 [==============================] - 0s 25ms/step - loss: 0.3712 - accuracy: 0.8668 - val_loss: 0.7951 - val_accuracy: 0.6893\n",
      "Epoch 108/500\n",
      "13/13 [==============================] - 0s 26ms/step - loss: 0.3679 - accuracy: 0.8547 - val_loss: 0.8132 - val_accuracy: 0.6893\n",
      "Epoch 109/500\n",
      "13/13 [==============================] - 0s 25ms/step - loss: 0.3665 - accuracy: 0.8547 - val_loss: 0.7937 - val_accuracy: 0.6602\n",
      "Epoch 110/500\n",
      "13/13 [==============================] - 0s 24ms/step - loss: 0.3542 - accuracy: 0.8717 - val_loss: 0.8044 - val_accuracy: 0.6893\n",
      "Epoch 111/500\n",
      "13/13 [==============================] - 0s 25ms/step - loss: 0.3538 - accuracy: 0.8692 - val_loss: 0.8035 - val_accuracy: 0.6796\n",
      "Epoch 112/500\n",
      "13/13 [==============================] - 0s 27ms/step - loss: 0.3652 - accuracy: 0.8596 - val_loss: 0.8099 - val_accuracy: 0.6796\n",
      "Epoch 113/500\n",
      "13/13 [==============================] - 0s 24ms/step - loss: 0.3510 - accuracy: 0.8620 - val_loss: 0.8111 - val_accuracy: 0.6796\n",
      "Epoch 114/500\n",
      "13/13 [==============================] - 0s 27ms/step - loss: 0.3583 - accuracy: 0.8547 - val_loss: 0.8059 - val_accuracy: 0.6602\n",
      "Epoch 115/500\n",
      "13/13 [==============================] - 0s 25ms/step - loss: 0.3451 - accuracy: 0.8717 - val_loss: 0.8378 - val_accuracy: 0.6796\n",
      "Epoch 116/500\n",
      "13/13 [==============================] - 0s 25ms/step - loss: 0.3567 - accuracy: 0.8596 - val_loss: 0.8167 - val_accuracy: 0.6796\n",
      "Epoch 117/500\n",
      "13/13 [==============================] - 0s 26ms/step - loss: 0.3443 - accuracy: 0.8717 - val_loss: 0.8176 - val_accuracy: 0.6796\n",
      "Epoch 118/500\n",
      "13/13 [==============================] - 0s 24ms/step - loss: 0.3383 - accuracy: 0.8789 - val_loss: 0.8060 - val_accuracy: 0.6019\n",
      "Epoch 119/500\n",
      "13/13 [==============================] - 0s 24ms/step - loss: 0.3449 - accuracy: 0.8644 - val_loss: 0.8218 - val_accuracy: 0.6699\n",
      "Epoch 120/500\n",
      "13/13 [==============================] - 0s 26ms/step - loss: 0.3557 - accuracy: 0.8717 - val_loss: 0.8291 - val_accuracy: 0.6796\n",
      "Epoch 121/500\n",
      "13/13 [==============================] - 0s 25ms/step - loss: 0.3517 - accuracy: 0.8765 - val_loss: 0.8136 - val_accuracy: 0.6699\n",
      "Epoch 122/500\n",
      "13/13 [==============================] - 0s 24ms/step - loss: 0.3471 - accuracy: 0.8571 - val_loss: 0.8251 - val_accuracy: 0.6796\n",
      "Epoch 123/500\n",
      "13/13 [==============================] - 0s 23ms/step - loss: 0.3369 - accuracy: 0.8959 - val_loss: 0.8300 - val_accuracy: 0.6602\n",
      "Epoch 124/500\n",
      "13/13 [==============================] - 0s 23ms/step - loss: 0.3395 - accuracy: 0.8692 - val_loss: 0.8306 - val_accuracy: 0.6796\n",
      "Epoch 125/500\n",
      "13/13 [==============================] - 0s 24ms/step - loss: 0.3389 - accuracy: 0.8838 - val_loss: 0.8212 - val_accuracy: 0.6699\n",
      "Epoch 126/500\n",
      "13/13 [==============================] - 0s 25ms/step - loss: 0.3349 - accuracy: 0.8910 - val_loss: 0.8433 - val_accuracy: 0.6893\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1/500\n",
      "13/13 [==============================] - 1s 36ms/step - loss: 0.6321 - accuracy: 0.6707 - val_loss: 0.5867 - val_accuracy: 0.6893\n",
      "Epoch 2/500\n",
      "13/13 [==============================] - 0s 26ms/step - loss: 0.6095 - accuracy: 0.6901 - val_loss: 0.5914 - val_accuracy: 0.6893\n",
      "Epoch 3/500\n",
      "13/13 [==============================] - 0s 25ms/step - loss: 0.6134 - accuracy: 0.6901 - val_loss: 0.5879 - val_accuracy: 0.6893\n",
      "Epoch 4/500\n",
      "13/13 [==============================] - 0s 27ms/step - loss: 0.6128 - accuracy: 0.6901 - val_loss: 0.5815 - val_accuracy: 0.6893\n",
      "Epoch 5/500\n",
      "13/13 [==============================] - 0s 25ms/step - loss: 0.6060 - accuracy: 0.6901 - val_loss: 0.5887 - val_accuracy: 0.6893\n",
      "Epoch 6/500\n",
      "13/13 [==============================] - 0s 30ms/step - loss: 0.6081 - accuracy: 0.6901 - val_loss: 0.5801 - val_accuracy: 0.6893\n",
      "Epoch 7/500\n",
      "13/13 [==============================] - 0s 27ms/step - loss: 0.6003 - accuracy: 0.6901 - val_loss: 0.5838 - val_accuracy: 0.6893\n",
      "Epoch 8/500\n",
      "13/13 [==============================] - 0s 25ms/step - loss: 0.6001 - accuracy: 0.6901 - val_loss: 0.5818 - val_accuracy: 0.6893\n",
      "Epoch 9/500\n",
      "13/13 [==============================] - 0s 25ms/step - loss: 0.5991 - accuracy: 0.6901 - val_loss: 0.5792 - val_accuracy: 0.6893\n",
      "Epoch 10/500\n",
      "13/13 [==============================] - 0s 25ms/step - loss: 0.5967 - accuracy: 0.6901 - val_loss: 0.5795 - val_accuracy: 0.6893\n",
      "Epoch 11/500\n",
      "13/13 [==============================] - 0s 24ms/step - loss: 0.5956 - accuracy: 0.6901 - val_loss: 0.5808 - val_accuracy: 0.6893\n",
      "Epoch 12/500\n",
      "13/13 [==============================] - 0s 24ms/step - loss: 0.5930 - accuracy: 0.6901 - val_loss: 0.5808 - val_accuracy: 0.6893\n",
      "Epoch 13/500\n",
      "13/13 [==============================] - 0s 24ms/step - loss: 0.5920 - accuracy: 0.6901 - val_loss: 0.5787 - val_accuracy: 0.6893\n",
      "Epoch 14/500\n",
      "13/13 [==============================] - 0s 25ms/step - loss: 0.5941 - accuracy: 0.6877 - val_loss: 0.5776 - val_accuracy: 0.6893\n",
      "Epoch 15/500\n",
      "13/13 [==============================] - 0s 26ms/step - loss: 0.5892 - accuracy: 0.6901 - val_loss: 0.5776 - val_accuracy: 0.6893\n",
      "Epoch 16/500\n",
      "13/13 [==============================] - 0s 23ms/step - loss: 0.5881 - accuracy: 0.6901 - val_loss: 0.5783 - val_accuracy: 0.6893\n",
      "Epoch 17/500\n",
      "13/13 [==============================] - 0s 24ms/step - loss: 0.5874 - accuracy: 0.6901 - val_loss: 0.5785 - val_accuracy: 0.6893\n",
      "Epoch 18/500\n",
      "13/13 [==============================] - 0s 28ms/step - loss: 0.5838 - accuracy: 0.6925 - val_loss: 0.5796 - val_accuracy: 0.6796\n",
      "Epoch 19/500\n",
      "13/13 [==============================] - 0s 25ms/step - loss: 0.5882 - accuracy: 0.6973 - val_loss: 0.5827 - val_accuracy: 0.6796\n",
      "Epoch 20/500\n",
      "13/13 [==============================] - 0s 24ms/step - loss: 0.5855 - accuracy: 0.6949 - val_loss: 0.5784 - val_accuracy: 0.6796\n",
      "Epoch 21/500\n",
      "13/13 [==============================] - 0s 26ms/step - loss: 0.5834 - accuracy: 0.6998 - val_loss: 0.5808 - val_accuracy: 0.6796\n",
      "Epoch 22/500\n",
      "13/13 [==============================] - 0s 25ms/step - loss: 0.5809 - accuracy: 0.6998 - val_loss: 0.5791 - val_accuracy: 0.6796\n",
      "Epoch 23/500\n",
      "13/13 [==============================] - 0s 25ms/step - loss: 0.5803 - accuracy: 0.6998 - val_loss: 0.5781 - val_accuracy: 0.6796\n",
      "Epoch 24/500\n",
      "13/13 [==============================] - 0s 26ms/step - loss: 0.5779 - accuracy: 0.7046 - val_loss: 0.5816 - val_accuracy: 0.6699\n",
      "Epoch 25/500\n",
      "13/13 [==============================] - 0s 26ms/step - loss: 0.5816 - accuracy: 0.7022 - val_loss: 0.5795 - val_accuracy: 0.6699\n",
      "Epoch 26/500\n",
      "13/13 [==============================] - 0s 25ms/step - loss: 0.5785 - accuracy: 0.7070 - val_loss: 0.5802 - val_accuracy: 0.6699\n",
      "Epoch 27/500\n",
      "13/13 [==============================] - 0s 26ms/step - loss: 0.5767 - accuracy: 0.7070 - val_loss: 0.5802 - val_accuracy: 0.6699\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/500\n",
      "13/13 [==============================] - 0s 26ms/step - loss: 0.5759 - accuracy: 0.7094 - val_loss: 0.5825 - val_accuracy: 0.6699\n",
      "Epoch 29/500\n",
      "13/13 [==============================] - 0s 25ms/step - loss: 0.5752 - accuracy: 0.7070 - val_loss: 0.5810 - val_accuracy: 0.6699\n",
      "Epoch 30/500\n",
      "13/13 [==============================] - 0s 27ms/step - loss: 0.5698 - accuracy: 0.7094 - val_loss: 0.5832 - val_accuracy: 0.6699\n",
      "Epoch 31/500\n",
      "13/13 [==============================] - 0s 24ms/step - loss: 0.5735 - accuracy: 0.7094 - val_loss: 0.5827 - val_accuracy: 0.6699\n",
      "Epoch 32/500\n",
      "13/13 [==============================] - 0s 26ms/step - loss: 0.5697 - accuracy: 0.7094 - val_loss: 0.5819 - val_accuracy: 0.6699\n",
      "Epoch 33/500\n",
      "13/13 [==============================] - 0s 24ms/step - loss: 0.5698 - accuracy: 0.7070 - val_loss: 0.5824 - val_accuracy: 0.6505\n",
      "Epoch 34/500\n",
      "13/13 [==============================] - 0s 25ms/step - loss: 0.5647 - accuracy: 0.7240 - val_loss: 0.5850 - val_accuracy: 0.6311\n",
      "Epoch 35/500\n",
      "13/13 [==============================] - 0s 26ms/step - loss: 0.5661 - accuracy: 0.7215 - val_loss: 0.5827 - val_accuracy: 0.6602\n",
      "Epoch 36/500\n",
      "13/13 [==============================] - 0s 27ms/step - loss: 0.5655 - accuracy: 0.7094 - val_loss: 0.5850 - val_accuracy: 0.6602\n",
      "Epoch 37/500\n",
      "13/13 [==============================] - 0s 24ms/step - loss: 0.5692 - accuracy: 0.7288 - val_loss: 0.5881 - val_accuracy: 0.6117\n",
      "Epoch 38/500\n",
      "13/13 [==============================] - 0s 28ms/step - loss: 0.5655 - accuracy: 0.7215 - val_loss: 0.5863 - val_accuracy: 0.6602\n",
      "Epoch 39/500\n",
      "13/13 [==============================] - 0s 25ms/step - loss: 0.5637 - accuracy: 0.7215 - val_loss: 0.5895 - val_accuracy: 0.6311\n",
      "Epoch 40/500\n",
      "13/13 [==============================] - 0s 25ms/step - loss: 0.5602 - accuracy: 0.7361 - val_loss: 0.5889 - val_accuracy: 0.6311\n",
      "Epoch 41/500\n",
      "13/13 [==============================] - 0s 25ms/step - loss: 0.5640 - accuracy: 0.7288 - val_loss: 0.5877 - val_accuracy: 0.6408\n",
      "Epoch 42/500\n",
      "13/13 [==============================] - 0s 27ms/step - loss: 0.5620 - accuracy: 0.7361 - val_loss: 0.5905 - val_accuracy: 0.6214\n",
      "Epoch 43/500\n",
      "13/13 [==============================] - 0s 26ms/step - loss: 0.5604 - accuracy: 0.7433 - val_loss: 0.5897 - val_accuracy: 0.6311\n",
      "Epoch 44/500\n",
      "13/13 [==============================] - 0s 27ms/step - loss: 0.5584 - accuracy: 0.7264 - val_loss: 0.5908 - val_accuracy: 0.6311\n",
      "Epoch 45/500\n",
      "13/13 [==============================] - 0s 26ms/step - loss: 0.5558 - accuracy: 0.7337 - val_loss: 0.5920 - val_accuracy: 0.6117\n",
      "Epoch 46/500\n",
      "13/13 [==============================] - 0s 27ms/step - loss: 0.5593 - accuracy: 0.7312 - val_loss: 0.5911 - val_accuracy: 0.6117\n",
      "Epoch 47/500\n",
      "13/13 [==============================] - 0s 26ms/step - loss: 0.5556 - accuracy: 0.7288 - val_loss: 0.5919 - val_accuracy: 0.6311\n",
      "Epoch 48/500\n",
      "13/13 [==============================] - 0s 28ms/step - loss: 0.5566 - accuracy: 0.7361 - val_loss: 0.5921 - val_accuracy: 0.6311\n",
      "Epoch 49/500\n",
      "13/13 [==============================] - 0s 27ms/step - loss: 0.5561 - accuracy: 0.7337 - val_loss: 0.5956 - val_accuracy: 0.6214\n",
      "Epoch 50/500\n",
      "13/13 [==============================] - 0s 26ms/step - loss: 0.5532 - accuracy: 0.7409 - val_loss: 0.5930 - val_accuracy: 0.6311\n",
      "Epoch 51/500\n",
      "13/13 [==============================] - 0s 27ms/step - loss: 0.5515 - accuracy: 0.7385 - val_loss: 0.5954 - val_accuracy: 0.6117\n",
      "Epoch 52/500\n",
      "13/13 [==============================] - 0s 29ms/step - loss: 0.5496 - accuracy: 0.7288 - val_loss: 0.5949 - val_accuracy: 0.6117\n",
      "Epoch 53/500\n",
      "13/13 [==============================] - 0s 27ms/step - loss: 0.5496 - accuracy: 0.7361 - val_loss: 0.5938 - val_accuracy: 0.6311\n",
      "Epoch 54/500\n",
      "13/13 [==============================] - 0s 29ms/step - loss: 0.5432 - accuracy: 0.7288 - val_loss: 0.5979 - val_accuracy: 0.6214\n",
      "Epoch 55/500\n",
      "13/13 [==============================] - 0s 25ms/step - loss: 0.5472 - accuracy: 0.7312 - val_loss: 0.5961 - val_accuracy: 0.6214\n",
      "Epoch 56/500\n",
      "13/13 [==============================] - 0s 31ms/step - loss: 0.5495 - accuracy: 0.7361 - val_loss: 0.5945 - val_accuracy: 0.6117\n",
      "Epoch 57/500\n",
      "13/13 [==============================] - 0s 32ms/step - loss: 0.5438 - accuracy: 0.7433 - val_loss: 0.5987 - val_accuracy: 0.6214\n",
      "Epoch 58/500\n",
      "13/13 [==============================] - 0s 30ms/step - loss: 0.5472 - accuracy: 0.7264 - val_loss: 0.5977 - val_accuracy: 0.6214\n",
      "Epoch 59/500\n",
      "13/13 [==============================] - 0s 25ms/step - loss: 0.5451 - accuracy: 0.7337 - val_loss: 0.5983 - val_accuracy: 0.6214\n",
      "Epoch 60/500\n",
      "13/13 [==============================] - 0s 25ms/step - loss: 0.5434 - accuracy: 0.7288 - val_loss: 0.5997 - val_accuracy: 0.6214\n",
      "Epoch 61/500\n",
      "13/13 [==============================] - 0s 29ms/step - loss: 0.5463 - accuracy: 0.7409 - val_loss: 0.5977 - val_accuracy: 0.6214\n",
      "Epoch 62/500\n",
      "13/13 [==============================] - 0s 26ms/step - loss: 0.5435 - accuracy: 0.7409 - val_loss: 0.5977 - val_accuracy: 0.6214\n",
      "Epoch 63/500\n",
      "13/13 [==============================] - 0s 25ms/step - loss: 0.5398 - accuracy: 0.7361 - val_loss: 0.6004 - val_accuracy: 0.6117\n",
      "Epoch 64/500\n",
      "13/13 [==============================] - 0s 26ms/step - loss: 0.5415 - accuracy: 0.7482 - val_loss: 0.5987 - val_accuracy: 0.6214\n",
      "Epoch 65/500\n",
      "13/13 [==============================] - 0s 25ms/step - loss: 0.5382 - accuracy: 0.7385 - val_loss: 0.6002 - val_accuracy: 0.6214\n",
      "Epoch 66/500\n",
      "13/13 [==============================] - 0s 25ms/step - loss: 0.5347 - accuracy: 0.7482 - val_loss: 0.6006 - val_accuracy: 0.6117\n",
      "Epoch 67/500\n",
      "13/13 [==============================] - 0s 24ms/step - loss: 0.5389 - accuracy: 0.7409 - val_loss: 0.6020 - val_accuracy: 0.6117\n",
      "Epoch 68/500\n",
      "13/13 [==============================] - 0s 27ms/step - loss: 0.5350 - accuracy: 0.7433 - val_loss: 0.6012 - val_accuracy: 0.6214\n",
      "Epoch 69/500\n",
      "13/13 [==============================] - 0s 26ms/step - loss: 0.5376 - accuracy: 0.7385 - val_loss: 0.6030 - val_accuracy: 0.6117\n",
      "Epoch 70/500\n",
      "13/13 [==============================] - 0s 26ms/step - loss: 0.5342 - accuracy: 0.7506 - val_loss: 0.6049 - val_accuracy: 0.6117\n",
      "Epoch 71/500\n",
      "13/13 [==============================] - 0s 25ms/step - loss: 0.5353 - accuracy: 0.7458 - val_loss: 0.6032 - val_accuracy: 0.6117\n",
      "Epoch 72/500\n",
      "13/13 [==============================] - 0s 28ms/step - loss: 0.5293 - accuracy: 0.7361 - val_loss: 0.6058 - val_accuracy: 0.6117\n",
      "Epoch 73/500\n",
      "13/13 [==============================] - 0s 25ms/step - loss: 0.5293 - accuracy: 0.7361 - val_loss: 0.6043 - val_accuracy: 0.6117\n",
      "Epoch 74/500\n",
      "13/13 [==============================] - 0s 26ms/step - loss: 0.5331 - accuracy: 0.7482 - val_loss: 0.6035 - val_accuracy: 0.6117\n",
      "Epoch 75/500\n",
      "13/13 [==============================] - 0s 25ms/step - loss: 0.5327 - accuracy: 0.7409 - val_loss: 0.6056 - val_accuracy: 0.6117\n",
      "Epoch 76/500\n",
      "13/13 [==============================] - 0s 25ms/step - loss: 0.5298 - accuracy: 0.7458 - val_loss: 0.6075 - val_accuracy: 0.6019\n",
      "Epoch 77/500\n",
      "13/13 [==============================] - 0s 25ms/step - loss: 0.5291 - accuracy: 0.7482 - val_loss: 0.6049 - val_accuracy: 0.6117\n",
      "Epoch 78/500\n",
      "13/13 [==============================] - 0s 22ms/step - loss: 0.5303 - accuracy: 0.7482 - val_loss: 0.6069 - val_accuracy: 0.6019\n",
      "Epoch 79/500\n",
      "13/13 [==============================] - 0s 24ms/step - loss: 0.5299 - accuracy: 0.7385 - val_loss: 0.6079 - val_accuracy: 0.5825\n",
      "Epoch 80/500\n",
      "13/13 [==============================] - 0s 24ms/step - loss: 0.5187 - accuracy: 0.7482 - val_loss: 0.6068 - val_accuracy: 0.6311\n",
      "Epoch 81/500\n",
      "13/13 [==============================] - 0s 25ms/step - loss: 0.5223 - accuracy: 0.7482 - val_loss: 0.6137 - val_accuracy: 0.5728\n",
      "Epoch 82/500\n",
      "13/13 [==============================] - 0s 30ms/step - loss: 0.5200 - accuracy: 0.7530 - val_loss: 0.6094 - val_accuracy: 0.5922\n",
      "Epoch 83/500\n",
      "13/13 [==============================] - 0s 27ms/step - loss: 0.5262 - accuracy: 0.7554 - val_loss: 0.6106 - val_accuracy: 0.6019\n",
      "Epoch 84/500\n",
      "13/13 [==============================] - 0s 27ms/step - loss: 0.5269 - accuracy: 0.7385 - val_loss: 0.6137 - val_accuracy: 0.5631\n",
      "Epoch 85/500\n",
      "13/13 [==============================] - 0s 29ms/step - loss: 0.5218 - accuracy: 0.7603 - val_loss: 0.6111 - val_accuracy: 0.5922\n",
      "Epoch 86/500\n",
      "13/13 [==============================] - 0s 27ms/step - loss: 0.5192 - accuracy: 0.7530 - val_loss: 0.6112 - val_accuracy: 0.5728\n",
      "Epoch 87/500\n",
      "13/13 [==============================] - 0s 28ms/step - loss: 0.5166 - accuracy: 0.7651 - val_loss: 0.6119 - val_accuracy: 0.5825\n",
      "Epoch 88/500\n",
      "13/13 [==============================] - 0s 28ms/step - loss: 0.5150 - accuracy: 0.7603 - val_loss: 0.6115 - val_accuracy: 0.5728\n",
      "Epoch 89/500\n",
      "13/13 [==============================] - 0s 29ms/step - loss: 0.5121 - accuracy: 0.7700 - val_loss: 0.6136 - val_accuracy: 0.5728\n",
      "Epoch 90/500\n",
      "13/13 [==============================] - 0s 27ms/step - loss: 0.5118 - accuracy: 0.7530 - val_loss: 0.6122 - val_accuracy: 0.5728\n",
      "Epoch 91/500\n",
      "13/13 [==============================] - 0s 25ms/step - loss: 0.5132 - accuracy: 0.7579 - val_loss: 0.6145 - val_accuracy: 0.5534\n",
      "Epoch 92/500\n",
      "13/13 [==============================] - 0s 27ms/step - loss: 0.5141 - accuracy: 0.7554 - val_loss: 0.6178 - val_accuracy: 0.5534\n",
      "Epoch 93/500\n",
      "13/13 [==============================] - 0s 25ms/step - loss: 0.5115 - accuracy: 0.7651 - val_loss: 0.6163 - val_accuracy: 0.5728\n",
      "Epoch 94/500\n",
      "13/13 [==============================] - 0s 26ms/step - loss: 0.5085 - accuracy: 0.7676 - val_loss: 0.6185 - val_accuracy: 0.5534\n",
      "Epoch 95/500\n",
      "13/13 [==============================] - 0s 26ms/step - loss: 0.5125 - accuracy: 0.7748 - val_loss: 0.6185 - val_accuracy: 0.5728\n",
      "Epoch 96/500\n",
      "13/13 [==============================] - 0s 27ms/step - loss: 0.5083 - accuracy: 0.7651 - val_loss: 0.6200 - val_accuracy: 0.5631\n",
      "Epoch 97/500\n",
      "13/13 [==============================] - 0s 27ms/step - loss: 0.5078 - accuracy: 0.7579 - val_loss: 0.6249 - val_accuracy: 0.5631\n",
      "Epoch 98/500\n",
      "13/13 [==============================] - 0s 28ms/step - loss: 0.5069 - accuracy: 0.7724 - val_loss: 0.6194 - val_accuracy: 0.5728\n",
      "Epoch 99/500\n",
      "13/13 [==============================] - 0s 26ms/step - loss: 0.5034 - accuracy: 0.7869 - val_loss: 0.6231 - val_accuracy: 0.5534\n",
      "Epoch 100/500\n",
      "13/13 [==============================] - 0s 27ms/step - loss: 0.5085 - accuracy: 0.7700 - val_loss: 0.6229 - val_accuracy: 0.5534\n",
      "Epoch 101/500\n",
      "13/13 [==============================] - 0s 35ms/step - loss: 0.5070 - accuracy: 0.7651 - val_loss: 0.6226 - val_accuracy: 0.5534\n",
      "2/2 [==============================] - 0s 8ms/step\n"
     ]
    }
   ],
   "source": [
    "# Variable for keeping count of split we are executing\n",
    "j = 0\n",
    "list_kfold = list(kfold.split(X_trainval, y_trainval))\n",
    "data_kfold = pd.DataFrame()\n",
    "for i in range(5):\n",
    "    train_idx, val_idx = list_kfold[i]\n",
    "    X_train_kfold = X_trainval[train_idx]\n",
    "    y_train_kfold = y_trainval[train_idx]\n",
    "    y_train_kfold_complement = 1 - y_train_kfold \n",
    "    #Compute its complement and try to join it with 'y_train_kfold' to make it 2-D.\n",
    "    y_train_kfold = np.stack((y_train_kfold, y_train_kfold_complement), axis=1)\n",
    "    #RMK, here col[0] is progressor, col[1] is non-pro\n",
    "    X_val_kfold = X_trainval[val_idx]\n",
    "    y_val_kfold = y_trainval[val_idx]\n",
    "    y_val_kfold_complement = 1 - y_val_kfold\n",
    "    y_val_kfold = np.stack((y_val_kfold, y_val_kfold_complement), axis=1)\n",
    "    ##RMK, here col[0] is progressor, col[1] is non-pro\n",
    "    j += 1\n",
    "    model_kfold = get_model()\n",
    "    model_kfold.fit(X_train_kfold, y_train_kfold,\n",
    "                   validation_data = (X_val_kfold, y_val_kfold),\n",
    "                   epochs = 500, verbose = True,\n",
    "                   callbacks = [early_stopping_monitor])\n",
    "    pred = model_kfold.predict(X_test)\n",
    "    predicted_class_indices=np.argmax(pred, axis = 1)\n",
    "    data_kfold[j] = predicted_class_indices\n",
    "#    gc.collect()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "aa0d5145",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    1  2  3  4  5\n",
       "0   1  1  1  1  1\n",
       "1   1  1  1  1  1\n",
       "2   1  1  1  1  1\n",
       "3   1  1  1  0  1\n",
       "4   1  1  1  1  1\n",
       "5   1  1  1  0  1\n",
       "6   1  1  1  1  1\n",
       "7   1  1  1  1  1\n",
       "8   1  1  1  0  1\n",
       "9   1  1  1  1  1\n",
       "10  1  1  1  1  1\n",
       "11  1  1  1  1  1\n",
       "12  1  1  1  1  1\n",
       "13  1  1  1  1  1\n",
       "14  1  1  1  1  1\n",
       "15  1  1  1  1  1\n",
       "16  1  1  1  1  1\n",
       "17  1  1  1  1  1\n",
       "18  1  1  1  1  1\n",
       "19  1  1  1  1  1\n",
       "20  1  1  1  1  1\n",
       "21  1  1  1  1  1\n",
       "22  1  1  1  1  1\n",
       "23  1  1  1  1  1\n",
       "24  1  1  1  1  1\n",
       "25  1  1  1  1  1\n",
       "26  1  1  1  1  1\n",
       "27  1  1  1  1  1\n",
       "28  1  1  1  1  1\n",
       "29  1  1  1  1  1\n",
       "30  1  1  1  1  1\n",
       "31  1  1  1  1  1\n",
       "32  1  1  1  1  1\n",
       "33  1  1  1  0  1\n",
       "34  1  1  1  1  1\n",
       "35  1  1  1  0  1\n",
       "36  1  1  1  1  1\n",
       "37  1  1  1  1  1\n",
       "38  1  1  1  1  1\n",
       "39  1  1  1  1  1\n",
       "40  1  1  1  1  1\n",
       "41  1  1  1  1  1\n",
       "42  1  1  1  1  1\n",
       "43  1  1  1  1  1\n",
       "44  1  1  1  1  1\n",
       "45  1  1  1  0  1\n",
       "46  1  1  1  1  1\n",
       "47  1  1  1  1  1\n",
       "48  1  1  1  1  1\n",
       "49  1  1  1  1  1\n",
       "50  1  1  1  1  1\n",
       "51  1  1  1  1  1\n",
       "52  1  1  1  0  1\n",
       "53  1  1  1  1  1\n",
       "54  1  1  1  1  1\n",
       "55  1  1  1  1  1\n",
       "56  1  1  1  1  1\n",
       "57  1  1  1  0  1\n",
       "58  1  1  1  1  1"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_kfold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "f7d9db79",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 1, 1, 1],\n",
       "       [1, 1, 1, 1, 1],\n",
       "       [1, 1, 1, 1, 1],\n",
       "       [1, 1, 1, 0, 1],\n",
       "       [1, 1, 1, 1, 1],\n",
       "       [1, 1, 1, 0, 1],\n",
       "       [1, 1, 1, 1, 1],\n",
       "       [1, 1, 1, 1, 1],\n",
       "       [1, 1, 1, 0, 1],\n",
       "       [1, 1, 1, 1, 1],\n",
       "       [1, 1, 1, 1, 1],\n",
       "       [1, 1, 1, 1, 1],\n",
       "       [1, 1, 1, 1, 1],\n",
       "       [1, 1, 1, 1, 1],\n",
       "       [1, 1, 1, 1, 1],\n",
       "       [1, 1, 1, 1, 1],\n",
       "       [1, 1, 1, 1, 1],\n",
       "       [1, 1, 1, 1, 1],\n",
       "       [1, 1, 1, 1, 1],\n",
       "       [1, 1, 1, 1, 1],\n",
       "       [1, 1, 1, 1, 1],\n",
       "       [1, 1, 1, 1, 1],\n",
       "       [1, 1, 1, 1, 1],\n",
       "       [1, 1, 1, 1, 1],\n",
       "       [1, 1, 1, 1, 1],\n",
       "       [1, 1, 1, 1, 1],\n",
       "       [1, 1, 1, 1, 1],\n",
       "       [1, 1, 1, 1, 1],\n",
       "       [1, 1, 1, 1, 1],\n",
       "       [1, 1, 1, 1, 1],\n",
       "       [1, 1, 1, 1, 1],\n",
       "       [1, 1, 1, 1, 1],\n",
       "       [1, 1, 1, 1, 1],\n",
       "       [1, 1, 1, 0, 1],\n",
       "       [1, 1, 1, 1, 1],\n",
       "       [1, 1, 1, 0, 1],\n",
       "       [1, 1, 1, 1, 1],\n",
       "       [1, 1, 1, 1, 1],\n",
       "       [1, 1, 1, 1, 1],\n",
       "       [1, 1, 1, 1, 1],\n",
       "       [1, 1, 1, 1, 1],\n",
       "       [1, 1, 1, 1, 1],\n",
       "       [1, 1, 1, 1, 1],\n",
       "       [1, 1, 1, 1, 1],\n",
       "       [1, 1, 1, 1, 1],\n",
       "       [1, 1, 1, 0, 1],\n",
       "       [1, 1, 1, 1, 1],\n",
       "       [1, 1, 1, 1, 1],\n",
       "       [1, 1, 1, 1, 1],\n",
       "       [1, 1, 1, 1, 1],\n",
       "       [1, 1, 1, 1, 1],\n",
       "       [1, 1, 1, 1, 1],\n",
       "       [1, 1, 1, 0, 1],\n",
       "       [1, 1, 1, 1, 1],\n",
       "       [1, 1, 1, 1, 1],\n",
       "       [1, 1, 1, 1, 1],\n",
       "       [1, 1, 1, 1, 1],\n",
       "       [1, 1, 1, 0, 1],\n",
       "       [1, 1, 1, 1, 1]])"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_kfold_arr = np.array(data_kfold) #Convert 'data_kfold' from a dataframe to an array.\n",
    "data_kfold_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "bc1805a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_kfold = np.array(range(59))\n",
    "for i in range(59):\n",
    "    sum_kfold = sum(data_kfold_arr[i])\n",
    "    if sum_kfold < 3:\n",
    "        predict_kfold[i] = 0\n",
    "    else:\n",
    "        predict_kfold[i] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "353ceb22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_kfold"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79354eac",
   "metadata": {},
   "source": [
    "All results are 1, so the prediction is too bad. I will try another threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "5c01db36",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_kfold = np.array(range(59))\n",
    "for i in range(59):\n",
    "    sum_kfold = sum(data_kfold_arr[i])\n",
    "    if sum_kfold <5:\n",
    "        predict_kfold[i] = 0\n",
    "    else:\n",
    "        predict_kfold[i] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "5f3b235c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1])"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_kfold"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dabc1ee",
   "metadata": {},
   "source": [
    "Once there is a 0, then we will consider this patient is a progressor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "8ed3d0e9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0],\n",
       "       [1, 0],\n",
       "       [1, 0],\n",
       "       [0, 1],\n",
       "       [1, 0],\n",
       "       [0, 1],\n",
       "       [1, 0],\n",
       "       [1, 0],\n",
       "       [0, 1],\n",
       "       [1, 0],\n",
       "       [1, 0],\n",
       "       [1, 0],\n",
       "       [1, 0],\n",
       "       [1, 0],\n",
       "       [1, 0],\n",
       "       [1, 0],\n",
       "       [1, 0],\n",
       "       [1, 0],\n",
       "       [1, 0],\n",
       "       [1, 0],\n",
       "       [1, 0],\n",
       "       [1, 0],\n",
       "       [1, 0],\n",
       "       [1, 0],\n",
       "       [1, 0],\n",
       "       [1, 0],\n",
       "       [1, 0],\n",
       "       [1, 0],\n",
       "       [1, 0],\n",
       "       [1, 0],\n",
       "       [1, 0],\n",
       "       [1, 0],\n",
       "       [1, 0],\n",
       "       [0, 1],\n",
       "       [1, 0],\n",
       "       [0, 1],\n",
       "       [1, 0],\n",
       "       [1, 0],\n",
       "       [1, 0],\n",
       "       [1, 0],\n",
       "       [1, 0],\n",
       "       [1, 0],\n",
       "       [1, 0],\n",
       "       [1, 0],\n",
       "       [1, 0],\n",
       "       [0, 1],\n",
       "       [1, 0],\n",
       "       [1, 0],\n",
       "       [1, 0],\n",
       "       [1, 0],\n",
       "       [1, 0],\n",
       "       [1, 0],\n",
       "       [0, 1],\n",
       "       [1, 0],\n",
       "       [1, 0],\n",
       "       [1, 0],\n",
       "       [1, 0],\n",
       "       [0, 1],\n",
       "       [1, 0]])"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_kfold_complement = 1 - predict_kfold\n",
    "predict_kfold_2D = np.stack((predict_kfold, predict_kfold_complement), axis=1)\n",
    "predict_kfold_2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "19f36831",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Non-Progressor</th>\n",
       "      <th>Progressor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>210</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>211</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>225</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>244</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>254</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>255</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>256</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>257</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>258</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>259</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>265</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>266</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>267</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>268</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>281</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>282</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>320</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>324</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>325</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>328</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>329</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>330</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>331</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>346</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>347</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>384</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>451</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>452</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>453</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>458</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>459</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>465</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>466</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>467</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>468</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>494</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>501</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>520</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>521</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>570</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Non-Progressor  Progressor\n",
       "28                1           0\n",
       "29                1           0\n",
       "71                0           1\n",
       "72                0           1\n",
       "73                0           1\n",
       "96                0           1\n",
       "97                1           0\n",
       "122               1           0\n",
       "123               0           1\n",
       "152               1           0\n",
       "153               1           0\n",
       "154               1           0\n",
       "155               0           1\n",
       "203               1           0\n",
       "204               1           0\n",
       "209               1           0\n",
       "210               1           0\n",
       "211               1           0\n",
       "212               0           1\n",
       "224               0           1\n",
       "225               0           1\n",
       "244               0           1\n",
       "254               0           1\n",
       "255               1           0\n",
       "256               1           0\n",
       "257               0           1\n",
       "258               1           0\n",
       "259               1           0\n",
       "265               1           0\n",
       "266               0           1\n",
       "267               1           0\n",
       "268               1           0\n",
       "281               1           0\n",
       "282               1           0\n",
       "320               1           0\n",
       "324               0           1\n",
       "325               1           0\n",
       "328               1           0\n",
       "329               1           0\n",
       "330               1           0\n",
       "331               1           0\n",
       "346               1           0\n",
       "347               1           0\n",
       "384               1           0\n",
       "451               1           0\n",
       "452               0           1\n",
       "453               0           1\n",
       "458               0           1\n",
       "459               1           0\n",
       "465               0           1\n",
       "466               0           1\n",
       "467               0           1\n",
       "468               1           0\n",
       "494               1           0\n",
       "495               1           0\n",
       "501               1           0\n",
       "520               1           0\n",
       "521               0           1\n",
       "570               1           0"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "a917dd38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(59, 2)"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "2ff61888",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Non-Progressor</th>\n",
       "      <th>Progressor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Non-Progressor  Progressor\n",
       "0                1           0\n",
       "1                1           0\n",
       "2                1           0\n",
       "3                0           1\n",
       "4                1           0\n",
       "5                0           1\n",
       "6                1           0\n",
       "7                1           0\n",
       "8                0           1\n",
       "9                1           0\n",
       "10               1           0\n",
       "11               1           0\n",
       "12               1           0\n",
       "13               1           0\n",
       "14               1           0\n",
       "15               1           0\n",
       "16               1           0\n",
       "17               1           0\n",
       "18               1           0\n",
       "19               1           0\n",
       "20               1           0\n",
       "21               1           0\n",
       "22               1           0\n",
       "23               1           0\n",
       "24               1           0\n",
       "25               1           0\n",
       "26               1           0\n",
       "27               1           0\n",
       "28               1           0\n",
       "29               1           0\n",
       "30               1           0\n",
       "31               1           0\n",
       "32               1           0\n",
       "33               0           1\n",
       "34               1           0\n",
       "35               0           1\n",
       "36               1           0\n",
       "37               1           0\n",
       "38               1           0\n",
       "39               1           0\n",
       "40               1           0\n",
       "41               1           0\n",
       "42               1           0\n",
       "43               1           0\n",
       "44               1           0\n",
       "45               0           1\n",
       "46               1           0\n",
       "47               1           0\n",
       "48               1           0\n",
       "49               1           0\n",
       "50               1           0\n",
       "51               1           0\n",
       "52               0           1\n",
       "53               1           0\n",
       "54               1           0\n",
       "55               1           0\n",
       "56               1           0\n",
       "57               0           1\n",
       "58               1           0"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_kfold_2D = pd.DataFrame(predict_kfold_2D, columns=('Non-Progressor', 'Progressor'))\n",
    "predict_kfold_2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "fff7578d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(59, 2)"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_kfold_2D.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dfd8bca",
   "metadata": {},
   "source": [
    "Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "ea4fcc84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "roc auc score:  0.6165413533834587\n",
      "average precision score:  0.585575978830426\n"
     ]
    }
   ],
   "source": [
    "roc_value = roc_auc_score(y_test, predict_kfold_2D)\n",
    "ap_score = average_precision_score(y_test, predict_kfold_2D)\n",
    "print('roc auc score: ', roc_value)\n",
    "print('average precision score: ', ap_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "872ce320",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_c = predict_kfold_2D.to_numpy()\n",
    "y_c = y_c.astype('int32')\n",
    "y_test_np = y_test.to_numpy()\n",
    "y_test_np = y_test_np.astype('int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "80d189e2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is 0.7119.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "cf_matrix = confusion_matrix(y_test_np.argmax(axis=1), y_c.argmax(axis=1)) \n",
    "#cf_matrix = confusion_matrix(y_test_np[:, 1], y_c[:, 1]) \n",
    "\n",
    "ax = sns.heatmap(cf_matrix, annot=True, cmap='Blues')\n",
    "\n",
    "ax.set_title('Seaborn Confusion Matrix with labels\\n\\n');\n",
    "ax.set_xlabel('\\nPredicted Values')\n",
    "ax.set_ylabel('Actual Values ');\n",
    "\n",
    "## Ticket labels - List must be in alphabetical order\n",
    "ax.xaxis.set_ticklabels(['Non-Progressor','Progressor'])\n",
    "ax.yaxis.set_ticklabels(['Non-Progressor','Progressor'])\n",
    "\n",
    "## Display the visualization of the Confusion Matrix.\n",
    "plt.savefig('M1_GRI_test.png')\n",
    "print(f'Accuracy is {(cf_matrix[0,0]+cf_matrix[1,1])/(sum(sum(cf_matrix)))}.')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8ce6ad2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82fa5a9e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "947f9028",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "98a0e911",
   "metadata": {},
   "source": [
    "### Idea 4: Using 'RNFLT 1 to 768' as the predictors, 'Y_combined' as the dependent variable, with resampling, k-fold validation method "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7af198ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "early_stopping_monitor = EarlyStopping(\n",
    "    monitor='val_accuracy',\n",
    "    min_delta=0,\n",
    "    patience=100,\n",
    "    verbose=0,\n",
    "    mode='auto',\n",
    "    baseline=None,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "opt1 = keras.optimizers.Adam(learning_rate = 1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f6a4bb4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define a function for getting new CNN models.\n",
    "def get_model():\n",
    "    #create model\n",
    "    model = Sequential()\n",
    "\n",
    "    #add layers\n",
    "    model.add(Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(768,1)))\n",
    "    model.add(MaxPooling1D(pool_size=3))\n",
    "    # model.add(Conv1D(filters=32, kernel_size=3, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    # model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dense(16, activation='relu'))\n",
    "    model.add(Dense(2, activation='softmax'))\n",
    "    model.compile(optimizer=opt1, \n",
    "                loss='categorical_crossentropy', \n",
    "                metrics=['accuracy'])\n",
    "    #Here we use cross-entropy as the criteria for loss.\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2a885527",
   "metadata": {},
   "outputs": [],
   "source": [
    "#k-fold\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "kfold = StratifiedKFold(n_splits=5,shuffle=True,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a47fae75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(516, 768, 1)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Join the training set and validation set together.\n",
    "X_trainval = np.concatenate((X_train, X_val), axis=0)\n",
    "X_trainval.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cc7d6f8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(516, 2)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_trainval = np.concatenate((y_train, y_val), axis = 0)\n",
    "y_trainval.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "733a7667",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0],\n",
       "       [0, 1],\n",
       "       [1, 0],\n",
       "       ...,\n",
       "       [1, 0],\n",
       "       [1, 0],\n",
       "       [0, 1]], dtype=uint8)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_trainval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "133db33a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.449438202247191"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(y_trainval[:, 1])/sum(y_trainval[:, 0])\n",
    "#since this result is less than 1, we can know that the first col is non-progressor and the second col is \n",
    "#progressor. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c9de740f",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_trainval = y_trainval[:, 1] #Only keep the progressor column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbfec587",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2acd083d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "01723b9a",
   "metadata": {},
   "source": [
    "**Testing:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "8fdd6b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_kfold = list(kfold.split(X_trainval, y_trainval))\n",
    "train_idx, val_idx = list_kfold[0]\n",
    "X_train_kfold = X_trainval[train_idx]\n",
    "y_train_kfold = y_trainval[train_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "fd15a97a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(412, 768, 1)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_kfold.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b5ff25ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(412, 768)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_kfold_2d = np.reshape(X_train_kfold, (X_train_kfold.shape[0], X_train_kfold.shape[1]))\n",
    "X_train_kfold_2d.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "767a5165",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(412,)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_kfold.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2e0797ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "412"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_kfold_arr=np.array(y_train_kfold)\n",
    "len(y_train_kfold_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e1205d4c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1,\n",
       "       1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0,\n",
       "       1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0,\n",
       "       1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0,\n",
       "       0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1,\n",
       "       0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1,\n",
       "       0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0,\n",
       "       1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1,\n",
       "       1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0,\n",
       "       0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0,\n",
       "       0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1,\n",
       "       0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0,\n",
       "       0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1], dtype=uint8)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_kfold_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ec060c45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(568, 768)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1,\n",
       "       1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0,\n",
       "       1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0,\n",
       "       1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0,\n",
       "       0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1,\n",
       "       0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1,\n",
       "       0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0,\n",
       "       1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1,\n",
       "       1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0,\n",
       "       0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0,\n",
       "       0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1,\n",
       "       0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0,\n",
       "       0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=uint8)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from imblearn.over_sampling import RandomOverSampler\n",
    "oversample = RandomOverSampler(sampling_strategy = 'minority')\n",
    "X_train_kfold_over, y_train_kfold_over = oversample.fit_resample(X_train_kfold_2d, y_train_kfold_arr)\n",
    "print(X_train_kfold_over.shape)\n",
    "y_train_kfold_over"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8dc31425",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>563</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>564</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>565</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>566</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>567</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>568 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     0  1\n",
       "0    1  0\n",
       "1    0  1\n",
       "2    1  0\n",
       "3    1  0\n",
       "4    1  0\n",
       "..  .. ..\n",
       "563  0  1\n",
       "564  0  1\n",
       "565  0  1\n",
       "566  0  1\n",
       "567  0  1\n",
       "\n",
       "[568 rows x 2 columns]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_kfold_over = pd.get_dummies(y_train_kfold_over)\n",
    "y_train_kfold_over"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "847ac3ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non-Progressor  Progressor\n",
      "0               1             284\n",
      "1               0             284\n",
      "dtype: int64 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_train_kfold_over=y_train_kfold_over.rename(columns={0: \"Non-Progressor\", 1: \"Progressor\"})\n",
    "print(y_train_kfold_over.value_counts(), '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "88aaaef5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Non-Progressor</th>\n",
       "      <th>Progressor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>563</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>564</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>565</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>566</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>567</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>568 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Non-Progressor  Progressor\n",
       "0                 1           0\n",
       "1                 0           1\n",
       "2                 1           0\n",
       "3                 1           0\n",
       "4                 1           0\n",
       "..              ...         ...\n",
       "563               0           1\n",
       "564               0           1\n",
       "565               0           1\n",
       "566               0           1\n",
       "567               0           1\n",
       "\n",
       "[568 rows x 2 columns]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_kfold_over"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "20c33940",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(568, 768, 1)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_kfold_over = np.reshape(X_train_kfold_over, (X_train_kfold_over.shape[0], X_train_kfold_over.shape[1], 1))\n",
    "X_train_kfold_over.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01a08522",
   "metadata": {},
   "source": [
    "**Testing End**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "674fa8f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf3e2826",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "28d5abab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define a method for getting the resampled data set from the original data set based on the above test.\n",
    "def get_resampling(X_train_kfold, y_train_kfold):\n",
    "    X_train_kfold_2d = np.reshape(X_train_kfold, (X_train_kfold.shape[0], X_train_kfold.shape[1]))\n",
    "    y_train_kfold_arr=np.array(y_train_kfold)\n",
    "    from imblearn.over_sampling import RandomOverSampler\n",
    "    oversample = RandomOverSampler(sampling_strategy = 'minority')\n",
    "    X_train_kfold_over, y_train_kfold_over = oversample.fit_resample(X_train_kfold_2d, y_train_kfold_arr)\n",
    "    y_train_kfold_over = pd.get_dummies(y_train_kfold_over)#Convert the array to a data frame.\n",
    "    y_train_kfold_over = y_train_kfold_over.rename(columns={0: \"Non-Progressor\", 1: \"Progressor\"})#Rename it.\n",
    "    X_train_kfold_over = np.reshape(X_train_kfold_over, (X_train_kfold_over.shape[0], X_train_kfold_over.shape[1], 1))\n",
    "    #Reshape the X_train_kfold_over in order to apply CNN model on it.\n",
    "    return (X_train_kfold_over, y_train_kfold_over)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "812d2c0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "18/18 [==============================] - 1s 35ms/step - loss: 0.6932 - accuracy: 0.4982 - val_loss: 0.6729 - val_accuracy: 0.6731\n",
      "Epoch 2/500\n",
      "18/18 [==============================] - 0s 25ms/step - loss: 0.6797 - accuracy: 0.5687 - val_loss: 0.6295 - val_accuracy: 0.6827\n",
      "Epoch 3/500\n",
      "18/18 [==============================] - 1s 30ms/step - loss: 0.6660 - accuracy: 0.6056 - val_loss: 0.6623 - val_accuracy: 0.6346\n",
      "Epoch 4/500\n",
      "18/18 [==============================] - 1s 32ms/step - loss: 0.6464 - accuracy: 0.6338 - val_loss: 0.6692 - val_accuracy: 0.5673\n",
      "Epoch 5/500\n",
      "18/18 [==============================] - 0s 27ms/step - loss: 0.6317 - accuracy: 0.6673 - val_loss: 0.7046 - val_accuracy: 0.5577\n",
      "Epoch 6/500\n",
      "18/18 [==============================] - 0s 27ms/step - loss: 0.6259 - accuracy: 0.6813 - val_loss: 0.6660 - val_accuracy: 0.5769\n",
      "Epoch 7/500\n",
      "18/18 [==============================] - 1s 31ms/step - loss: 0.6138 - accuracy: 0.6866 - val_loss: 0.6505 - val_accuracy: 0.6346\n",
      "Epoch 8/500\n",
      "18/18 [==============================] - 0s 25ms/step - loss: 0.6136 - accuracy: 0.6761 - val_loss: 0.6714 - val_accuracy: 0.5577\n",
      "Epoch 9/500\n",
      "18/18 [==============================] - 0s 27ms/step - loss: 0.6038 - accuracy: 0.6972 - val_loss: 0.6740 - val_accuracy: 0.5577\n",
      "Epoch 10/500\n",
      "18/18 [==============================] - 0s 26ms/step - loss: 0.6014 - accuracy: 0.6919 - val_loss: 0.6707 - val_accuracy: 0.5769\n",
      "Epoch 11/500\n",
      "18/18 [==============================] - 0s 24ms/step - loss: 0.5985 - accuracy: 0.6831 - val_loss: 0.6769 - val_accuracy: 0.5673\n",
      "Epoch 12/500\n",
      "18/18 [==============================] - 0s 26ms/step - loss: 0.5940 - accuracy: 0.6919 - val_loss: 0.6980 - val_accuracy: 0.5288\n",
      "Epoch 13/500\n",
      "18/18 [==============================] - 0s 27ms/step - loss: 0.5930 - accuracy: 0.6849 - val_loss: 0.6893 - val_accuracy: 0.5481\n",
      "Epoch 14/500\n",
      "18/18 [==============================] - 0s 22ms/step - loss: 0.5865 - accuracy: 0.6796 - val_loss: 0.6587 - val_accuracy: 0.6346\n",
      "Epoch 15/500\n",
      "18/18 [==============================] - 0s 23ms/step - loss: 0.6013 - accuracy: 0.6690 - val_loss: 0.6691 - val_accuracy: 0.5865\n",
      "Epoch 16/500\n",
      "18/18 [==============================] - 0s 23ms/step - loss: 0.5828 - accuracy: 0.6989 - val_loss: 0.7098 - val_accuracy: 0.5192\n",
      "Epoch 17/500\n",
      "18/18 [==============================] - 0s 23ms/step - loss: 0.5771 - accuracy: 0.6989 - val_loss: 0.6792 - val_accuracy: 0.5577\n",
      "Epoch 18/500\n",
      "18/18 [==============================] - 1s 30ms/step - loss: 0.5774 - accuracy: 0.7025 - val_loss: 0.6922 - val_accuracy: 0.5385\n",
      "Epoch 19/500\n",
      "18/18 [==============================] - 0s 25ms/step - loss: 0.5735 - accuracy: 0.6989 - val_loss: 0.6697 - val_accuracy: 0.6346\n",
      "Epoch 20/500\n",
      "18/18 [==============================] - 0s 26ms/step - loss: 0.5712 - accuracy: 0.7007 - val_loss: 0.6977 - val_accuracy: 0.5577\n",
      "Epoch 21/500\n",
      "18/18 [==============================] - 0s 27ms/step - loss: 0.5683 - accuracy: 0.7113 - val_loss: 0.7036 - val_accuracy: 0.5481\n",
      "Epoch 22/500\n",
      "18/18 [==============================] - 0s 28ms/step - loss: 0.5615 - accuracy: 0.7060 - val_loss: 0.6975 - val_accuracy: 0.5577\n",
      "Epoch 23/500\n",
      "18/18 [==============================] - 1s 28ms/step - loss: 0.5613 - accuracy: 0.7201 - val_loss: 0.7179 - val_accuracy: 0.5000\n",
      "Epoch 24/500\n",
      "18/18 [==============================] - 0s 27ms/step - loss: 0.5557 - accuracy: 0.7148 - val_loss: 0.6983 - val_accuracy: 0.5577\n",
      "Epoch 25/500\n",
      "18/18 [==============================] - 0s 28ms/step - loss: 0.5558 - accuracy: 0.7165 - val_loss: 0.7068 - val_accuracy: 0.5385\n",
      "Epoch 26/500\n",
      "18/18 [==============================] - 0s 25ms/step - loss: 0.5520 - accuracy: 0.7183 - val_loss: 0.7112 - val_accuracy: 0.5385\n",
      "Epoch 27/500\n",
      "18/18 [==============================] - 0s 27ms/step - loss: 0.5455 - accuracy: 0.7236 - val_loss: 0.7026 - val_accuracy: 0.5481\n",
      "Epoch 28/500\n",
      "18/18 [==============================] - 0s 25ms/step - loss: 0.5499 - accuracy: 0.7165 - val_loss: 0.7130 - val_accuracy: 0.5385\n",
      "Epoch 29/500\n",
      "18/18 [==============================] - 0s 28ms/step - loss: 0.5445 - accuracy: 0.7113 - val_loss: 0.7062 - val_accuracy: 0.5481\n",
      "Epoch 30/500\n",
      "18/18 [==============================] - 1s 29ms/step - loss: 0.5425 - accuracy: 0.7324 - val_loss: 0.7314 - val_accuracy: 0.5192\n",
      "Epoch 31/500\n",
      "18/18 [==============================] - 0s 26ms/step - loss: 0.5421 - accuracy: 0.7306 - val_loss: 0.7001 - val_accuracy: 0.5288\n",
      "Epoch 32/500\n",
      "18/18 [==============================] - 1s 30ms/step - loss: 0.5323 - accuracy: 0.7394 - val_loss: 0.7514 - val_accuracy: 0.5096\n",
      "Epoch 33/500\n",
      "18/18 [==============================] - 0s 26ms/step - loss: 0.5379 - accuracy: 0.7324 - val_loss: 0.7055 - val_accuracy: 0.5192\n",
      "Epoch 34/500\n",
      "18/18 [==============================] - 0s 28ms/step - loss: 0.5308 - accuracy: 0.7447 - val_loss: 0.7353 - val_accuracy: 0.5096\n",
      "Epoch 35/500\n",
      "18/18 [==============================] - 1s 29ms/step - loss: 0.5239 - accuracy: 0.7465 - val_loss: 0.7172 - val_accuracy: 0.5288\n",
      "Epoch 36/500\n",
      "18/18 [==============================] - 0s 26ms/step - loss: 0.5257 - accuracy: 0.7342 - val_loss: 0.7105 - val_accuracy: 0.5288\n",
      "Epoch 37/500\n",
      "18/18 [==============================] - 1s 28ms/step - loss: 0.5176 - accuracy: 0.7518 - val_loss: 0.7362 - val_accuracy: 0.5288\n",
      "Epoch 38/500\n",
      "18/18 [==============================] - 1s 29ms/step - loss: 0.5195 - accuracy: 0.7606 - val_loss: 0.7094 - val_accuracy: 0.5385\n",
      "Epoch 39/500\n",
      "18/18 [==============================] - 0s 27ms/step - loss: 0.5110 - accuracy: 0.7447 - val_loss: 0.7400 - val_accuracy: 0.5288\n",
      "Epoch 40/500\n",
      "18/18 [==============================] - 0s 27ms/step - loss: 0.5171 - accuracy: 0.7588 - val_loss: 0.7218 - val_accuracy: 0.5481\n",
      "Epoch 41/500\n",
      "18/18 [==============================] - 0s 24ms/step - loss: 0.5052 - accuracy: 0.7694 - val_loss: 0.7582 - val_accuracy: 0.5096\n",
      "Epoch 42/500\n",
      "18/18 [==============================] - 0s 22ms/step - loss: 0.5019 - accuracy: 0.7694 - val_loss: 0.7064 - val_accuracy: 0.5577\n",
      "Epoch 43/500\n",
      "18/18 [==============================] - 0s 21ms/step - loss: 0.5002 - accuracy: 0.7694 - val_loss: 0.7742 - val_accuracy: 0.5288\n",
      "Epoch 44/500\n",
      "18/18 [==============================] - 0s 23ms/step - loss: 0.4974 - accuracy: 0.7694 - val_loss: 0.7188 - val_accuracy: 0.5481\n",
      "Epoch 45/500\n",
      "18/18 [==============================] - 0s 20ms/step - loss: 0.4971 - accuracy: 0.7606 - val_loss: 0.7601 - val_accuracy: 0.5577\n",
      "Epoch 46/500\n",
      "18/18 [==============================] - 0s 21ms/step - loss: 0.4924 - accuracy: 0.7711 - val_loss: 0.7324 - val_accuracy: 0.5577\n",
      "Epoch 47/500\n",
      "18/18 [==============================] - 0s 24ms/step - loss: 0.4935 - accuracy: 0.7852 - val_loss: 0.7656 - val_accuracy: 0.5385\n",
      "Epoch 48/500\n",
      "18/18 [==============================] - 0s 24ms/step - loss: 0.4985 - accuracy: 0.7588 - val_loss: 0.7545 - val_accuracy: 0.5673\n",
      "Epoch 49/500\n",
      "18/18 [==============================] - 0s 21ms/step - loss: 0.4866 - accuracy: 0.7799 - val_loss: 0.7386 - val_accuracy: 0.5769\n",
      "Epoch 50/500\n",
      "18/18 [==============================] - 0s 22ms/step - loss: 0.4890 - accuracy: 0.7676 - val_loss: 0.7377 - val_accuracy: 0.5865\n",
      "Epoch 51/500\n",
      "18/18 [==============================] - 0s 23ms/step - loss: 0.4802 - accuracy: 0.7782 - val_loss: 0.7695 - val_accuracy: 0.5673\n",
      "Epoch 52/500\n",
      "18/18 [==============================] - 0s 25ms/step - loss: 0.4756 - accuracy: 0.7835 - val_loss: 0.7100 - val_accuracy: 0.5577\n",
      "Epoch 53/500\n",
      "18/18 [==============================] - 0s 23ms/step - loss: 0.4865 - accuracy: 0.7694 - val_loss: 0.7881 - val_accuracy: 0.5577\n",
      "Epoch 54/500\n",
      "18/18 [==============================] - 0s 22ms/step - loss: 0.4712 - accuracy: 0.7923 - val_loss: 0.7404 - val_accuracy: 0.5673\n",
      "Epoch 55/500\n",
      "18/18 [==============================] - 0s 25ms/step - loss: 0.4713 - accuracy: 0.7923 - val_loss: 0.7654 - val_accuracy: 0.5673\n",
      "Epoch 56/500\n",
      "18/18 [==============================] - 0s 23ms/step - loss: 0.4686 - accuracy: 0.7940 - val_loss: 0.7308 - val_accuracy: 0.5577\n",
      "Epoch 57/500\n",
      "18/18 [==============================] - 0s 23ms/step - loss: 0.4697 - accuracy: 0.7852 - val_loss: 0.7596 - val_accuracy: 0.5769\n",
      "Epoch 58/500\n",
      "18/18 [==============================] - 0s 21ms/step - loss: 0.4712 - accuracy: 0.7887 - val_loss: 0.7650 - val_accuracy: 0.5769\n",
      "Epoch 59/500\n",
      "18/18 [==============================] - 0s 23ms/step - loss: 0.4546 - accuracy: 0.7958 - val_loss: 0.7597 - val_accuracy: 0.5673\n",
      "Epoch 60/500\n",
      "18/18 [==============================] - 0s 25ms/step - loss: 0.4572 - accuracy: 0.7975 - val_loss: 0.7647 - val_accuracy: 0.5769\n",
      "Epoch 61/500\n",
      "18/18 [==============================] - 0s 24ms/step - loss: 0.4601 - accuracy: 0.7923 - val_loss: 0.7729 - val_accuracy: 0.5962\n",
      "Epoch 62/500\n",
      "18/18 [==============================] - 0s 22ms/step - loss: 0.4455 - accuracy: 0.8134 - val_loss: 0.7517 - val_accuracy: 0.5769\n",
      "Epoch 63/500\n",
      "18/18 [==============================] - 0s 24ms/step - loss: 0.4428 - accuracy: 0.8187 - val_loss: 0.7640 - val_accuracy: 0.5673\n",
      "Epoch 64/500\n",
      "18/18 [==============================] - 0s 23ms/step - loss: 0.4489 - accuracy: 0.8028 - val_loss: 0.7803 - val_accuracy: 0.5865\n",
      "Epoch 65/500\n",
      "18/18 [==============================] - 0s 24ms/step - loss: 0.4427 - accuracy: 0.8081 - val_loss: 0.7506 - val_accuracy: 0.5577\n",
      "Epoch 66/500\n",
      "18/18 [==============================] - 0s 24ms/step - loss: 0.4494 - accuracy: 0.7940 - val_loss: 0.7423 - val_accuracy: 0.5769\n",
      "Epoch 67/500\n",
      "18/18 [==============================] - 0s 23ms/step - loss: 0.4559 - accuracy: 0.8063 - val_loss: 0.7859 - val_accuracy: 0.5865\n",
      "Epoch 68/500\n",
      "18/18 [==============================] - 0s 24ms/step - loss: 0.4385 - accuracy: 0.8151 - val_loss: 0.7572 - val_accuracy: 0.5577\n",
      "Epoch 69/500\n",
      "18/18 [==============================] - 0s 24ms/step - loss: 0.4326 - accuracy: 0.8081 - val_loss: 0.7810 - val_accuracy: 0.5673\n",
      "Epoch 70/500\n",
      "18/18 [==============================] - 0s 26ms/step - loss: 0.4321 - accuracy: 0.8151 - val_loss: 0.7535 - val_accuracy: 0.5769\n",
      "Epoch 71/500\n",
      "18/18 [==============================] - 0s 27ms/step - loss: 0.4271 - accuracy: 0.8239 - val_loss: 0.7766 - val_accuracy: 0.5673\n",
      "Epoch 72/500\n",
      "18/18 [==============================] - 0s 27ms/step - loss: 0.4315 - accuracy: 0.8239 - val_loss: 0.7768 - val_accuracy: 0.5577\n",
      "Epoch 73/500\n",
      "18/18 [==============================] - 0s 26ms/step - loss: 0.4288 - accuracy: 0.8063 - val_loss: 0.7841 - val_accuracy: 0.5673\n",
      "Epoch 74/500\n",
      "18/18 [==============================] - 0s 24ms/step - loss: 0.4246 - accuracy: 0.8239 - val_loss: 0.7725 - val_accuracy: 0.5673\n",
      "Epoch 75/500\n",
      "18/18 [==============================] - 0s 21ms/step - loss: 0.4265 - accuracy: 0.8063 - val_loss: 0.7816 - val_accuracy: 0.5673\n",
      "Epoch 76/500\n",
      "18/18 [==============================] - 0s 25ms/step - loss: 0.4272 - accuracy: 0.8239 - val_loss: 0.8135 - val_accuracy: 0.5962\n",
      "Epoch 77/500\n",
      "18/18 [==============================] - 0s 25ms/step - loss: 0.4200 - accuracy: 0.8116 - val_loss: 0.7316 - val_accuracy: 0.5577\n",
      "Epoch 78/500\n",
      "18/18 [==============================] - 0s 25ms/step - loss: 0.4344 - accuracy: 0.8169 - val_loss: 0.7923 - val_accuracy: 0.5865\n",
      "Epoch 79/500\n",
      "18/18 [==============================] - 0s 23ms/step - loss: 0.4204 - accuracy: 0.8204 - val_loss: 0.7969 - val_accuracy: 0.5962\n",
      "Epoch 80/500\n",
      "18/18 [==============================] - 0s 25ms/step - loss: 0.4086 - accuracy: 0.8363 - val_loss: 0.7799 - val_accuracy: 0.5673\n",
      "Epoch 81/500\n",
      "18/18 [==============================] - 0s 23ms/step - loss: 0.4243 - accuracy: 0.8046 - val_loss: 0.7610 - val_accuracy: 0.5769\n",
      "Epoch 82/500\n",
      "18/18 [==============================] - 0s 25ms/step - loss: 0.3957 - accuracy: 0.8363 - val_loss: 0.8124 - val_accuracy: 0.5865\n",
      "Epoch 83/500\n",
      "18/18 [==============================] - 0s 24ms/step - loss: 0.4167 - accuracy: 0.8239 - val_loss: 0.7805 - val_accuracy: 0.5865\n",
      "Epoch 84/500\n",
      "18/18 [==============================] - 0s 26ms/step - loss: 0.4017 - accuracy: 0.8327 - val_loss: 0.7936 - val_accuracy: 0.5673\n",
      "Epoch 85/500\n",
      "18/18 [==============================] - 0s 21ms/step - loss: 0.3954 - accuracy: 0.8292 - val_loss: 0.7994 - val_accuracy: 0.5673\n",
      "Epoch 86/500\n",
      "18/18 [==============================] - 0s 23ms/step - loss: 0.4028 - accuracy: 0.8204 - val_loss: 0.8514 - val_accuracy: 0.5577\n",
      "Epoch 87/500\n",
      "18/18 [==============================] - 0s 25ms/step - loss: 0.3998 - accuracy: 0.8239 - val_loss: 0.7793 - val_accuracy: 0.5769\n",
      "Epoch 88/500\n",
      "18/18 [==============================] - 0s 26ms/step - loss: 0.3952 - accuracy: 0.8292 - val_loss: 0.8087 - val_accuracy: 0.5673\n",
      "Epoch 89/500\n",
      "18/18 [==============================] - 0s 25ms/step - loss: 0.3938 - accuracy: 0.8310 - val_loss: 0.7994 - val_accuracy: 0.5865\n",
      "Epoch 90/500\n",
      "18/18 [==============================] - 0s 26ms/step - loss: 0.3946 - accuracy: 0.8151 - val_loss: 0.7898 - val_accuracy: 0.5769\n",
      "Epoch 91/500\n",
      "18/18 [==============================] - 0s 24ms/step - loss: 0.3878 - accuracy: 0.8398 - val_loss: 0.8068 - val_accuracy: 0.5673\n",
      "Epoch 92/500\n",
      "18/18 [==============================] - 0s 25ms/step - loss: 0.3851 - accuracy: 0.8504 - val_loss: 0.8036 - val_accuracy: 0.5769\n",
      "Epoch 93/500\n",
      "18/18 [==============================] - 0s 27ms/step - loss: 0.3817 - accuracy: 0.8451 - val_loss: 0.7901 - val_accuracy: 0.5865\n",
      "Epoch 94/500\n",
      "18/18 [==============================] - 0s 27ms/step - loss: 0.3890 - accuracy: 0.8327 - val_loss: 0.7911 - val_accuracy: 0.5962\n",
      "Epoch 95/500\n",
      "18/18 [==============================] - 0s 26ms/step - loss: 0.3826 - accuracy: 0.8327 - val_loss: 0.8265 - val_accuracy: 0.5769\n",
      "Epoch 96/500\n",
      "18/18 [==============================] - 0s 26ms/step - loss: 0.3819 - accuracy: 0.8398 - val_loss: 0.7875 - val_accuracy: 0.5865\n",
      "Epoch 97/500\n",
      "18/18 [==============================] - 0s 24ms/step - loss: 0.3794 - accuracy: 0.8433 - val_loss: 0.8016 - val_accuracy: 0.6058\n",
      "Epoch 98/500\n",
      "18/18 [==============================] - 0s 23ms/step - loss: 0.3675 - accuracy: 0.8539 - val_loss: 0.8231 - val_accuracy: 0.5673\n",
      "Epoch 99/500\n",
      "18/18 [==============================] - 0s 26ms/step - loss: 0.3731 - accuracy: 0.8468 - val_loss: 0.8062 - val_accuracy: 0.5962\n",
      "Epoch 100/500\n",
      "18/18 [==============================] - 0s 27ms/step - loss: 0.3747 - accuracy: 0.8486 - val_loss: 0.8560 - val_accuracy: 0.5673\n",
      "Epoch 101/500\n",
      "18/18 [==============================] - 0s 24ms/step - loss: 0.3684 - accuracy: 0.8486 - val_loss: 0.7939 - val_accuracy: 0.5865\n",
      "Epoch 102/500\n",
      "18/18 [==============================] - 1s 28ms/step - loss: 0.3727 - accuracy: 0.8415 - val_loss: 0.8286 - val_accuracy: 0.5865\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "Epoch 1/500\n",
      "18/18 [==============================] - 1s 34ms/step - loss: 0.6941 - accuracy: 0.5018 - val_loss: 0.6843 - val_accuracy: 0.6893\n",
      "Epoch 2/500\n",
      "18/18 [==============================] - 0s 24ms/step - loss: 0.6899 - accuracy: 0.5018 - val_loss: 0.6732 - val_accuracy: 0.6796\n",
      "Epoch 3/500\n",
      "18/18 [==============================] - 0s 26ms/step - loss: 0.6851 - accuracy: 0.5737 - val_loss: 0.6580 - val_accuracy: 0.6699\n",
      "Epoch 4/500\n",
      "18/18 [==============================] - 0s 25ms/step - loss: 0.6834 - accuracy: 0.5614 - val_loss: 0.6807 - val_accuracy: 0.5728\n",
      "Epoch 5/500\n",
      "18/18 [==============================] - 0s 26ms/step - loss: 0.6775 - accuracy: 0.5860 - val_loss: 0.6528 - val_accuracy: 0.6505\n",
      "Epoch 6/500\n",
      "18/18 [==============================] - 0s 24ms/step - loss: 0.6738 - accuracy: 0.6088 - val_loss: 0.6615 - val_accuracy: 0.5922\n",
      "Epoch 7/500\n",
      "18/18 [==============================] - 0s 27ms/step - loss: 0.6679 - accuracy: 0.6211 - val_loss: 0.6537 - val_accuracy: 0.5728\n",
      "Epoch 8/500\n",
      "18/18 [==============================] - 0s 23ms/step - loss: 0.6619 - accuracy: 0.6439 - val_loss: 0.6555 - val_accuracy: 0.5922\n",
      "Epoch 9/500\n",
      "18/18 [==============================] - 0s 24ms/step - loss: 0.6565 - accuracy: 0.6368 - val_loss: 0.6546 - val_accuracy: 0.5922\n",
      "Epoch 10/500\n",
      "18/18 [==============================] - 0s 24ms/step - loss: 0.6485 - accuracy: 0.6509 - val_loss: 0.6708 - val_accuracy: 0.5922\n",
      "Epoch 11/500\n",
      "18/18 [==============================] - 0s 27ms/step - loss: 0.6427 - accuracy: 0.6614 - val_loss: 0.6782 - val_accuracy: 0.5825\n",
      "Epoch 12/500\n",
      "18/18 [==============================] - 0s 25ms/step - loss: 0.6357 - accuracy: 0.6737 - val_loss: 0.6738 - val_accuracy: 0.6019\n",
      "Epoch 13/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18/18 [==============================] - 0s 24ms/step - loss: 0.6327 - accuracy: 0.6737 - val_loss: 0.6866 - val_accuracy: 0.5728\n",
      "Epoch 14/500\n",
      "18/18 [==============================] - 0s 24ms/step - loss: 0.6308 - accuracy: 0.6596 - val_loss: 0.7446 - val_accuracy: 0.4951\n",
      "Epoch 15/500\n",
      "18/18 [==============================] - 0s 24ms/step - loss: 0.6287 - accuracy: 0.6719 - val_loss: 0.6669 - val_accuracy: 0.6019\n",
      "Epoch 16/500\n",
      "18/18 [==============================] - 0s 26ms/step - loss: 0.6224 - accuracy: 0.6632 - val_loss: 0.7233 - val_accuracy: 0.5243\n",
      "Epoch 17/500\n",
      "18/18 [==============================] - 0s 27ms/step - loss: 0.6203 - accuracy: 0.6895 - val_loss: 0.6954 - val_accuracy: 0.5728\n",
      "Epoch 18/500\n",
      "18/18 [==============================] - 0s 25ms/step - loss: 0.6158 - accuracy: 0.6789 - val_loss: 0.7183 - val_accuracy: 0.5534\n",
      "Epoch 19/500\n",
      "18/18 [==============================] - 1s 29ms/step - loss: 0.6187 - accuracy: 0.6772 - val_loss: 0.6716 - val_accuracy: 0.5922\n",
      "Epoch 20/500\n",
      "18/18 [==============================] - 1s 28ms/step - loss: 0.6163 - accuracy: 0.6667 - val_loss: 0.7077 - val_accuracy: 0.5534\n",
      "Epoch 21/500\n",
      "18/18 [==============================] - 0s 23ms/step - loss: 0.6076 - accuracy: 0.6789 - val_loss: 0.6890 - val_accuracy: 0.6019\n",
      "Epoch 22/500\n",
      "18/18 [==============================] - 0s 25ms/step - loss: 0.6043 - accuracy: 0.6860 - val_loss: 0.7062 - val_accuracy: 0.5631\n",
      "Epoch 23/500\n",
      "18/18 [==============================] - 1s 29ms/step - loss: 0.6062 - accuracy: 0.6895 - val_loss: 0.6955 - val_accuracy: 0.5825\n",
      "Epoch 24/500\n",
      "18/18 [==============================] - 0s 24ms/step - loss: 0.5983 - accuracy: 0.6965 - val_loss: 0.7130 - val_accuracy: 0.5631\n",
      "Epoch 25/500\n",
      "18/18 [==============================] - 0s 27ms/step - loss: 0.5949 - accuracy: 0.6860 - val_loss: 0.7166 - val_accuracy: 0.5534\n",
      "Epoch 26/500\n",
      "18/18 [==============================] - 0s 27ms/step - loss: 0.5955 - accuracy: 0.6842 - val_loss: 0.7153 - val_accuracy: 0.5631\n",
      "Epoch 27/500\n",
      "18/18 [==============================] - 1s 29ms/step - loss: 0.5891 - accuracy: 0.6895 - val_loss: 0.7118 - val_accuracy: 0.6019\n",
      "Epoch 28/500\n",
      "18/18 [==============================] - 0s 25ms/step - loss: 0.5931 - accuracy: 0.6912 - val_loss: 0.7236 - val_accuracy: 0.5728\n",
      "Epoch 29/500\n",
      "18/18 [==============================] - 0s 26ms/step - loss: 0.5905 - accuracy: 0.6947 - val_loss: 0.7178 - val_accuracy: 0.5728\n",
      "Epoch 30/500\n",
      "18/18 [==============================] - 1s 31ms/step - loss: 0.5847 - accuracy: 0.7035 - val_loss: 0.7061 - val_accuracy: 0.5922\n",
      "Epoch 31/500\n",
      "18/18 [==============================] - 1s 29ms/step - loss: 0.5844 - accuracy: 0.6947 - val_loss: 0.7326 - val_accuracy: 0.5728\n",
      "Epoch 32/500\n",
      "18/18 [==============================] - 0s 27ms/step - loss: 0.5835 - accuracy: 0.6912 - val_loss: 0.7284 - val_accuracy: 0.5922\n",
      "Epoch 33/500\n",
      "18/18 [==============================] - 1s 29ms/step - loss: 0.5773 - accuracy: 0.7088 - val_loss: 0.7215 - val_accuracy: 0.5922\n",
      "Epoch 34/500\n",
      "18/18 [==============================] - 1s 31ms/step - loss: 0.5758 - accuracy: 0.7158 - val_loss: 0.7282 - val_accuracy: 0.6019\n",
      "Epoch 35/500\n",
      "18/18 [==============================] - 1s 31ms/step - loss: 0.5730 - accuracy: 0.7070 - val_loss: 0.7309 - val_accuracy: 0.5922\n",
      "Epoch 36/500\n",
      "18/18 [==============================] - 1s 28ms/step - loss: 0.5687 - accuracy: 0.7175 - val_loss: 0.7253 - val_accuracy: 0.5922\n",
      "Epoch 37/500\n",
      "18/18 [==============================] - 0s 28ms/step - loss: 0.5686 - accuracy: 0.7105 - val_loss: 0.7369 - val_accuracy: 0.5922\n",
      "Epoch 38/500\n",
      "18/18 [==============================] - 0s 26ms/step - loss: 0.5680 - accuracy: 0.7088 - val_loss: 0.7338 - val_accuracy: 0.5825\n",
      "Epoch 39/500\n",
      "18/18 [==============================] - 0s 24ms/step - loss: 0.5662 - accuracy: 0.7211 - val_loss: 0.7434 - val_accuracy: 0.5631\n",
      "Epoch 40/500\n",
      "18/18 [==============================] - 0s 26ms/step - loss: 0.5567 - accuracy: 0.7158 - val_loss: 0.7306 - val_accuracy: 0.6019\n",
      "Epoch 41/500\n",
      "18/18 [==============================] - 0s 26ms/step - loss: 0.5622 - accuracy: 0.7105 - val_loss: 0.7342 - val_accuracy: 0.5825\n",
      "Epoch 42/500\n",
      "18/18 [==============================] - 0s 25ms/step - loss: 0.5587 - accuracy: 0.7123 - val_loss: 0.7437 - val_accuracy: 0.5825\n",
      "Epoch 43/500\n",
      "18/18 [==============================] - 0s 27ms/step - loss: 0.5604 - accuracy: 0.7193 - val_loss: 0.7318 - val_accuracy: 0.5922\n",
      "Epoch 44/500\n",
      "18/18 [==============================] - 0s 27ms/step - loss: 0.5570 - accuracy: 0.7193 - val_loss: 0.7491 - val_accuracy: 0.5728\n",
      "Epoch 45/500\n",
      "18/18 [==============================] - 0s 28ms/step - loss: 0.5521 - accuracy: 0.7228 - val_loss: 0.7385 - val_accuracy: 0.5825\n",
      "Epoch 46/500\n",
      "18/18 [==============================] - 0s 26ms/step - loss: 0.5483 - accuracy: 0.7298 - val_loss: 0.7568 - val_accuracy: 0.5534\n",
      "Epoch 47/500\n",
      "18/18 [==============================] - 1s 29ms/step - loss: 0.5509 - accuracy: 0.7246 - val_loss: 0.7559 - val_accuracy: 0.5534\n",
      "Epoch 48/500\n",
      "18/18 [==============================] - 0s 26ms/step - loss: 0.5453 - accuracy: 0.7316 - val_loss: 0.7516 - val_accuracy: 0.5631\n",
      "Epoch 49/500\n",
      "18/18 [==============================] - 0s 25ms/step - loss: 0.5433 - accuracy: 0.7228 - val_loss: 0.7538 - val_accuracy: 0.5534\n",
      "Epoch 50/500\n",
      "18/18 [==============================] - 0s 26ms/step - loss: 0.5333 - accuracy: 0.7316 - val_loss: 0.7421 - val_accuracy: 0.5825\n",
      "Epoch 51/500\n",
      "18/18 [==============================] - 0s 26ms/step - loss: 0.5408 - accuracy: 0.7228 - val_loss: 0.7533 - val_accuracy: 0.5534\n",
      "Epoch 52/500\n",
      "18/18 [==============================] - 0s 26ms/step - loss: 0.5386 - accuracy: 0.7316 - val_loss: 0.7517 - val_accuracy: 0.5825\n",
      "Epoch 53/500\n",
      "18/18 [==============================] - 0s 25ms/step - loss: 0.5276 - accuracy: 0.7298 - val_loss: 0.7605 - val_accuracy: 0.5534\n",
      "Epoch 54/500\n",
      "18/18 [==============================] - 0s 27ms/step - loss: 0.5274 - accuracy: 0.7351 - val_loss: 0.7708 - val_accuracy: 0.5534\n",
      "Epoch 55/500\n",
      "18/18 [==============================] - 0s 26ms/step - loss: 0.5287 - accuracy: 0.7333 - val_loss: 0.7655 - val_accuracy: 0.5534\n",
      "Epoch 56/500\n",
      "18/18 [==============================] - 0s 28ms/step - loss: 0.5235 - accuracy: 0.7596 - val_loss: 0.7649 - val_accuracy: 0.5534\n",
      "Epoch 57/500\n",
      "18/18 [==============================] - 0s 22ms/step - loss: 0.5206 - accuracy: 0.7509 - val_loss: 0.7651 - val_accuracy: 0.5631\n",
      "Epoch 58/500\n",
      "18/18 [==============================] - 0s 27ms/step - loss: 0.5231 - accuracy: 0.7333 - val_loss: 0.7772 - val_accuracy: 0.5534\n",
      "Epoch 59/500\n",
      "18/18 [==============================] - 0s 26ms/step - loss: 0.5168 - accuracy: 0.7491 - val_loss: 0.7637 - val_accuracy: 0.5631\n",
      "Epoch 60/500\n",
      "18/18 [==============================] - 0s 28ms/step - loss: 0.5143 - accuracy: 0.7474 - val_loss: 0.7921 - val_accuracy: 0.5146\n",
      "Epoch 61/500\n",
      "18/18 [==============================] - 1s 28ms/step - loss: 0.5096 - accuracy: 0.7491 - val_loss: 0.7749 - val_accuracy: 0.5631\n",
      "Epoch 62/500\n",
      "18/18 [==============================] - 0s 28ms/step - loss: 0.5129 - accuracy: 0.7386 - val_loss: 0.7749 - val_accuracy: 0.5534\n",
      "Epoch 63/500\n",
      "18/18 [==============================] - 0s 23ms/step - loss: 0.5039 - accuracy: 0.7596 - val_loss: 0.7825 - val_accuracy: 0.5631\n",
      "Epoch 64/500\n",
      "18/18 [==============================] - 0s 21ms/step - loss: 0.5033 - accuracy: 0.7526 - val_loss: 0.7892 - val_accuracy: 0.5340\n",
      "Epoch 65/500\n",
      "18/18 [==============================] - 0s 23ms/step - loss: 0.5061 - accuracy: 0.7544 - val_loss: 0.7940 - val_accuracy: 0.5146\n",
      "Epoch 66/500\n",
      "18/18 [==============================] - 0s 21ms/step - loss: 0.5012 - accuracy: 0.7474 - val_loss: 0.7837 - val_accuracy: 0.5631\n",
      "Epoch 67/500\n",
      "18/18 [==============================] - 0s 26ms/step - loss: 0.4911 - accuracy: 0.7579 - val_loss: 0.7971 - val_accuracy: 0.5340\n",
      "Epoch 68/500\n",
      "18/18 [==============================] - 1s 29ms/step - loss: 0.4927 - accuracy: 0.7614 - val_loss: 0.8112 - val_accuracy: 0.5340\n",
      "Epoch 69/500\n",
      "18/18 [==============================] - 0s 28ms/step - loss: 0.4945 - accuracy: 0.7649 - val_loss: 0.8087 - val_accuracy: 0.5243\n",
      "Epoch 70/500\n",
      "18/18 [==============================] - 1s 30ms/step - loss: 0.4946 - accuracy: 0.7561 - val_loss: 0.7964 - val_accuracy: 0.5631\n",
      "Epoch 71/500\n",
      "18/18 [==============================] - 0s 25ms/step - loss: 0.4875 - accuracy: 0.7825 - val_loss: 0.8286 - val_accuracy: 0.4854\n",
      "Epoch 72/500\n",
      "18/18 [==============================] - 1s 29ms/step - loss: 0.4861 - accuracy: 0.7684 - val_loss: 0.8088 - val_accuracy: 0.5631\n",
      "Epoch 73/500\n",
      "18/18 [==============================] - 1s 30ms/step - loss: 0.4773 - accuracy: 0.7930 - val_loss: 0.8181 - val_accuracy: 0.5243\n",
      "Epoch 74/500\n",
      "18/18 [==============================] - 1s 28ms/step - loss: 0.4786 - accuracy: 0.7825 - val_loss: 0.8175 - val_accuracy: 0.5340\n",
      "Epoch 75/500\n",
      "18/18 [==============================] - 0s 25ms/step - loss: 0.4809 - accuracy: 0.7807 - val_loss: 0.8288 - val_accuracy: 0.5243\n",
      "Epoch 76/500\n",
      "18/18 [==============================] - 1s 32ms/step - loss: 0.4691 - accuracy: 0.7860 - val_loss: 0.8385 - val_accuracy: 0.5437\n",
      "Epoch 77/500\n",
      "18/18 [==============================] - 0s 27ms/step - loss: 0.4734 - accuracy: 0.7754 - val_loss: 0.8357 - val_accuracy: 0.5243\n",
      "Epoch 78/500\n",
      "18/18 [==============================] - 1s 28ms/step - loss: 0.4645 - accuracy: 0.8018 - val_loss: 0.8520 - val_accuracy: 0.5049\n",
      "Epoch 79/500\n",
      "18/18 [==============================] - 0s 26ms/step - loss: 0.4735 - accuracy: 0.7772 - val_loss: 0.8291 - val_accuracy: 0.5534\n",
      "Epoch 80/500\n",
      "18/18 [==============================] - 0s 27ms/step - loss: 0.4611 - accuracy: 0.7947 - val_loss: 0.8535 - val_accuracy: 0.5049\n",
      "Epoch 81/500\n",
      "18/18 [==============================] - 0s 25ms/step - loss: 0.4675 - accuracy: 0.7825 - val_loss: 0.8603 - val_accuracy: 0.5049\n",
      "Epoch 82/500\n",
      "18/18 [==============================] - 0s 25ms/step - loss: 0.4659 - accuracy: 0.7947 - val_loss: 0.8588 - val_accuracy: 0.4854\n",
      "Epoch 83/500\n",
      "18/18 [==============================] - 0s 27ms/step - loss: 0.4598 - accuracy: 0.8105 - val_loss: 0.8506 - val_accuracy: 0.5340\n",
      "Epoch 84/500\n",
      "18/18 [==============================] - 1s 29ms/step - loss: 0.4585 - accuracy: 0.7982 - val_loss: 0.8636 - val_accuracy: 0.5049\n",
      "Epoch 85/500\n",
      "18/18 [==============================] - 0s 25ms/step - loss: 0.4604 - accuracy: 0.7807 - val_loss: 0.8567 - val_accuracy: 0.5437\n",
      "Epoch 86/500\n",
      "18/18 [==============================] - 0s 27ms/step - loss: 0.4522 - accuracy: 0.8175 - val_loss: 0.8716 - val_accuracy: 0.5146\n",
      "Epoch 87/500\n",
      "18/18 [==============================] - 0s 24ms/step - loss: 0.4495 - accuracy: 0.8140 - val_loss: 0.8817 - val_accuracy: 0.5146\n",
      "Epoch 88/500\n",
      "18/18 [==============================] - 0s 25ms/step - loss: 0.4408 - accuracy: 0.8053 - val_loss: 0.8727 - val_accuracy: 0.5243\n",
      "Epoch 89/500\n",
      "18/18 [==============================] - 1s 28ms/step - loss: 0.4464 - accuracy: 0.8158 - val_loss: 0.8697 - val_accuracy: 0.5631\n",
      "Epoch 90/500\n",
      "18/18 [==============================] - 0s 26ms/step - loss: 0.4361 - accuracy: 0.8263 - val_loss: 0.8995 - val_accuracy: 0.5049\n",
      "Epoch 91/500\n",
      "18/18 [==============================] - 0s 27ms/step - loss: 0.4400 - accuracy: 0.8158 - val_loss: 0.8779 - val_accuracy: 0.5631\n",
      "Epoch 92/500\n",
      "18/18 [==============================] - 0s 26ms/step - loss: 0.4400 - accuracy: 0.8000 - val_loss: 0.8920 - val_accuracy: 0.5340\n",
      "Epoch 93/500\n",
      "18/18 [==============================] - 0s 26ms/step - loss: 0.4339 - accuracy: 0.8088 - val_loss: 0.8773 - val_accuracy: 0.5534\n",
      "Epoch 94/500\n",
      "18/18 [==============================] - 1s 29ms/step - loss: 0.4312 - accuracy: 0.8123 - val_loss: 0.9197 - val_accuracy: 0.5049\n",
      "Epoch 95/500\n",
      "18/18 [==============================] - 1s 29ms/step - loss: 0.4382 - accuracy: 0.8070 - val_loss: 0.8801 - val_accuracy: 0.5825\n",
      "Epoch 96/500\n",
      "18/18 [==============================] - 1s 30ms/step - loss: 0.4318 - accuracy: 0.8281 - val_loss: 0.9216 - val_accuracy: 0.5146\n",
      "Epoch 97/500\n",
      "18/18 [==============================] - 1s 28ms/step - loss: 0.4321 - accuracy: 0.8298 - val_loss: 0.9108 - val_accuracy: 0.5437\n",
      "Epoch 98/500\n",
      "18/18 [==============================] - 0s 26ms/step - loss: 0.4279 - accuracy: 0.8263 - val_loss: 0.9019 - val_accuracy: 0.5437\n",
      "Epoch 99/500\n",
      "18/18 [==============================] - 0s 23ms/step - loss: 0.4350 - accuracy: 0.8140 - val_loss: 0.9125 - val_accuracy: 0.5437\n",
      "Epoch 100/500\n",
      "18/18 [==============================] - 0s 28ms/step - loss: 0.4151 - accuracy: 0.8246 - val_loss: 0.9150 - val_accuracy: 0.5340\n",
      "Epoch 101/500\n",
      "18/18 [==============================] - 0s 27ms/step - loss: 0.4155 - accuracy: 0.8316 - val_loss: 0.9372 - val_accuracy: 0.5243\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1/500\n",
      "18/18 [==============================] - 1s 35ms/step - loss: 0.7014 - accuracy: 0.4684 - val_loss: 0.6717 - val_accuracy: 0.6602\n",
      "Epoch 2/500\n",
      "18/18 [==============================] - 0s 27ms/step - loss: 0.6913 - accuracy: 0.5298 - val_loss: 0.6777 - val_accuracy: 0.6796\n",
      "Epoch 3/500\n",
      "18/18 [==============================] - 0s 26ms/step - loss: 0.6843 - accuracy: 0.5316 - val_loss: 0.6523 - val_accuracy: 0.6408\n",
      "Epoch 4/500\n",
      "18/18 [==============================] - 0s 27ms/step - loss: 0.6767 - accuracy: 0.5912 - val_loss: 0.6976 - val_accuracy: 0.4951\n",
      "Epoch 5/500\n",
      "18/18 [==============================] - 0s 24ms/step - loss: 0.6712 - accuracy: 0.6193 - val_loss: 0.6713 - val_accuracy: 0.5922\n",
      "Epoch 6/500\n",
      "18/18 [==============================] - 0s 27ms/step - loss: 0.6570 - accuracy: 0.6193 - val_loss: 0.6989 - val_accuracy: 0.5340\n",
      "Epoch 7/500\n",
      "18/18 [==============================] - 0s 25ms/step - loss: 0.6516 - accuracy: 0.6368 - val_loss: 0.6877 - val_accuracy: 0.5825\n",
      "Epoch 8/500\n",
      "18/18 [==============================] - 0s 24ms/step - loss: 0.6405 - accuracy: 0.6404 - val_loss: 0.6747 - val_accuracy: 0.5825\n",
      "Epoch 9/500\n",
      "18/18 [==============================] - 0s 26ms/step - loss: 0.6360 - accuracy: 0.6509 - val_loss: 0.6755 - val_accuracy: 0.5825\n",
      "Epoch 10/500\n",
      "18/18 [==============================] - 0s 23ms/step - loss: 0.6308 - accuracy: 0.6439 - val_loss: 0.6844 - val_accuracy: 0.5825\n",
      "Epoch 11/500\n",
      "18/18 [==============================] - 0s 24ms/step - loss: 0.6239 - accuracy: 0.6544 - val_loss: 0.6506 - val_accuracy: 0.6214\n",
      "Epoch 12/500\n",
      "18/18 [==============================] - 0s 27ms/step - loss: 0.6227 - accuracy: 0.6526 - val_loss: 0.6933 - val_accuracy: 0.5631\n",
      "Epoch 13/500\n",
      "18/18 [==============================] - 0s 26ms/step - loss: 0.6186 - accuracy: 0.6544 - val_loss: 0.6737 - val_accuracy: 0.6019\n",
      "Epoch 14/500\n",
      "18/18 [==============================] - 0s 27ms/step - loss: 0.6159 - accuracy: 0.6491 - val_loss: 0.6353 - val_accuracy: 0.6408\n",
      "Epoch 15/500\n",
      "18/18 [==============================] - 0s 26ms/step - loss: 0.6114 - accuracy: 0.6754 - val_loss: 0.6947 - val_accuracy: 0.5922\n",
      "Epoch 16/500\n",
      "18/18 [==============================] - 0s 26ms/step - loss: 0.6075 - accuracy: 0.6544 - val_loss: 0.6615 - val_accuracy: 0.6019\n",
      "Epoch 17/500\n",
      "18/18 [==============================] - 0s 24ms/step - loss: 0.6005 - accuracy: 0.6737 - val_loss: 0.6929 - val_accuracy: 0.6214\n",
      "Epoch 18/500\n",
      "18/18 [==============================] - 1s 28ms/step - loss: 0.6016 - accuracy: 0.6702 - val_loss: 0.6977 - val_accuracy: 0.6214\n",
      "Epoch 19/500\n",
      "18/18 [==============================] - 1s 28ms/step - loss: 0.5994 - accuracy: 0.6702 - val_loss: 0.6678 - val_accuracy: 0.6117\n",
      "Epoch 20/500\n",
      "18/18 [==============================] - 0s 26ms/step - loss: 0.5907 - accuracy: 0.6632 - val_loss: 0.6760 - val_accuracy: 0.6214\n",
      "Epoch 21/500\n",
      "18/18 [==============================] - 0s 26ms/step - loss: 0.5858 - accuracy: 0.6702 - val_loss: 0.7022 - val_accuracy: 0.6311\n",
      "Epoch 22/500\n",
      "18/18 [==============================] - 0s 27ms/step - loss: 0.5844 - accuracy: 0.6754 - val_loss: 0.6837 - val_accuracy: 0.6117\n",
      "Epoch 23/500\n",
      "18/18 [==============================] - 0s 25ms/step - loss: 0.5827 - accuracy: 0.6860 - val_loss: 0.6501 - val_accuracy: 0.6311\n",
      "Epoch 24/500\n",
      "18/18 [==============================] - 0s 25ms/step - loss: 0.5822 - accuracy: 0.6719 - val_loss: 0.7102 - val_accuracy: 0.6019\n",
      "Epoch 25/500\n",
      "18/18 [==============================] - 1s 29ms/step - loss: 0.5842 - accuracy: 0.6719 - val_loss: 0.6791 - val_accuracy: 0.6019\n",
      "Epoch 26/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18/18 [==============================] - 1s 30ms/step - loss: 0.5753 - accuracy: 0.6754 - val_loss: 0.6829 - val_accuracy: 0.6117\n",
      "Epoch 27/500\n",
      "18/18 [==============================] - 0s 26ms/step - loss: 0.5699 - accuracy: 0.6860 - val_loss: 0.6926 - val_accuracy: 0.6117\n",
      "Epoch 28/500\n",
      "18/18 [==============================] - 0s 27ms/step - loss: 0.5682 - accuracy: 0.6877 - val_loss: 0.6945 - val_accuracy: 0.6214\n",
      "Epoch 29/500\n",
      "18/18 [==============================] - 0s 28ms/step - loss: 0.5641 - accuracy: 0.6754 - val_loss: 0.6899 - val_accuracy: 0.6117\n",
      "Epoch 30/500\n",
      "18/18 [==============================] - 0s 21ms/step - loss: 0.5599 - accuracy: 0.6930 - val_loss: 0.6884 - val_accuracy: 0.6311\n",
      "Epoch 31/500\n",
      "18/18 [==============================] - 0s 27ms/step - loss: 0.5612 - accuracy: 0.6947 - val_loss: 0.6923 - val_accuracy: 0.5922\n",
      "Epoch 32/500\n",
      "18/18 [==============================] - 0s 27ms/step - loss: 0.5596 - accuracy: 0.6947 - val_loss: 0.7436 - val_accuracy: 0.5728\n",
      "Epoch 33/500\n",
      "18/18 [==============================] - 0s 27ms/step - loss: 0.5632 - accuracy: 0.6982 - val_loss: 0.6715 - val_accuracy: 0.6214\n",
      "Epoch 34/500\n",
      "18/18 [==============================] - 0s 27ms/step - loss: 0.5506 - accuracy: 0.7053 - val_loss: 0.7012 - val_accuracy: 0.6019\n",
      "Epoch 35/500\n",
      "18/18 [==============================] - 1s 30ms/step - loss: 0.5472 - accuracy: 0.7105 - val_loss: 0.6653 - val_accuracy: 0.6311\n",
      "Epoch 36/500\n",
      "18/18 [==============================] - 0s 26ms/step - loss: 0.5508 - accuracy: 0.7123 - val_loss: 0.7149 - val_accuracy: 0.5825\n",
      "Epoch 37/500\n",
      "18/18 [==============================] - 0s 25ms/step - loss: 0.5424 - accuracy: 0.7228 - val_loss: 0.6947 - val_accuracy: 0.6019\n",
      "Epoch 38/500\n",
      "18/18 [==============================] - 0s 27ms/step - loss: 0.5382 - accuracy: 0.7316 - val_loss: 0.6950 - val_accuracy: 0.6019\n",
      "Epoch 39/500\n",
      "18/18 [==============================] - 0s 27ms/step - loss: 0.5323 - accuracy: 0.7158 - val_loss: 0.7212 - val_accuracy: 0.5825\n",
      "Epoch 40/500\n",
      "18/18 [==============================] - 0s 27ms/step - loss: 0.5379 - accuracy: 0.7246 - val_loss: 0.6819 - val_accuracy: 0.6311\n",
      "Epoch 41/500\n",
      "18/18 [==============================] - 0s 24ms/step - loss: 0.5317 - accuracy: 0.7351 - val_loss: 0.7373 - val_accuracy: 0.5631\n",
      "Epoch 42/500\n",
      "18/18 [==============================] - 0s 25ms/step - loss: 0.5379 - accuracy: 0.7316 - val_loss: 0.7135 - val_accuracy: 0.6117\n",
      "Epoch 43/500\n",
      "18/18 [==============================] - 0s 26ms/step - loss: 0.5349 - accuracy: 0.7228 - val_loss: 0.7023 - val_accuracy: 0.6214\n",
      "Epoch 44/500\n",
      "18/18 [==============================] - 1s 29ms/step - loss: 0.5258 - accuracy: 0.7368 - val_loss: 0.7146 - val_accuracy: 0.6214\n",
      "Epoch 45/500\n",
      "18/18 [==============================] - 0s 27ms/step - loss: 0.5233 - accuracy: 0.7351 - val_loss: 0.6934 - val_accuracy: 0.6311\n",
      "Epoch 46/500\n",
      "18/18 [==============================] - 0s 25ms/step - loss: 0.5260 - accuracy: 0.7386 - val_loss: 0.6644 - val_accuracy: 0.6117\n",
      "Epoch 47/500\n",
      "18/18 [==============================] - 0s 24ms/step - loss: 0.5227 - accuracy: 0.7368 - val_loss: 0.7566 - val_accuracy: 0.5728\n",
      "Epoch 48/500\n",
      "18/18 [==============================] - 0s 26ms/step - loss: 0.5251 - accuracy: 0.7474 - val_loss: 0.7106 - val_accuracy: 0.6311\n",
      "Epoch 49/500\n",
      "18/18 [==============================] - 0s 25ms/step - loss: 0.5078 - accuracy: 0.7456 - val_loss: 0.7159 - val_accuracy: 0.6311\n",
      "Epoch 50/500\n",
      "18/18 [==============================] - 0s 24ms/step - loss: 0.5025 - accuracy: 0.7737 - val_loss: 0.7198 - val_accuracy: 0.6311\n",
      "Epoch 51/500\n",
      "18/18 [==============================] - 0s 26ms/step - loss: 0.5053 - accuracy: 0.7456 - val_loss: 0.6908 - val_accuracy: 0.6602\n",
      "Epoch 52/500\n",
      "18/18 [==============================] - 0s 26ms/step - loss: 0.5023 - accuracy: 0.7667 - val_loss: 0.7597 - val_accuracy: 0.5922\n",
      "Epoch 53/500\n",
      "18/18 [==============================] - 0s 24ms/step - loss: 0.4944 - accuracy: 0.7544 - val_loss: 0.7165 - val_accuracy: 0.6505\n",
      "Epoch 54/500\n",
      "18/18 [==============================] - 0s 26ms/step - loss: 0.4984 - accuracy: 0.7737 - val_loss: 0.7247 - val_accuracy: 0.6408\n",
      "Epoch 55/500\n",
      "18/18 [==============================] - 0s 26ms/step - loss: 0.4955 - accuracy: 0.7649 - val_loss: 0.7046 - val_accuracy: 0.6311\n",
      "Epoch 56/500\n",
      "18/18 [==============================] - 0s 26ms/step - loss: 0.4935 - accuracy: 0.7702 - val_loss: 0.7568 - val_accuracy: 0.6311\n",
      "Epoch 57/500\n",
      "18/18 [==============================] - 1s 31ms/step - loss: 0.4930 - accuracy: 0.7667 - val_loss: 0.6992 - val_accuracy: 0.6311\n",
      "Epoch 58/500\n",
      "18/18 [==============================] - 0s 27ms/step - loss: 0.4886 - accuracy: 0.7649 - val_loss: 0.7218 - val_accuracy: 0.6505\n",
      "Epoch 59/500\n",
      "18/18 [==============================] - 0s 24ms/step - loss: 0.4865 - accuracy: 0.7772 - val_loss: 0.7533 - val_accuracy: 0.6117\n",
      "Epoch 60/500\n",
      "18/18 [==============================] - 0s 24ms/step - loss: 0.4803 - accuracy: 0.7842 - val_loss: 0.7294 - val_accuracy: 0.6408\n",
      "Epoch 61/500\n",
      "18/18 [==============================] - 0s 24ms/step - loss: 0.4666 - accuracy: 0.7842 - val_loss: 0.7252 - val_accuracy: 0.6505\n",
      "Epoch 62/500\n",
      "18/18 [==============================] - 0s 27ms/step - loss: 0.4683 - accuracy: 0.7930 - val_loss: 0.7451 - val_accuracy: 0.6408\n",
      "Epoch 63/500\n",
      "18/18 [==============================] - 0s 24ms/step - loss: 0.4676 - accuracy: 0.7912 - val_loss: 0.7403 - val_accuracy: 0.6311\n",
      "Epoch 64/500\n",
      "18/18 [==============================] - 1s 28ms/step - loss: 0.4756 - accuracy: 0.7860 - val_loss: 0.7712 - val_accuracy: 0.6214\n",
      "Epoch 65/500\n",
      "18/18 [==============================] - 0s 26ms/step - loss: 0.4665 - accuracy: 0.7789 - val_loss: 0.7306 - val_accuracy: 0.6311\n",
      "Epoch 66/500\n",
      "18/18 [==============================] - 0s 25ms/step - loss: 0.4506 - accuracy: 0.8018 - val_loss: 0.7437 - val_accuracy: 0.6311\n",
      "Epoch 67/500\n",
      "18/18 [==============================] - 0s 28ms/step - loss: 0.4501 - accuracy: 0.8070 - val_loss: 0.7330 - val_accuracy: 0.6311\n",
      "Epoch 68/500\n",
      "18/18 [==============================] - 0s 25ms/step - loss: 0.4548 - accuracy: 0.7825 - val_loss: 0.7538 - val_accuracy: 0.6117\n",
      "Epoch 69/500\n",
      "18/18 [==============================] - 0s 24ms/step - loss: 0.4597 - accuracy: 0.7895 - val_loss: 0.7948 - val_accuracy: 0.5922\n",
      "Epoch 70/500\n",
      "18/18 [==============================] - 0s 26ms/step - loss: 0.4513 - accuracy: 0.8035 - val_loss: 0.7575 - val_accuracy: 0.6214\n",
      "Epoch 71/500\n",
      "18/18 [==============================] - 0s 27ms/step - loss: 0.4444 - accuracy: 0.8000 - val_loss: 0.7665 - val_accuracy: 0.6117\n",
      "Epoch 72/500\n",
      "18/18 [==============================] - 0s 28ms/step - loss: 0.4450 - accuracy: 0.7965 - val_loss: 0.7384 - val_accuracy: 0.6408\n",
      "Epoch 73/500\n",
      "18/18 [==============================] - 0s 28ms/step - loss: 0.4409 - accuracy: 0.7947 - val_loss: 0.7701 - val_accuracy: 0.6117\n",
      "Epoch 74/500\n",
      "18/18 [==============================] - 0s 24ms/step - loss: 0.4393 - accuracy: 0.7912 - val_loss: 0.7727 - val_accuracy: 0.6214\n",
      "Epoch 75/500\n",
      "18/18 [==============================] - 0s 28ms/step - loss: 0.4319 - accuracy: 0.8105 - val_loss: 0.7705 - val_accuracy: 0.6311\n",
      "Epoch 76/500\n",
      "18/18 [==============================] - 0s 27ms/step - loss: 0.4373 - accuracy: 0.8105 - val_loss: 0.7716 - val_accuracy: 0.6214\n",
      "Epoch 77/500\n",
      "18/18 [==============================] - 0s 27ms/step - loss: 0.4201 - accuracy: 0.8211 - val_loss: 0.7969 - val_accuracy: 0.6214\n",
      "Epoch 78/500\n",
      "18/18 [==============================] - 0s 24ms/step - loss: 0.4285 - accuracy: 0.8105 - val_loss: 0.7552 - val_accuracy: 0.6214\n",
      "Epoch 79/500\n",
      "18/18 [==============================] - 0s 28ms/step - loss: 0.4198 - accuracy: 0.8105 - val_loss: 0.7880 - val_accuracy: 0.6311\n",
      "Epoch 80/500\n",
      "18/18 [==============================] - 0s 27ms/step - loss: 0.4192 - accuracy: 0.8053 - val_loss: 0.7883 - val_accuracy: 0.6214\n",
      "Epoch 81/500\n",
      "18/18 [==============================] - 0s 26ms/step - loss: 0.4213 - accuracy: 0.8246 - val_loss: 0.8153 - val_accuracy: 0.6214\n",
      "Epoch 82/500\n",
      "18/18 [==============================] - 0s 27ms/step - loss: 0.4110 - accuracy: 0.8228 - val_loss: 0.7786 - val_accuracy: 0.6311\n",
      "Epoch 83/500\n",
      "18/18 [==============================] - 0s 25ms/step - loss: 0.4073 - accuracy: 0.8316 - val_loss: 0.8073 - val_accuracy: 0.6311\n",
      "Epoch 84/500\n",
      "18/18 [==============================] - 0s 24ms/step - loss: 0.4166 - accuracy: 0.8123 - val_loss: 0.7979 - val_accuracy: 0.6311\n",
      "Epoch 85/500\n",
      "18/18 [==============================] - 0s 26ms/step - loss: 0.4123 - accuracy: 0.8070 - val_loss: 0.7834 - val_accuracy: 0.6311\n",
      "Epoch 86/500\n",
      "18/18 [==============================] - 0s 25ms/step - loss: 0.4120 - accuracy: 0.8246 - val_loss: 0.7981 - val_accuracy: 0.6311\n",
      "Epoch 87/500\n",
      "18/18 [==============================] - 0s 27ms/step - loss: 0.4086 - accuracy: 0.8193 - val_loss: 0.8502 - val_accuracy: 0.5922\n",
      "Epoch 88/500\n",
      "18/18 [==============================] - 0s 28ms/step - loss: 0.3977 - accuracy: 0.8351 - val_loss: 0.8013 - val_accuracy: 0.6311\n",
      "Epoch 89/500\n",
      "18/18 [==============================] - 1s 28ms/step - loss: 0.4037 - accuracy: 0.8263 - val_loss: 0.8447 - val_accuracy: 0.6311\n",
      "Epoch 90/500\n",
      "18/18 [==============================] - 1s 28ms/step - loss: 0.3908 - accuracy: 0.8386 - val_loss: 0.8191 - val_accuracy: 0.6214\n",
      "Epoch 91/500\n",
      "18/18 [==============================] - 0s 25ms/step - loss: 0.3942 - accuracy: 0.8298 - val_loss: 0.8402 - val_accuracy: 0.6311\n",
      "Epoch 92/500\n",
      "18/18 [==============================] - 0s 26ms/step - loss: 0.3799 - accuracy: 0.8421 - val_loss: 0.8171 - val_accuracy: 0.6408\n",
      "Epoch 93/500\n",
      "18/18 [==============================] - 0s 25ms/step - loss: 0.3780 - accuracy: 0.8368 - val_loss: 0.8438 - val_accuracy: 0.6214\n",
      "Epoch 94/500\n",
      "18/18 [==============================] - 0s 25ms/step - loss: 0.3865 - accuracy: 0.8368 - val_loss: 0.8636 - val_accuracy: 0.6117\n",
      "Epoch 95/500\n",
      "18/18 [==============================] - 0s 23ms/step - loss: 0.3753 - accuracy: 0.8596 - val_loss: 0.8377 - val_accuracy: 0.6214\n",
      "Epoch 96/500\n",
      "18/18 [==============================] - 0s 25ms/step - loss: 0.3788 - accuracy: 0.8421 - val_loss: 0.8362 - val_accuracy: 0.6214\n",
      "Epoch 97/500\n",
      "18/18 [==============================] - 0s 28ms/step - loss: 0.3802 - accuracy: 0.8298 - val_loss: 0.8407 - val_accuracy: 0.6214\n",
      "Epoch 98/500\n",
      "18/18 [==============================] - 0s 27ms/step - loss: 0.3744 - accuracy: 0.8404 - val_loss: 0.8818 - val_accuracy: 0.5922\n",
      "Epoch 99/500\n",
      "18/18 [==============================] - 0s 27ms/step - loss: 0.3670 - accuracy: 0.8456 - val_loss: 0.8608 - val_accuracy: 0.6117\n",
      "Epoch 100/500\n",
      "18/18 [==============================] - 0s 24ms/step - loss: 0.3634 - accuracy: 0.8439 - val_loss: 0.8643 - val_accuracy: 0.6311\n",
      "Epoch 101/500\n",
      "18/18 [==============================] - 0s 25ms/step - loss: 0.3596 - accuracy: 0.8474 - val_loss: 0.8593 - val_accuracy: 0.6019\n",
      "Epoch 102/500\n",
      "18/18 [==============================] - 1s 30ms/step - loss: 0.3534 - accuracy: 0.8526 - val_loss: 0.9335 - val_accuracy: 0.5631\n",
      "WARNING:tensorflow:5 out of the last 9 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f89038e2790> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 1/500\n",
      "18/18 [==============================] - 1s 36ms/step - loss: 0.6934 - accuracy: 0.5070 - val_loss: 0.6336 - val_accuracy: 0.6893\n",
      "Epoch 2/500\n",
      "18/18 [==============================] - 1s 30ms/step - loss: 0.6856 - accuracy: 0.5421 - val_loss: 0.6659 - val_accuracy: 0.6117\n",
      "Epoch 3/500\n",
      "18/18 [==============================] - 1s 28ms/step - loss: 0.6883 - accuracy: 0.5439 - val_loss: 0.6863 - val_accuracy: 0.5631\n",
      "Epoch 4/500\n",
      "18/18 [==============================] - 1s 31ms/step - loss: 0.6774 - accuracy: 0.6211 - val_loss: 0.6628 - val_accuracy: 0.6019\n",
      "Epoch 5/500\n",
      "18/18 [==============================] - 0s 27ms/step - loss: 0.6666 - accuracy: 0.6246 - val_loss: 0.6881 - val_accuracy: 0.5825\n",
      "Epoch 6/500\n",
      "18/18 [==============================] - 1s 29ms/step - loss: 0.6578 - accuracy: 0.6211 - val_loss: 0.6294 - val_accuracy: 0.6893\n",
      "Epoch 7/500\n",
      "18/18 [==============================] - 1s 29ms/step - loss: 0.6557 - accuracy: 0.6386 - val_loss: 0.6626 - val_accuracy: 0.5631\n",
      "Epoch 8/500\n",
      "18/18 [==============================] - 1s 30ms/step - loss: 0.6455 - accuracy: 0.6368 - val_loss: 0.6121 - val_accuracy: 0.7379\n",
      "Epoch 9/500\n",
      "18/18 [==============================] - 1s 28ms/step - loss: 0.6552 - accuracy: 0.6123 - val_loss: 0.6568 - val_accuracy: 0.5922\n",
      "Epoch 10/500\n",
      "18/18 [==============================] - 0s 27ms/step - loss: 0.6351 - accuracy: 0.6439 - val_loss: 0.6680 - val_accuracy: 0.5534\n",
      "Epoch 11/500\n",
      "18/18 [==============================] - 1s 30ms/step - loss: 0.6333 - accuracy: 0.6439 - val_loss: 0.6630 - val_accuracy: 0.5728\n",
      "Epoch 12/500\n",
      "18/18 [==============================] - 1s 29ms/step - loss: 0.6262 - accuracy: 0.6632 - val_loss: 0.6313 - val_accuracy: 0.6019\n",
      "Epoch 13/500\n",
      "18/18 [==============================] - 0s 27ms/step - loss: 0.6225 - accuracy: 0.6596 - val_loss: 0.6420 - val_accuracy: 0.5922\n",
      "Epoch 14/500\n",
      "18/18 [==============================] - 0s 28ms/step - loss: 0.6116 - accuracy: 0.6754 - val_loss: 0.6469 - val_accuracy: 0.5922\n",
      "Epoch 15/500\n",
      "18/18 [==============================] - 1s 31ms/step - loss: 0.6077 - accuracy: 0.6807 - val_loss: 0.6671 - val_accuracy: 0.5534\n",
      "Epoch 16/500\n",
      "18/18 [==============================] - 0s 24ms/step - loss: 0.6046 - accuracy: 0.6860 - val_loss: 0.6328 - val_accuracy: 0.6117\n",
      "Epoch 17/500\n",
      "18/18 [==============================] - 0s 27ms/step - loss: 0.6056 - accuracy: 0.6982 - val_loss: 0.6609 - val_accuracy: 0.5825\n",
      "Epoch 18/500\n",
      "18/18 [==============================] - 0s 27ms/step - loss: 0.6000 - accuracy: 0.6947 - val_loss: 0.6726 - val_accuracy: 0.5534\n",
      "Epoch 19/500\n",
      "18/18 [==============================] - 1s 28ms/step - loss: 0.5948 - accuracy: 0.6860 - val_loss: 0.6591 - val_accuracy: 0.5825\n",
      "Epoch 20/500\n",
      "18/18 [==============================] - 1s 31ms/step - loss: 0.5907 - accuracy: 0.6965 - val_loss: 0.6561 - val_accuracy: 0.5922\n",
      "Epoch 21/500\n",
      "18/18 [==============================] - 0s 28ms/step - loss: 0.5878 - accuracy: 0.6877 - val_loss: 0.6626 - val_accuracy: 0.6117\n",
      "Epoch 22/500\n",
      "18/18 [==============================] - 0s 25ms/step - loss: 0.5867 - accuracy: 0.6912 - val_loss: 0.6910 - val_accuracy: 0.5146\n",
      "Epoch 23/500\n",
      "18/18 [==============================] - 0s 24ms/step - loss: 0.5895 - accuracy: 0.6719 - val_loss: 0.6949 - val_accuracy: 0.5049\n",
      "Epoch 24/500\n",
      "18/18 [==============================] - 0s 24ms/step - loss: 0.5810 - accuracy: 0.7018 - val_loss: 0.6449 - val_accuracy: 0.6602\n",
      "Epoch 25/500\n",
      "18/18 [==============================] - 1s 28ms/step - loss: 0.5706 - accuracy: 0.7193 - val_loss: 0.7011 - val_accuracy: 0.5146\n",
      "Epoch 26/500\n",
      "18/18 [==============================] - 1s 29ms/step - loss: 0.5715 - accuracy: 0.7070 - val_loss: 0.6412 - val_accuracy: 0.6796\n",
      "Epoch 27/500\n",
      "18/18 [==============================] - 0s 28ms/step - loss: 0.5745 - accuracy: 0.6965 - val_loss: 0.6938 - val_accuracy: 0.5243\n",
      "Epoch 28/500\n",
      "18/18 [==============================] - 0s 28ms/step - loss: 0.5682 - accuracy: 0.7105 - val_loss: 0.6857 - val_accuracy: 0.5534\n",
      "Epoch 29/500\n",
      "18/18 [==============================] - 0s 26ms/step - loss: 0.5581 - accuracy: 0.7175 - val_loss: 0.6571 - val_accuracy: 0.6602\n",
      "Epoch 30/500\n",
      "18/18 [==============================] - 1s 29ms/step - loss: 0.5636 - accuracy: 0.7088 - val_loss: 0.6809 - val_accuracy: 0.5922\n",
      "Epoch 31/500\n",
      "18/18 [==============================] - 0s 28ms/step - loss: 0.5513 - accuracy: 0.7228 - val_loss: 0.6581 - val_accuracy: 0.6602\n",
      "Epoch 32/500\n",
      "18/18 [==============================] - 0s 23ms/step - loss: 0.5472 - accuracy: 0.7281 - val_loss: 0.7063 - val_accuracy: 0.5437\n",
      "Epoch 33/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18/18 [==============================] - 0s 28ms/step - loss: 0.5487 - accuracy: 0.7281 - val_loss: 0.6531 - val_accuracy: 0.6796\n",
      "Epoch 34/500\n",
      "18/18 [==============================] - 1s 29ms/step - loss: 0.5528 - accuracy: 0.7018 - val_loss: 0.6646 - val_accuracy: 0.6602\n",
      "Epoch 35/500\n",
      "18/18 [==============================] - 0s 28ms/step - loss: 0.5468 - accuracy: 0.7211 - val_loss: 0.7138 - val_accuracy: 0.5631\n",
      "Epoch 36/500\n",
      "18/18 [==============================] - 1s 28ms/step - loss: 0.5429 - accuracy: 0.7228 - val_loss: 0.6601 - val_accuracy: 0.6796\n",
      "Epoch 37/500\n",
      "18/18 [==============================] - 1s 31ms/step - loss: 0.5330 - accuracy: 0.7333 - val_loss: 0.6731 - val_accuracy: 0.6505\n",
      "Epoch 38/500\n",
      "18/18 [==============================] - 0s 28ms/step - loss: 0.5278 - accuracy: 0.7316 - val_loss: 0.7044 - val_accuracy: 0.5922\n",
      "Epoch 39/500\n",
      "18/18 [==============================] - 1s 30ms/step - loss: 0.5241 - accuracy: 0.7561 - val_loss: 0.6663 - val_accuracy: 0.6699\n",
      "Epoch 40/500\n",
      "18/18 [==============================] - 1s 30ms/step - loss: 0.5272 - accuracy: 0.7561 - val_loss: 0.7262 - val_accuracy: 0.5631\n",
      "Epoch 41/500\n",
      "18/18 [==============================] - 1s 29ms/step - loss: 0.5261 - accuracy: 0.7368 - val_loss: 0.7003 - val_accuracy: 0.6214\n",
      "Epoch 42/500\n",
      "18/18 [==============================] - 0s 27ms/step - loss: 0.5204 - accuracy: 0.7491 - val_loss: 0.6447 - val_accuracy: 0.6699\n",
      "Epoch 43/500\n",
      "18/18 [==============================] - 0s 27ms/step - loss: 0.5191 - accuracy: 0.7544 - val_loss: 0.7298 - val_accuracy: 0.5825\n",
      "Epoch 44/500\n",
      "18/18 [==============================] - 1s 29ms/step - loss: 0.5109 - accuracy: 0.7579 - val_loss: 0.6677 - val_accuracy: 0.6602\n",
      "Epoch 45/500\n",
      "18/18 [==============================] - 1s 30ms/step - loss: 0.5069 - accuracy: 0.7491 - val_loss: 0.6950 - val_accuracy: 0.6505\n",
      "Epoch 46/500\n",
      "18/18 [==============================] - 1s 29ms/step - loss: 0.5049 - accuracy: 0.7544 - val_loss: 0.6773 - val_accuracy: 0.6505\n",
      "Epoch 47/500\n",
      "18/18 [==============================] - 1s 28ms/step - loss: 0.4991 - accuracy: 0.7702 - val_loss: 0.6943 - val_accuracy: 0.6311\n",
      "Epoch 48/500\n",
      "18/18 [==============================] - 1s 30ms/step - loss: 0.4982 - accuracy: 0.7632 - val_loss: 0.7372 - val_accuracy: 0.5922\n",
      "Epoch 49/500\n",
      "18/18 [==============================] - 1s 28ms/step - loss: 0.4984 - accuracy: 0.7509 - val_loss: 0.6940 - val_accuracy: 0.6408\n",
      "Epoch 50/500\n",
      "18/18 [==============================] - 1s 30ms/step - loss: 0.4824 - accuracy: 0.7754 - val_loss: 0.7149 - val_accuracy: 0.6117\n",
      "Epoch 51/500\n",
      "18/18 [==============================] - 0s 25ms/step - loss: 0.4847 - accuracy: 0.7614 - val_loss: 0.6837 - val_accuracy: 0.6602\n",
      "Epoch 52/500\n",
      "18/18 [==============================] - 1s 29ms/step - loss: 0.4868 - accuracy: 0.7702 - val_loss: 0.6931 - val_accuracy: 0.6311\n",
      "Epoch 53/500\n",
      "18/18 [==============================] - 0s 28ms/step - loss: 0.4812 - accuracy: 0.7772 - val_loss: 0.6952 - val_accuracy: 0.6311\n",
      "Epoch 54/500\n",
      "18/18 [==============================] - 1s 29ms/step - loss: 0.4813 - accuracy: 0.7684 - val_loss: 0.7144 - val_accuracy: 0.6311\n",
      "Epoch 55/500\n",
      "18/18 [==============================] - 1s 29ms/step - loss: 0.4761 - accuracy: 0.7754 - val_loss: 0.7029 - val_accuracy: 0.6214\n",
      "Epoch 56/500\n",
      "18/18 [==============================] - 0s 25ms/step - loss: 0.4724 - accuracy: 0.7596 - val_loss: 0.7047 - val_accuracy: 0.6311\n",
      "Epoch 57/500\n",
      "18/18 [==============================] - 1s 30ms/step - loss: 0.4753 - accuracy: 0.7754 - val_loss: 0.7493 - val_accuracy: 0.6214\n",
      "Epoch 58/500\n",
      "18/18 [==============================] - 1s 29ms/step - loss: 0.4666 - accuracy: 0.7702 - val_loss: 0.7326 - val_accuracy: 0.6311\n",
      "Epoch 59/500\n",
      "18/18 [==============================] - 1s 29ms/step - loss: 0.4600 - accuracy: 0.7667 - val_loss: 0.7163 - val_accuracy: 0.6311\n",
      "Epoch 60/500\n",
      "18/18 [==============================] - 0s 24ms/step - loss: 0.4556 - accuracy: 0.7842 - val_loss: 0.7533 - val_accuracy: 0.5922\n",
      "Epoch 61/500\n",
      "18/18 [==============================] - 1s 29ms/step - loss: 0.4562 - accuracy: 0.7807 - val_loss: 0.7312 - val_accuracy: 0.6408\n",
      "Epoch 62/500\n",
      "18/18 [==============================] - 0s 25ms/step - loss: 0.4497 - accuracy: 0.7842 - val_loss: 0.6973 - val_accuracy: 0.6505\n",
      "Epoch 63/500\n",
      "18/18 [==============================] - 0s 26ms/step - loss: 0.4600 - accuracy: 0.7772 - val_loss: 0.7014 - val_accuracy: 0.6505\n",
      "Epoch 64/500\n",
      "18/18 [==============================] - 1s 31ms/step - loss: 0.4544 - accuracy: 0.7912 - val_loss: 0.7336 - val_accuracy: 0.6311\n",
      "Epoch 65/500\n",
      "18/18 [==============================] - 1s 28ms/step - loss: 0.4387 - accuracy: 0.7860 - val_loss: 0.7134 - val_accuracy: 0.6311\n",
      "Epoch 66/500\n",
      "18/18 [==============================] - 0s 28ms/step - loss: 0.4319 - accuracy: 0.8070 - val_loss: 0.7488 - val_accuracy: 0.6311\n",
      "Epoch 67/500\n",
      "18/18 [==============================] - 0s 27ms/step - loss: 0.4304 - accuracy: 0.7912 - val_loss: 0.7630 - val_accuracy: 0.6214\n",
      "Epoch 68/500\n",
      "18/18 [==============================] - 1s 29ms/step - loss: 0.4268 - accuracy: 0.8070 - val_loss: 0.6918 - val_accuracy: 0.6893\n",
      "Epoch 69/500\n",
      "18/18 [==============================] - 0s 26ms/step - loss: 0.4415 - accuracy: 0.7842 - val_loss: 0.7909 - val_accuracy: 0.6019\n",
      "Epoch 70/500\n",
      "18/18 [==============================] - 0s 27ms/step - loss: 0.4163 - accuracy: 0.8175 - val_loss: 0.7506 - val_accuracy: 0.6408\n",
      "Epoch 71/500\n",
      "18/18 [==============================] - 0s 28ms/step - loss: 0.4182 - accuracy: 0.8140 - val_loss: 0.7424 - val_accuracy: 0.6408\n",
      "Epoch 72/500\n",
      "18/18 [==============================] - 0s 26ms/step - loss: 0.4281 - accuracy: 0.8070 - val_loss: 0.7870 - val_accuracy: 0.6117\n",
      "Epoch 73/500\n",
      "18/18 [==============================] - 1s 29ms/step - loss: 0.4171 - accuracy: 0.8088 - val_loss: 0.7719 - val_accuracy: 0.6117\n",
      "Epoch 74/500\n",
      "18/18 [==============================] - 0s 25ms/step - loss: 0.4095 - accuracy: 0.8035 - val_loss: 0.7379 - val_accuracy: 0.6408\n",
      "Epoch 75/500\n",
      "18/18 [==============================] - 0s 28ms/step - loss: 0.4107 - accuracy: 0.8070 - val_loss: 0.7943 - val_accuracy: 0.6117\n",
      "Epoch 76/500\n",
      "18/18 [==============================] - 0s 28ms/step - loss: 0.4054 - accuracy: 0.8228 - val_loss: 0.8008 - val_accuracy: 0.6117\n",
      "Epoch 77/500\n",
      "18/18 [==============================] - 1s 29ms/step - loss: 0.4038 - accuracy: 0.8211 - val_loss: 0.7715 - val_accuracy: 0.6214\n",
      "Epoch 78/500\n",
      "18/18 [==============================] - 0s 24ms/step - loss: 0.4027 - accuracy: 0.8158 - val_loss: 0.7714 - val_accuracy: 0.6311\n",
      "Epoch 79/500\n",
      "18/18 [==============================] - 1s 29ms/step - loss: 0.4037 - accuracy: 0.8018 - val_loss: 0.8193 - val_accuracy: 0.5922\n",
      "Epoch 80/500\n",
      "18/18 [==============================] - 0s 27ms/step - loss: 0.3949 - accuracy: 0.8105 - val_loss: 0.7471 - val_accuracy: 0.6408\n",
      "Epoch 81/500\n",
      "18/18 [==============================] - 0s 26ms/step - loss: 0.3904 - accuracy: 0.8193 - val_loss: 0.8045 - val_accuracy: 0.6019\n",
      "Epoch 82/500\n",
      "18/18 [==============================] - 1s 29ms/step - loss: 0.3936 - accuracy: 0.8228 - val_loss: 0.7845 - val_accuracy: 0.6311\n",
      "Epoch 83/500\n",
      "18/18 [==============================] - 1s 30ms/step - loss: 0.3817 - accuracy: 0.8368 - val_loss: 0.7727 - val_accuracy: 0.6408\n",
      "Epoch 84/500\n",
      "18/18 [==============================] - 1s 29ms/step - loss: 0.3922 - accuracy: 0.8228 - val_loss: 0.8119 - val_accuracy: 0.6019\n",
      "Epoch 85/500\n",
      "18/18 [==============================] - 1s 30ms/step - loss: 0.3736 - accuracy: 0.8421 - val_loss: 0.7945 - val_accuracy: 0.6408\n",
      "Epoch 86/500\n",
      "18/18 [==============================] - 0s 27ms/step - loss: 0.3683 - accuracy: 0.8351 - val_loss: 0.8162 - val_accuracy: 0.6117\n",
      "Epoch 87/500\n",
      "18/18 [==============================] - 1s 29ms/step - loss: 0.3727 - accuracy: 0.8386 - val_loss: 0.7807 - val_accuracy: 0.6505\n",
      "Epoch 88/500\n",
      "18/18 [==============================] - 1s 31ms/step - loss: 0.3709 - accuracy: 0.8211 - val_loss: 0.8077 - val_accuracy: 0.6214\n",
      "Epoch 89/500\n",
      "18/18 [==============================] - 1s 30ms/step - loss: 0.3577 - accuracy: 0.8561 - val_loss: 0.8084 - val_accuracy: 0.6408\n",
      "Epoch 90/500\n",
      "18/18 [==============================] - 1s 30ms/step - loss: 0.3725 - accuracy: 0.8333 - val_loss: 0.8132 - val_accuracy: 0.6408\n",
      "Epoch 91/500\n",
      "18/18 [==============================] - 1s 31ms/step - loss: 0.3640 - accuracy: 0.8439 - val_loss: 0.8532 - val_accuracy: 0.5825\n",
      "Epoch 92/500\n",
      "18/18 [==============================] - 0s 27ms/step - loss: 0.3506 - accuracy: 0.8491 - val_loss: 0.8015 - val_accuracy: 0.6408\n",
      "Epoch 93/500\n",
      "18/18 [==============================] - 0s 27ms/step - loss: 0.3593 - accuracy: 0.8351 - val_loss: 0.8194 - val_accuracy: 0.6311\n",
      "Epoch 94/500\n",
      "18/18 [==============================] - 0s 27ms/step - loss: 0.3585 - accuracy: 0.8368 - val_loss: 0.8411 - val_accuracy: 0.5922\n",
      "Epoch 95/500\n",
      "18/18 [==============================] - 0s 26ms/step - loss: 0.3500 - accuracy: 0.8544 - val_loss: 0.8144 - val_accuracy: 0.6408\n",
      "Epoch 96/500\n",
      "18/18 [==============================] - 1s 29ms/step - loss: 0.3471 - accuracy: 0.8421 - val_loss: 0.8412 - val_accuracy: 0.6117\n",
      "Epoch 97/500\n",
      "18/18 [==============================] - 0s 26ms/step - loss: 0.3483 - accuracy: 0.8368 - val_loss: 0.8279 - val_accuracy: 0.6311\n",
      "Epoch 98/500\n",
      "18/18 [==============================] - 0s 24ms/step - loss: 0.3466 - accuracy: 0.8456 - val_loss: 0.8334 - val_accuracy: 0.6214\n",
      "Epoch 99/500\n",
      "18/18 [==============================] - 0s 28ms/step - loss: 0.3431 - accuracy: 0.8491 - val_loss: 0.8516 - val_accuracy: 0.6311\n",
      "Epoch 100/500\n",
      "18/18 [==============================] - 0s 28ms/step - loss: 0.3362 - accuracy: 0.8474 - val_loss: 0.8340 - val_accuracy: 0.6311\n",
      "Epoch 101/500\n",
      "18/18 [==============================] - 0s 27ms/step - loss: 0.3409 - accuracy: 0.8456 - val_loss: 0.8493 - val_accuracy: 0.6311\n",
      "Epoch 102/500\n",
      "18/18 [==============================] - 0s 26ms/step - loss: 0.3259 - accuracy: 0.8544 - val_loss: 0.8492 - val_accuracy: 0.6311\n",
      "Epoch 103/500\n",
      "18/18 [==============================] - 1s 30ms/step - loss: 0.3297 - accuracy: 0.8614 - val_loss: 0.8899 - val_accuracy: 0.6019\n",
      "Epoch 104/500\n",
      "18/18 [==============================] - 1s 30ms/step - loss: 0.3156 - accuracy: 0.8702 - val_loss: 0.8430 - val_accuracy: 0.6408\n",
      "Epoch 105/500\n",
      "18/18 [==============================] - 0s 27ms/step - loss: 0.3151 - accuracy: 0.8632 - val_loss: 0.8998 - val_accuracy: 0.6117\n",
      "Epoch 106/500\n",
      "18/18 [==============================] - 0s 27ms/step - loss: 0.3222 - accuracy: 0.8632 - val_loss: 0.8045 - val_accuracy: 0.6505\n",
      "Epoch 107/500\n",
      "18/18 [==============================] - 0s 27ms/step - loss: 0.3077 - accuracy: 0.8667 - val_loss: 0.9069 - val_accuracy: 0.5922\n",
      "Epoch 108/500\n",
      "18/18 [==============================] - 0s 26ms/step - loss: 0.3151 - accuracy: 0.8579 - val_loss: 0.8790 - val_accuracy: 0.6214\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f89034d2b80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "2/2 [==============================] - 0s 9ms/step\n",
      "Epoch 1/500\n",
      "18/18 [==============================] - 1s 34ms/step - loss: 0.6981 - accuracy: 0.5053 - val_loss: 0.6672 - val_accuracy: 0.6214\n",
      "Epoch 2/500\n",
      "18/18 [==============================] - 0s 25ms/step - loss: 0.6806 - accuracy: 0.5789 - val_loss: 0.6505 - val_accuracy: 0.6117\n",
      "Epoch 3/500\n",
      "18/18 [==============================] - 0s 25ms/step - loss: 0.6751 - accuracy: 0.5877 - val_loss: 0.6693 - val_accuracy: 0.6408\n",
      "Epoch 4/500\n",
      "18/18 [==============================] - 0s 27ms/step - loss: 0.6585 - accuracy: 0.6386 - val_loss: 0.6416 - val_accuracy: 0.6699\n",
      "Epoch 5/500\n",
      "18/18 [==============================] - 0s 25ms/step - loss: 0.6574 - accuracy: 0.6351 - val_loss: 0.6345 - val_accuracy: 0.6311\n",
      "Epoch 6/500\n",
      "18/18 [==============================] - 0s 23ms/step - loss: 0.6540 - accuracy: 0.6228 - val_loss: 0.6435 - val_accuracy: 0.6408\n",
      "Epoch 7/500\n",
      "18/18 [==============================] - 0s 25ms/step - loss: 0.6524 - accuracy: 0.6228 - val_loss: 0.6870 - val_accuracy: 0.6019\n",
      "Epoch 8/500\n",
      "18/18 [==============================] - 0s 28ms/step - loss: 0.6480 - accuracy: 0.6298 - val_loss: 0.6457 - val_accuracy: 0.6214\n",
      "Epoch 9/500\n",
      "18/18 [==============================] - 1s 30ms/step - loss: 0.6420 - accuracy: 0.6368 - val_loss: 0.6335 - val_accuracy: 0.6117\n",
      "Epoch 10/500\n",
      "18/18 [==============================] - 0s 25ms/step - loss: 0.6340 - accuracy: 0.6579 - val_loss: 0.7143 - val_accuracy: 0.5631\n",
      "Epoch 11/500\n",
      "18/18 [==============================] - 0s 27ms/step - loss: 0.6369 - accuracy: 0.6596 - val_loss: 0.6705 - val_accuracy: 0.6117\n",
      "Epoch 12/500\n",
      "18/18 [==============================] - 0s 28ms/step - loss: 0.6259 - accuracy: 0.6719 - val_loss: 0.6820 - val_accuracy: 0.6214\n",
      "Epoch 13/500\n",
      "18/18 [==============================] - 0s 25ms/step - loss: 0.6262 - accuracy: 0.6737 - val_loss: 0.7005 - val_accuracy: 0.6019\n",
      "Epoch 14/500\n",
      "18/18 [==============================] - 0s 25ms/step - loss: 0.6230 - accuracy: 0.6684 - val_loss: 0.6845 - val_accuracy: 0.5922\n",
      "Epoch 15/500\n",
      "18/18 [==============================] - 0s 26ms/step - loss: 0.6174 - accuracy: 0.7035 - val_loss: 0.6968 - val_accuracy: 0.5825\n",
      "Epoch 16/500\n",
      "18/18 [==============================] - 0s 28ms/step - loss: 0.6152 - accuracy: 0.6702 - val_loss: 0.6427 - val_accuracy: 0.6214\n",
      "Epoch 17/500\n",
      "18/18 [==============================] - 0s 26ms/step - loss: 0.6191 - accuracy: 0.6684 - val_loss: 0.6629 - val_accuracy: 0.6117\n",
      "Epoch 18/500\n",
      "18/18 [==============================] - 0s 24ms/step - loss: 0.6091 - accuracy: 0.6825 - val_loss: 0.6534 - val_accuracy: 0.6019\n",
      "Epoch 19/500\n",
      "18/18 [==============================] - 1s 30ms/step - loss: 0.6042 - accuracy: 0.6772 - val_loss: 0.6977 - val_accuracy: 0.5728\n",
      "Epoch 20/500\n",
      "18/18 [==============================] - 1s 28ms/step - loss: 0.6006 - accuracy: 0.6807 - val_loss: 0.6921 - val_accuracy: 0.5825\n",
      "Epoch 21/500\n",
      "18/18 [==============================] - 0s 27ms/step - loss: 0.5972 - accuracy: 0.6825 - val_loss: 0.6725 - val_accuracy: 0.5922\n",
      "Epoch 22/500\n",
      "18/18 [==============================] - 0s 28ms/step - loss: 0.5969 - accuracy: 0.6965 - val_loss: 0.6850 - val_accuracy: 0.5922\n",
      "Epoch 23/500\n",
      "18/18 [==============================] - 0s 24ms/step - loss: 0.5960 - accuracy: 0.7088 - val_loss: 0.6748 - val_accuracy: 0.5922\n",
      "Epoch 24/500\n",
      "18/18 [==============================] - 0s 24ms/step - loss: 0.5907 - accuracy: 0.7140 - val_loss: 0.7364 - val_accuracy: 0.5728\n",
      "Epoch 25/500\n",
      "18/18 [==============================] - 0s 21ms/step - loss: 0.5968 - accuracy: 0.6684 - val_loss: 0.6812 - val_accuracy: 0.5922\n",
      "Epoch 26/500\n",
      "18/18 [==============================] - 0s 23ms/step - loss: 0.5864 - accuracy: 0.6947 - val_loss: 0.6646 - val_accuracy: 0.6019\n",
      "Epoch 27/500\n",
      "18/18 [==============================] - 1s 29ms/step - loss: 0.5850 - accuracy: 0.7053 - val_loss: 0.6856 - val_accuracy: 0.5728\n",
      "Epoch 28/500\n",
      "18/18 [==============================] - 0s 26ms/step - loss: 0.5696 - accuracy: 0.7211 - val_loss: 0.6820 - val_accuracy: 0.5825\n",
      "Epoch 29/500\n",
      "18/18 [==============================] - 0s 27ms/step - loss: 0.5707 - accuracy: 0.7246 - val_loss: 0.6910 - val_accuracy: 0.5825\n",
      "Epoch 30/500\n",
      "18/18 [==============================] - 0s 26ms/step - loss: 0.5687 - accuracy: 0.7158 - val_loss: 0.7400 - val_accuracy: 0.5825\n",
      "Epoch 31/500\n",
      "18/18 [==============================] - 1s 31ms/step - loss: 0.5717 - accuracy: 0.7053 - val_loss: 0.6556 - val_accuracy: 0.5922\n",
      "Epoch 32/500\n",
      "18/18 [==============================] - 0s 27ms/step - loss: 0.5698 - accuracy: 0.7175 - val_loss: 0.7128 - val_accuracy: 0.5631\n",
      "Epoch 33/500\n",
      "18/18 [==============================] - 0s 23ms/step - loss: 0.5649 - accuracy: 0.7193 - val_loss: 0.7360 - val_accuracy: 0.5631\n",
      "Epoch 34/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18/18 [==============================] - 0s 25ms/step - loss: 0.5618 - accuracy: 0.7228 - val_loss: 0.6661 - val_accuracy: 0.6019\n",
      "Epoch 35/500\n",
      "18/18 [==============================] - 0s 24ms/step - loss: 0.5633 - accuracy: 0.7298 - val_loss: 0.6909 - val_accuracy: 0.5825\n",
      "Epoch 36/500\n",
      "18/18 [==============================] - 0s 24ms/step - loss: 0.5556 - accuracy: 0.7333 - val_loss: 0.7281 - val_accuracy: 0.5437\n",
      "Epoch 37/500\n",
      "18/18 [==============================] - 0s 25ms/step - loss: 0.5513 - accuracy: 0.7246 - val_loss: 0.6912 - val_accuracy: 0.5631\n",
      "Epoch 38/500\n",
      "18/18 [==============================] - 0s 25ms/step - loss: 0.5497 - accuracy: 0.7439 - val_loss: 0.6987 - val_accuracy: 0.5631\n",
      "Epoch 39/500\n",
      "18/18 [==============================] - 0s 25ms/step - loss: 0.5447 - accuracy: 0.7368 - val_loss: 0.6817 - val_accuracy: 0.5631\n",
      "Epoch 40/500\n",
      "18/18 [==============================] - 0s 27ms/step - loss: 0.5446 - accuracy: 0.7263 - val_loss: 0.7253 - val_accuracy: 0.5534\n",
      "Epoch 41/500\n",
      "18/18 [==============================] - 0s 25ms/step - loss: 0.5420 - accuracy: 0.7298 - val_loss: 0.7019 - val_accuracy: 0.5922\n",
      "Epoch 42/500\n",
      "18/18 [==============================] - 0s 28ms/step - loss: 0.5391 - accuracy: 0.7333 - val_loss: 0.7436 - val_accuracy: 0.5631\n",
      "Epoch 43/500\n",
      "18/18 [==============================] - 1s 31ms/step - loss: 0.5425 - accuracy: 0.7439 - val_loss: 0.7431 - val_accuracy: 0.5534\n",
      "Epoch 44/500\n",
      "18/18 [==============================] - 0s 25ms/step - loss: 0.5218 - accuracy: 0.7596 - val_loss: 0.7238 - val_accuracy: 0.5534\n",
      "Epoch 45/500\n",
      "18/18 [==============================] - 0s 27ms/step - loss: 0.5215 - accuracy: 0.7526 - val_loss: 0.7470 - val_accuracy: 0.5437\n",
      "Epoch 46/500\n",
      "18/18 [==============================] - 0s 25ms/step - loss: 0.5480 - accuracy: 0.7246 - val_loss: 0.6886 - val_accuracy: 0.6019\n",
      "Epoch 47/500\n",
      "18/18 [==============================] - 0s 25ms/step - loss: 0.5312 - accuracy: 0.7439 - val_loss: 0.6882 - val_accuracy: 0.5922\n",
      "Epoch 48/500\n",
      "18/18 [==============================] - 0s 27ms/step - loss: 0.5205 - accuracy: 0.7596 - val_loss: 0.7327 - val_accuracy: 0.5340\n",
      "Epoch 49/500\n",
      "18/18 [==============================] - 0s 28ms/step - loss: 0.5147 - accuracy: 0.7596 - val_loss: 0.7468 - val_accuracy: 0.5437\n",
      "Epoch 50/500\n",
      "18/18 [==============================] - 1s 28ms/step - loss: 0.5162 - accuracy: 0.7649 - val_loss: 0.7133 - val_accuracy: 0.5728\n",
      "Epoch 51/500\n",
      "18/18 [==============================] - 1s 27ms/step - loss: 0.5057 - accuracy: 0.7561 - val_loss: 0.7339 - val_accuracy: 0.5534\n",
      "Epoch 52/500\n",
      "18/18 [==============================] - 0s 26ms/step - loss: 0.5102 - accuracy: 0.7544 - val_loss: 0.7455 - val_accuracy: 0.5340\n",
      "Epoch 53/500\n",
      "18/18 [==============================] - 0s 27ms/step - loss: 0.5030 - accuracy: 0.7632 - val_loss: 0.7441 - val_accuracy: 0.5340\n",
      "Epoch 54/500\n",
      "18/18 [==============================] - 0s 25ms/step - loss: 0.5155 - accuracy: 0.7526 - val_loss: 0.7085 - val_accuracy: 0.5922\n",
      "Epoch 55/500\n",
      "18/18 [==============================] - 0s 27ms/step - loss: 0.4938 - accuracy: 0.7667 - val_loss: 0.7638 - val_accuracy: 0.5243\n",
      "Epoch 56/500\n",
      "18/18 [==============================] - 0s 27ms/step - loss: 0.4922 - accuracy: 0.7789 - val_loss: 0.7347 - val_accuracy: 0.5728\n",
      "Epoch 57/500\n",
      "18/18 [==============================] - 0s 24ms/step - loss: 0.4890 - accuracy: 0.7772 - val_loss: 0.7295 - val_accuracy: 0.5922\n",
      "Epoch 58/500\n",
      "18/18 [==============================] - 0s 24ms/step - loss: 0.4934 - accuracy: 0.7596 - val_loss: 0.7822 - val_accuracy: 0.5437\n",
      "Epoch 59/500\n",
      "18/18 [==============================] - 0s 28ms/step - loss: 0.4905 - accuracy: 0.7772 - val_loss: 0.7329 - val_accuracy: 0.5922\n",
      "Epoch 60/500\n",
      "18/18 [==============================] - 0s 25ms/step - loss: 0.4844 - accuracy: 0.7825 - val_loss: 0.7774 - val_accuracy: 0.5534\n",
      "Epoch 61/500\n",
      "18/18 [==============================] - 0s 28ms/step - loss: 0.4760 - accuracy: 0.7930 - val_loss: 0.7404 - val_accuracy: 0.6019\n",
      "Epoch 62/500\n",
      "18/18 [==============================] - 0s 26ms/step - loss: 0.4842 - accuracy: 0.7737 - val_loss: 0.7332 - val_accuracy: 0.5922\n",
      "Epoch 63/500\n",
      "18/18 [==============================] - 0s 26ms/step - loss: 0.4766 - accuracy: 0.7842 - val_loss: 0.7738 - val_accuracy: 0.5728\n",
      "Epoch 64/500\n",
      "18/18 [==============================] - 0s 26ms/step - loss: 0.4777 - accuracy: 0.7772 - val_loss: 0.7615 - val_accuracy: 0.5922\n",
      "Epoch 65/500\n",
      "18/18 [==============================] - 1s 28ms/step - loss: 0.4742 - accuracy: 0.7842 - val_loss: 0.7752 - val_accuracy: 0.5728\n",
      "Epoch 66/500\n",
      "18/18 [==============================] - 0s 23ms/step - loss: 0.4653 - accuracy: 0.7912 - val_loss: 0.8207 - val_accuracy: 0.5534\n",
      "Epoch 67/500\n",
      "18/18 [==============================] - 0s 26ms/step - loss: 0.4720 - accuracy: 0.7825 - val_loss: 0.7462 - val_accuracy: 0.6019\n",
      "Epoch 68/500\n",
      "18/18 [==============================] - 0s 25ms/step - loss: 0.4710 - accuracy: 0.7842 - val_loss: 0.7514 - val_accuracy: 0.6117\n",
      "Epoch 69/500\n",
      "18/18 [==============================] - 0s 23ms/step - loss: 0.4632 - accuracy: 0.7912 - val_loss: 0.7776 - val_accuracy: 0.5825\n",
      "Epoch 70/500\n",
      "18/18 [==============================] - 0s 26ms/step - loss: 0.4695 - accuracy: 0.7842 - val_loss: 0.7984 - val_accuracy: 0.5728\n",
      "Epoch 71/500\n",
      "18/18 [==============================] - 0s 23ms/step - loss: 0.4594 - accuracy: 0.7947 - val_loss: 0.8256 - val_accuracy: 0.5631\n",
      "Epoch 72/500\n",
      "18/18 [==============================] - 0s 25ms/step - loss: 0.4659 - accuracy: 0.7860 - val_loss: 0.7713 - val_accuracy: 0.6117\n",
      "Epoch 73/500\n",
      "18/18 [==============================] - 1s 29ms/step - loss: 0.4604 - accuracy: 0.7947 - val_loss: 0.7994 - val_accuracy: 0.5728\n",
      "Epoch 74/500\n",
      "18/18 [==============================] - 0s 27ms/step - loss: 0.4579 - accuracy: 0.8053 - val_loss: 0.7592 - val_accuracy: 0.6019\n",
      "Epoch 75/500\n",
      "18/18 [==============================] - 0s 27ms/step - loss: 0.4526 - accuracy: 0.8018 - val_loss: 0.8071 - val_accuracy: 0.5728\n",
      "Epoch 76/500\n",
      "18/18 [==============================] - 0s 27ms/step - loss: 0.4556 - accuracy: 0.7912 - val_loss: 0.7857 - val_accuracy: 0.6019\n",
      "Epoch 77/500\n",
      "18/18 [==============================] - 0s 28ms/step - loss: 0.4562 - accuracy: 0.7895 - val_loss: 0.8232 - val_accuracy: 0.5728\n",
      "Epoch 78/500\n",
      "18/18 [==============================] - 0s 28ms/step - loss: 0.4553 - accuracy: 0.7860 - val_loss: 0.7708 - val_accuracy: 0.6117\n",
      "Epoch 79/500\n",
      "18/18 [==============================] - 0s 28ms/step - loss: 0.4416 - accuracy: 0.8246 - val_loss: 0.7891 - val_accuracy: 0.6019\n",
      "Epoch 80/500\n",
      "18/18 [==============================] - 0s 27ms/step - loss: 0.4313 - accuracy: 0.8246 - val_loss: 0.8329 - val_accuracy: 0.5922\n",
      "Epoch 81/500\n",
      "18/18 [==============================] - 1s 31ms/step - loss: 0.4452 - accuracy: 0.7877 - val_loss: 0.7908 - val_accuracy: 0.6019\n",
      "Epoch 82/500\n",
      "18/18 [==============================] - 1s 30ms/step - loss: 0.4243 - accuracy: 0.8263 - val_loss: 0.7963 - val_accuracy: 0.5825\n",
      "Epoch 83/500\n",
      "18/18 [==============================] - 1s 29ms/step - loss: 0.4305 - accuracy: 0.7930 - val_loss: 0.7920 - val_accuracy: 0.6019\n",
      "Epoch 84/500\n",
      "18/18 [==============================] - 1s 29ms/step - loss: 0.4365 - accuracy: 0.8018 - val_loss: 0.7855 - val_accuracy: 0.6019\n",
      "Epoch 85/500\n",
      "18/18 [==============================] - 1s 30ms/step - loss: 0.4325 - accuracy: 0.8088 - val_loss: 0.8425 - val_accuracy: 0.5631\n",
      "Epoch 86/500\n",
      "18/18 [==============================] - 0s 27ms/step - loss: 0.4264 - accuracy: 0.8123 - val_loss: 0.8273 - val_accuracy: 0.6019\n",
      "Epoch 87/500\n",
      "18/18 [==============================] - 0s 24ms/step - loss: 0.4166 - accuracy: 0.8193 - val_loss: 0.7943 - val_accuracy: 0.6019\n",
      "Epoch 88/500\n",
      "18/18 [==============================] - 0s 24ms/step - loss: 0.4301 - accuracy: 0.8105 - val_loss: 0.8672 - val_accuracy: 0.5728\n",
      "Epoch 89/500\n",
      "18/18 [==============================] - 0s 25ms/step - loss: 0.4243 - accuracy: 0.8070 - val_loss: 0.8305 - val_accuracy: 0.6019\n",
      "Epoch 90/500\n",
      "18/18 [==============================] - 0s 24ms/step - loss: 0.4148 - accuracy: 0.8246 - val_loss: 0.8097 - val_accuracy: 0.5825\n",
      "Epoch 91/500\n",
      "18/18 [==============================] - 0s 26ms/step - loss: 0.4076 - accuracy: 0.8263 - val_loss: 0.8290 - val_accuracy: 0.5922\n",
      "Epoch 92/500\n",
      "18/18 [==============================] - 0s 27ms/step - loss: 0.3951 - accuracy: 0.8474 - val_loss: 0.8274 - val_accuracy: 0.5922\n",
      "Epoch 93/500\n",
      "18/18 [==============================] - 0s 24ms/step - loss: 0.3984 - accuracy: 0.8368 - val_loss: 0.8160 - val_accuracy: 0.6117\n",
      "Epoch 94/500\n",
      "18/18 [==============================] - 0s 25ms/step - loss: 0.4116 - accuracy: 0.8193 - val_loss: 0.8227 - val_accuracy: 0.5922\n",
      "Epoch 95/500\n",
      "18/18 [==============================] - 0s 26ms/step - loss: 0.4028 - accuracy: 0.8333 - val_loss: 0.8869 - val_accuracy: 0.5631\n",
      "Epoch 96/500\n",
      "18/18 [==============================] - 0s 26ms/step - loss: 0.4030 - accuracy: 0.8175 - val_loss: 0.8228 - val_accuracy: 0.6019\n",
      "Epoch 97/500\n",
      "18/18 [==============================] - 0s 26ms/step - loss: 0.4072 - accuracy: 0.8281 - val_loss: 0.8957 - val_accuracy: 0.5631\n",
      "Epoch 98/500\n",
      "18/18 [==============================] - 0s 26ms/step - loss: 0.4090 - accuracy: 0.8316 - val_loss: 0.8121 - val_accuracy: 0.6214\n",
      "Epoch 99/500\n",
      "18/18 [==============================] - 1s 30ms/step - loss: 0.3911 - accuracy: 0.8316 - val_loss: 0.8242 - val_accuracy: 0.6311\n",
      "Epoch 100/500\n",
      "18/18 [==============================] - 0s 27ms/step - loss: 0.3943 - accuracy: 0.8386 - val_loss: 0.8334 - val_accuracy: 0.6117\n",
      "Epoch 101/500\n",
      "18/18 [==============================] - 0s 27ms/step - loss: 0.3974 - accuracy: 0.8404 - val_loss: 0.8319 - val_accuracy: 0.6117\n",
      "Epoch 102/500\n",
      "18/18 [==============================] - 1s 28ms/step - loss: 0.3798 - accuracy: 0.8649 - val_loss: 0.8377 - val_accuracy: 0.6214\n",
      "Epoch 103/500\n",
      "18/18 [==============================] - 1s 29ms/step - loss: 0.3809 - accuracy: 0.8439 - val_loss: 0.8672 - val_accuracy: 0.5728\n",
      "Epoch 104/500\n",
      "18/18 [==============================] - 1s 30ms/step - loss: 0.3894 - accuracy: 0.8404 - val_loss: 0.8352 - val_accuracy: 0.6214\n",
      "2/2 [==============================] - 0s 3ms/step\n"
     ]
    }
   ],
   "source": [
    "# Variable for keeping count of split we are executing\n",
    "j = 0\n",
    "list_kfold = list(kfold.split(X_trainval, y_trainval))\n",
    "data_kfold = pd.DataFrame()\n",
    "for i in range(5):\n",
    "    train_idx, val_idx = list_kfold[i]\n",
    "    X_train_kfold = X_trainval[train_idx]\n",
    "    y_train_kfold = y_trainval[train_idx]\n",
    "    X_train_kfold_over, y_train_kfold_over = get_resampling(X_train_kfold, y_train_kfold)  \n",
    "    X_val_kfold = X_trainval[val_idx]\n",
    "    y_val_kfold = y_trainval[val_idx]\n",
    "    y_val_kfold_complement = 1 - y_val_kfold\n",
    "    y_val_kfold = np.stack((y_val_kfold_complement, y_val_kfold), axis=1)\n",
    "    #RMK, here since y_train_kfold_over's order is non-pro, pro, we set y_val_kfold's order also be non-pro, pro\n",
    "    j += 1\n",
    "    model_kfold = get_model()\n",
    "    model_kfold.fit(X_train_kfold_over, y_train_kfold_over,\n",
    "                   validation_data = (X_val_kfold, y_val_kfold),\n",
    "                   epochs = 500, verbose = True,\n",
    "                   callbacks = [early_stopping_monitor])\n",
    "    pred = model_kfold.predict(X_test)\n",
    "    predicted_class_indices=np.argmax(pred, axis = 1)\n",
    "    data_kfold[j] = predicted_class_indices\n",
    "#    gc.collect()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "b4817b02",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    1  2  3  4  5\n",
       "0   0  0  0  0  0\n",
       "1   0  0  0  0  0\n",
       "2   0  0  0  0  0\n",
       "3   0  0  0  0  0\n",
       "4   0  0  0  0  0\n",
       "5   0  0  1  1  1\n",
       "6   0  0  1  1  0\n",
       "7   0  0  0  0  0\n",
       "8   0  0  0  1  0\n",
       "9   0  0  0  0  0\n",
       "10  0  0  0  0  0\n",
       "11  0  0  0  0  0\n",
       "12  0  0  0  0  1\n",
       "13  0  0  0  0  0\n",
       "14  0  0  0  0  0\n",
       "15  0  0  0  0  0\n",
       "16  0  0  0  0  0\n",
       "17  0  0  0  0  0\n",
       "18  0  0  0  0  0\n",
       "19  0  0  0  0  0\n",
       "20  0  0  0  0  0\n",
       "21  0  0  0  0  0\n",
       "22  0  0  0  0  0\n",
       "23  0  0  0  0  0\n",
       "24  0  0  0  0  0\n",
       "25  0  0  0  0  0\n",
       "26  0  0  0  0  0\n",
       "27  0  0  0  0  0\n",
       "28  0  0  0  0  0\n",
       "29  0  0  1  0  0\n",
       "30  0  0  0  0  0\n",
       "31  0  0  0  0  0\n",
       "32  0  0  0  0  1\n",
       "33  0  0  0  1  0\n",
       "34  0  0  0  0  0\n",
       "35  0  0  1  1  1\n",
       "36  0  0  0  0  0\n",
       "37  0  0  0  0  0\n",
       "38  0  0  0  0  0\n",
       "39  0  0  0  0  0\n",
       "40  0  0  0  0  0\n",
       "41  0  0  0  0  0\n",
       "42  0  0  0  0  0\n",
       "43  0  0  0  0  0\n",
       "44  0  0  0  0  0\n",
       "45  1  0  1  1  1\n",
       "46  1  0  1  1  1\n",
       "47  0  0  0  0  0\n",
       "48  0  0  0  0  1\n",
       "49  0  0  0  0  0\n",
       "50  0  0  0  0  0\n",
       "51  0  0  0  0  0\n",
       "52  0  0  0  0  0\n",
       "53  0  0  0  0  0\n",
       "54  0  0  0  0  0\n",
       "55  0  0  0  0  1\n",
       "56  0  0  1  1  1\n",
       "57  0  0  0  1  0\n",
       "58  0  0  1  1  1"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_kfold\n",
    "#RMK, since we set the first column to be non-pro, second to be pro, we get 0 as predicted non-pro, 1 as predicted \n",
    "#pro."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "fe829531",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0],\n",
       "       [0, 0, 1, 1, 1],\n",
       "       [0, 0, 1, 1, 0],\n",
       "       [0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 1, 0],\n",
       "       [0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 1],\n",
       "       [0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0],\n",
       "       [0, 0, 1, 0, 0],\n",
       "       [0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 1],\n",
       "       [0, 0, 0, 1, 0],\n",
       "       [0, 0, 0, 0, 0],\n",
       "       [0, 0, 1, 1, 1],\n",
       "       [0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0],\n",
       "       [1, 0, 1, 1, 1],\n",
       "       [1, 0, 1, 1, 1],\n",
       "       [0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 1],\n",
       "       [0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 1],\n",
       "       [0, 0, 1, 1, 1],\n",
       "       [0, 0, 0, 1, 0],\n",
       "       [0, 0, 1, 1, 1]])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_kfold_arr = np.array(data_kfold) #Convert 'data_kfold' from a dataframe to an array.\n",
    "data_kfold_arr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f78bbd69",
   "metadata": {},
   "source": [
    "Once there is at least one 1, then we will consider this patient is a progressor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "8f290cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_kfold = np.array(range(len(data_kfold_arr)))\n",
    "for i in range(len(data_kfold_arr)):\n",
    "    sum_kfold = sum(data_kfold_arr[i])\n",
    "    if sum_kfold > 0:\n",
    "        predict_kfold[i] = 1 #Progressor\n",
    "    else:\n",
    "        predict_kfold[i] = 0 #Non-progressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "1b07b0fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_kfold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "c820c440",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Non-Progressor</th>\n",
       "      <th>Progressor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Non-Progressor  Progressor\n",
       "0                1           0\n",
       "1                1           0\n",
       "2                1           0\n",
       "3                1           0\n",
       "4                1           0\n",
       "5                0           1\n",
       "6                0           1\n",
       "7                1           0\n",
       "8                0           1\n",
       "9                1           0\n",
       "10               1           0\n",
       "11               1           0\n",
       "12               0           1\n",
       "13               1           0\n",
       "14               1           0\n",
       "15               1           0\n",
       "16               1           0\n",
       "17               1           0\n",
       "18               1           0\n",
       "19               1           0\n",
       "20               1           0\n",
       "21               1           0\n",
       "22               1           0\n",
       "23               1           0\n",
       "24               1           0\n",
       "25               1           0\n",
       "26               1           0\n",
       "27               1           0\n",
       "28               1           0\n",
       "29               0           1\n",
       "30               1           0\n",
       "31               1           0\n",
       "32               0           1\n",
       "33               0           1\n",
       "34               1           0\n",
       "35               0           1\n",
       "36               1           0\n",
       "37               1           0\n",
       "38               1           0\n",
       "39               1           0\n",
       "40               1           0\n",
       "41               1           0\n",
       "42               1           0\n",
       "43               1           0\n",
       "44               1           0\n",
       "45               0           1\n",
       "46               0           1\n",
       "47               1           0\n",
       "48               0           1\n",
       "49               1           0\n",
       "50               1           0\n",
       "51               1           0\n",
       "52               1           0\n",
       "53               1           0\n",
       "54               1           0\n",
       "55               0           1\n",
       "56               0           1\n",
       "57               0           1\n",
       "58               0           1"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_kfold_complement = 1 - predict_kfold #Column for non-pro\n",
    "predict_kfold_2D = np.stack((predict_kfold_complement, predict_kfold), axis=1)\n",
    "predict_kfold_2D = pd.DataFrame(predict_kfold_2D, columns=('Non-Progressor', 'Progressor'))\n",
    "predict_kfold_2D"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ac3fb62",
   "metadata": {},
   "source": [
    "Evaluation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "fe255b1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "roc auc score:  0.5983709273182958\n",
      "average precision score:  0.55845920978615\n"
     ]
    }
   ],
   "source": [
    "roc_value = roc_auc_score(y_test, predict_kfold_2D)\n",
    "ap_score = average_precision_score(y_test, predict_kfold_2D)\n",
    "print('roc auc score: ', roc_value)\n",
    "print('average precision score: ', ap_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "9c2c4def",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_c = predict_kfold_2D.to_numpy()\n",
    "y_c = y_c.astype('int32')\n",
    "y_test_np = y_test.to_numpy()\n",
    "y_test_np = y_test_np.astype('int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "39e78d8b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is 0.6610169491525424.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "cf_matrix = confusion_matrix(y_test_np.argmax(axis=1), y_c.argmax(axis=1)) \n",
    "#cf_matrix = confusion_matrix(y_test_np[:, 1], y_c[:, 1]) \n",
    "\n",
    "ax = sns.heatmap(cf_matrix, annot=True, cmap='Blues')\n",
    "\n",
    "ax.set_title('Seaborn Confusion Matrix with labels\\n\\n');\n",
    "ax.set_xlabel('\\nPredicted Values')\n",
    "ax.set_ylabel('Actual Values ');\n",
    "\n",
    "## Ticket labels - List must be in alphabetical order\n",
    "ax.xaxis.set_ticklabels(['Non-Progressor','Progressor'])\n",
    "ax.yaxis.set_ticklabels(['Non-Progressor','Progressor'])\n",
    "\n",
    "## Display the visualization of the Confusion Matrix.\n",
    "plt.savefig('M1_GRI_test.png')\n",
    "print(f'Accuracy is {(cf_matrix[0,0]+cf_matrix[1,1])/(sum(sum(cf_matrix)))}.')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc1ff362",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f908268",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8745bdec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "898a7565",
   "metadata": {},
   "source": [
    "### Idea 5: Using all numerical values as the predictors, 'Y_combined' as the dependent variable, no resampling, CNN method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b967abdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adding every numerical variables into the list of predictors.\n",
    "df = df_raw.iloc[:, np.r_[1, 28:797, 814]]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bcf98e6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
